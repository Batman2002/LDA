{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import tabula\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from pdfminer.high_level import extract_pages,extract_text\n",
    "import fitz\n",
    "from PIL import Image\n",
    "import io\n",
    "import cv2\n",
    "\n",
    "image_names_arr=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(path):\n",
    "    pdf=fitz.open(path)\n",
    "    counter=1\n",
    "    for i in range(len(pdf)):\n",
    "        page=pdf[i]\n",
    "        images=page.get_images()\n",
    "        for image in images:\n",
    "            base_img=pdf.extract_image(image[0])\n",
    "            image_data=base_img['image']\n",
    "            img=Image.open(io.BytesIO(image_data))\n",
    "            extension=base_img['ext']\n",
    "            img.save(open(f'image{counter}.{extension}','wb'))\n",
    "            image_names_arr.append(f'image{counter}.{extension}')\n",
    "            counter+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_images(r\"C:\\Users\\Chinmay\\Desktop\\Chinmay\\be pws1\\Mapping_Climate_Themes_From_2008-2021An_Analysis_of_Business_News_Using_Topic_Models.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(path):\n",
    "    text=extract_text(path)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=get_text(r\"C:\\Users\\Chinmay\\Desktop\\Chinmay\\be pws1\\Mapping_Climate_Themes_From_2008-2021An_Analysis_of_Business_News_Using_Topic_Models.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Received 7 February 2023, accepted 2 March 2023, date of publication 13 March 2023, date of current version 21 March 2023.\\n\\nDigital Object Identifier 10.1109/ACCESS.2023.3256530\\n\\nMapping Climate Themes From 2008-2021—An\\nAnalysis of Business News Using Topic Models\\n\\nSWARNALAKSHMI UMAMAHESWARAN, VANDITA DAR , ELIZA SHARMA ,\\nAND JIKKU SUSAN KURIAN\\nSymbiosis Institute of Business Management, Symbiosis International (Deemed University), Bengaluru 560100, India\\n\\nCorresponding author: Vandita Dar (vandita.dar@sibm.edu.in)\\n\\nABSTRACT India and other developing economies are receiving more attention in the context of climate\\nchange due to their rapid rates of economic expansion and large populations. In terms of absolute emissions,\\nIndia surpassed China and the U.S. in 2018 to become the third-largest emitter. Solving this wicked problem\\ncalls for climate action across the stakeholder spectrum involving governments, business communities,\\nand citizens. While extant literature has focused significantly on the role of governments and individual\\nperceptions, the business sector needs to be more represented. In this study, we consider business news\\nmedia as a platform that reflects the industry engagement in climate change and as a source of information\\non climate change for business decision-makers. Hence, understanding the topic and themes in the nexus\\nof climate and business is important to evaluate the business sector’s stance towards climate change and\\nhow it has evolved. This work explores business news related to climate change using natural language\\ntechniques. We first experiment with three topic-modeling techniques, such as LDA, NMF, and BERTopic,\\non the business news and two more benchmark news datasets. Our test data is derived from digital news\\narchives of ’The Economic Times – India’s leading business news daily. We evaluate the performance\\nbased on quantitative metrics commonly used for topic models. We choose the algorithm that provides the\\nhighest precision for climate-specific information represented by the test dataset. We then apply the algorithm\\nwith the best performance, as evaluated by the experiment, to a large corpus of Indian climate news from\\nE.T. spanning from 2008 -2021. We present how different themes, including industry engagement, evolved\\nover the last two decades. The results suggest that climate cooperation has the highest contribution in the\\ncorpus, with other themes on resource management, energy and business gaining traction in recent years.\\n\\nINDEX TERMS Climate change, media, topic models, NLP, computational social sciences, experiment.\\n\\nI. INTRODUCTION\\nClimate change is the most critical issue of this millennium.\\nIndia has high stakes in the global debate since it’s the third\\nlargest emitter and also high on climate vulnerability [1].\\nIn its most recent update to the Paris pledge, the country has\\nfurther committed to reducing its emission intensity to 35% of\\n2005 levels by 2030. Emission reduction of this scale requires\\nsolid public support across the stakeholder spectrum, includ-\\ning citizens and enterprises. The role of the commercial sector\\nis particularly relevant given its contribution to country-level\\nemissions.\\n\\nThe associate editor coordinating the review of this manuscript and\\n\\napproving it for publication was Arianna Dulizia\\n\\n.\\n\\nA practical approach toward understanding how different\\nstakeholders engage in climate change can be through the\\nanalysis of news coverage. While all news media reflect\\ncontemporary public discourse, business newspapers tra-\\nditionally report news that is more relevant for business\\ndecision-makers. Therefore, mapping the topics pertinent to\\nclimate change in business media and how such issues have\\nevolved can help us understand how the commercial sector\\nengages with the issue of climate change.\\n\\nAutomated content analysis of large corpora is an estab-\\nlished technique in the social sciences. Specifically, the use\\nof topic models for extracting latent topics from newspapers,\\ndiscussion forums, and other social media content has been\\ngaining traction in extant literature. Among the different\\n\\n26554\\n\\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\\n\\nVOLUME 11, 2023\\n\\n\\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models\\n\\nNatural Language Processing (NLP) techniques, topic mod-\\neling is a machine-learning task that groups documents and\\nwords with similar meanings. All topic models yield topics,\\neach ranked based on the specific terms and their relevance\\nto the topic. The topic emerges automatically without prior\\nannotation or labeling of the raw data. In this sense, topic\\nmodels form an important category of unsupervised tech-\\nniques within the larger field of Natural Language Process-\\ning. The are many mathematical approaches to topic models\\nand an even higher spread of algorithms that learn the param-\\neters of such model approaches. Conventional topic models\\nrange from linear algebraic-based techniques such as LSA\\nto more popular probabilistic models like LDA and PLSA.\\nMetrics decomposition-based methods such as NMF have\\nalso been a mainstay of NLP applications. The underlying\\nBag Of Words (BOW) approach is a distinguishing factor\\namong all these approaches. However, the BoW-based topic\\nmodel is based on the unrealistic assumption that words\\nare independent of each other and relies on frequencies of\\nword occurrences. A more recent development in NLP, is the\\nrepresentation of text using embeddings. Embeddings in the\\ncontext of NLP are dense representations of units of the text\\nin the vector space in a way that the semantic similarity\\nbetween the units of text is captured. Although automated\\ncontent analysis is gaining importance within the climate\\ndiscourse literature, there is an overreliance on probabilistic\\ntopic models [2], [3], [4], [5]. More importantly, the choice of\\ntopic models such as LDA is motivated by popular use and is\\nnot backed by experiments to evaluate performance metrics\\nespecially in the social sciences [6]. Given this background,\\nthe objective of this article is to evaluate the performance\\nof conventional BOW-based models with embeddings-based\\nmodels. Specifically, we run experiments using LDA, NMF,\\nand BERTopic. We choose the most appropriate model based\\non the evaluation metrics and use it to map themes within the\\nEconomic Times corpus.\\n\\nThe article is organized as follows. Section II introduces\\nthe literature on automated content analysis of climate news\\nand discusses how topic models are used in the literature.\\nSection III presents the data and methods. Section IV offers\\nthe details of the experiment and its results. Section V\\npresents the results of the final implantation of the chosen\\nmodel.\\n\\nII. LITERATURE REVIEW\\nThe adoption of NLP techniques in social sciences has\\nseen significant growth in the recent decade. Language is a\\nsocial construct and can be considered a proxy for behav-\\nior [7]. As expressed by people,\\nlanguage may contain\\nrich latent information that can help identify several dimen-\\nsions and traits of behavior. Therefore, the rapid growth of\\nNLP methods within social sciences is not surprising. The\\nsocial science applications of NLP range from understand-\\ning political affiliations and voter intentions [8], [9], [10],\\nhealth care delivery [11], [12], media monitoring [13], and\\n\\nimproving public policy implementation [14]. The use of\\nNLP in mining social discourse around climate change is\\nstill nascent and broadly categorized based on the analy-\\nsis of social media platforms, online news, and scientific\\nor technical documents. Several studies analyze societal\\nstance on climate change through the study of Twitter and\\nmicroblogs. For example, Elgasem et al. combine sentiment\\nand networking analysis to identify climate skeptics and\\naccept communities [15]. Stance detection in climate change\\ntweets emerged as a separate subdomain with the release\\nof the SemEval dataset, which defined detecting climate\\nconcerns as a task [8]. Several researchers have employed\\nadvanced NLP and machine learning techniques for improv-\\ning stance detection in climate tweets with the Semeval\\ndataset [16].\\n\\nWhile content analysis of climate news is an established\\nnorm, the use of NLP in analyzing climate news is relatively\\nnew. Compared with the studies on social media, news analy-\\nsis has received less attention [17]. Keller et al. in [2] applied\\nLDA to ∼18000 climate change articles sourced from two\\nIndian dailies. They extracted 29 themes ranging from cli-\\nmate change impact and politics to climate science. Similarly,\\nBohr [3] segments 52 US newspapers based on geography,\\npartisan bias, circulation, etc. He then uses structural topic\\nmodels to discover topics and further analyses the influence\\nof these attributes on the topics [3]. Benites-Lazaro et al. [4]\\nuse a corpus of news articles between 2007 -2017 to analyze\\nthe climate, food, and energy nexus using topic models. From\\na methodological perspective, most authors have used topic\\nmodels, especially the popular Latent Dirichlet Allocation\\n(LDA) [2], [3], [4]. Despite the popularity of LDA, it suf-\\nfers from certain shortcomings. Notably, the foundational\\nassumption of LDA is that documents are an unordered set,\\ni.e., the words that appear in the text are independent of each\\nother [4]. This assumption is common to all models based\\non the Bag of Words text representation. This assumption\\nconflicts with linguistic theory, which suggests that words\\nin any human language are always connected and sequen-\\ntial [18]. Another aspect of LDA is that it needs the number\\nof topics extracted as input right at the beginning. Know-\\ning the optimal number of issues at the beginning is only\\nfeasible in some contexts and hence introduces an element\\nof subjectivity [17]. Finally, labeling the extracted topics is\\nalso left to the users’ discretion and can vary considerably\\nbetween coders. Therefore for credible topic extraction as\\nwell as validation, additional information from subject matter\\nexperts becomes crucial [19]. In this sense, LDA models do\\nnot lend to automation. While some of the shortcomings,\\nsuch as having to specify a specific number of topics, can be\\ncountered using matrix factorization methods such as NMF,\\nthe underlying issues related to BoW representation remain\\nthe same. The dependency on classical topic models is partic-\\nularly conspicuous given several algorithmic developments in\\nthe domain. For example Topic models based on embeddings\\nhave gained traction in recent years. Embeddings are dense\\ntext representations such that words with semantic similarity\\n\\nVOLUME 11, 2023\\n\\n26555\\n\\n\\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models\\n\\nFIGURE 1. Diagrammatic representation of LDA.\\n\\nare closer in a low dimensional space [20]. Word embeddings\\ncapture the relationship between words and convey context\\nbetter than the word count-based BoW representation of text.\\nIn general, embeddings-based models have reported supe-\\nrior performance. However, the application of embeddings\\nbased topic models in social sciences has been sparse [21].\\nAnother significant gap in social science based applications\\nis the lack of robust experiments for model selection. Stud-\\nies typically choose a particular model on a apriori basis\\nbased on theoretical assumptions or prior literature contribu-\\ntions [14], [22], [23]\\n\\nIII. METHODS\\nA. LATENT DIRICHLET ALLOCATION\\nLDA is the most basic model in probabilistic topic models\\nand is popular in several social science disciplines [5]. LDA\\ntypically works on a Bag of Words vector representation of\\ndocuments of a specific length. The learning algorithm in\\nLDA is unsupervised and is used to discover latent semantic\\npatterns in unstructured documents. The technique assumes\\nthat each document is generated by a complex probabilistic\\ngenerative mechanism.LDA assumes that each document is\\ncreated one word at a time by selecting a topic from the\\ndocument’s topic distribution and then picking a word from\\nthat topic. Therefore, each document is modeled as a mixture\\nof latent topics and each topic is modeled as a multinomial\\ndistribution of words. Thus, the learning in LDA is to estimate\\nthe parameters of the underlying mechanism that most likely\\ngenerated the corpus as precisely as possible. Fig 1 provides\\nthe outline of the mathematical model\\n\\nWere\\nN — Number of words in each document.\\nk\\n\\n— Number of topics a document belongs to (a\\nfixed number).\\n\\n— Single document.\\n\\nD — Corpus, a collection of M documents.\\nd\\nzd,n —topic for the nth word in the dth document.\\nθ\\n— Distribution of topics for each document.\\n\\nβk — Distribution of words within each topic up\\n\\nα, η — parameters of prior distributions over θ\\n\\nto k.\\n\\nand β.\\n\\nThe distribution of topics θ derives from a Dirichlet distri-\\nbution. The use of Dirichlet allows each document to embed\\ntopic sparsity, thus mimicking real-word documents. The\\ngenerative process then can be defined as the joint probability\\nof the documents and topics as represented in\\n\\np(θ, z, w | α, β)\\n\\n(1)\\n\\nwhere α, and β refer to the hyperparameter related to the\\nDirichlet distribution. The joint distributions are further com-\\nputed as the conditional distribution of the hidden topics\\ngiven the set of documents. The mathematical formulation is\\ngiven in Eqn 2.\\n\\np(θ, z | w, α, β) =\\n\\np(θ, z, w | α, β)\\np(w | α, β)\\n\\n(2)\\n\\nHowever, computing the above expression can be difficult\\nsince the denominator requires the summation of all possible\\ncombinations of topics. The use of approximation methods\\nsuch as Markov Chain Monte Carlo and variational inference\\nis standard in this context.\\n\\nB. NON-NEGATIVE MATRIX FACTORIZATION\\nNMF is a linear algebra-based multivariate technique based\\non matrix decompositions of a high dimensional vector space.\\nThe decomposed non-negative matrices represent hidden\\nstructures considered coordinate axes in a transformed low-\\ndimensional vector space [24]. NMF considers every individ-\\nual document d as a vector of its terms. We then represent the\\nterm-document matrix D as follows\\n\\nD = [d 1, d 2, . . . , d n] ∈ RMN\\n\\n(3)\\n\\nD can be decomposed into low dimensional vector spaces as\\nrepresented by matrices U & V shown in equation 6. Elements\\nof U and V are non-negative real numbers indicated by the\\n\\n26556\\n\\nVOLUME 11, 2023\\n\\n\\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models\\n\\nFIGURE 2. Diagrammatic representation of NMF.\\n\\nmatrix R.U has dimensions M and K where M denotes topics\\nand K denotes terms or words\\n\\ndistribution for every cluster, thus creating topic representa-\\ntions from clustered embeddings\\n\\nU = [u1, u2, . . . , uK ] ∈ RMN\\nV = [v1, v2, . . . , vN ] ∈ RKN\\nD ≈ UV\\n\\n(4)\\n(5)\\n(6)\\n\\nWhile V represents K topics and N documents. A more\\nintuitive representation of NMF is given in Fig 2. The quality\\nof the formulation in Equation 4 is measured as the squared\\ndifference between the document term matrix D and U.V.\\nThis can be formally represented below.\\n\\nMin ||D − UV∥2\\n\\n(7)\\n\\nNMF learns U and V iteratively using multiplicative updates\\nsuch that the error is minimized [25]\\n\\nC. BERTopic\\nBERTopic [19] is a combination of several modules. At the\\ntop it involves representing text as Embeddings using a\\npre-trained language model-specifically the Bidirectional\\nEncoder Representations from Transformers(BERT). Such a\\nrepresentation generates a dense matrix where text either in\\nsentences or paragraphs is presented as numeric vectors that\\ncapture the semantic similarity. The second layer involves\\nusing a dimensionality reduction algorithm such as UMAP.\\nto convert the embeddings into a low-dimensional space.\\nFinally, the reduced embeddings are clustered by HDB-\\nSCAN. or K-means algorithms. At the final layer the topic\\nrepresentations are done such that each cluster is assigned a\\ntopic. The topics are then represented using the TF-IDF mea-\\nsure, which combines term frequency and inverse document\\nfrequency for rank ordering the importance of words in the\\ntext. We define the classical TF-IDF process below\\n\\nDt,d = tft,d · log\\n\\n(cid:19)\\n\\n(cid:18) N\\ndft\\n\\n(8)\\n\\nwhere\\n\\nD - Term document matrix\\nTf - Term frequency\\nDf - Document frequency\\nBERTopic alters the application of the TF-IDF procedure\\nusing a cluster of documents rather than an individual doc-\\nument. This enables the BERTopic to output the topic word\\n\\nD. EVALUATION METRICS\\nWhen measuring topic quality, human evaluation of a topic\\nis considered a gold standard. However, it is a resource\\nintensive and expensive strategy to implement. As an alter-\\nnative, recent literature has focused on the automatic mea-\\nsurement of coherence measures that closely mimic human\\njudgment. Topic coherence generally measures the degree\\nto which the words in a topic are semantically related and\\nmake sense together [26] Alternatively, coherence can also\\nbe defined as a measure of the internal consistency of a\\ntopic and how well it represents the documents it is gen-\\nerated from [27].. For measuring semantic similarity, the\\nmost common methods are the Normalized Pointwise Mutual\\nInformation (NPMI.). The NPMI. method is based on context\\nvectors\\nj. It is constructed by extracting co-occurrences\\nand counts within a context window of ±n tokens around\\n[26]. Therefore mathematically, the jth element of the\\ni has an NPMI as represented in\\n\\ni of word\\n\\nj.\\n\\ncontext vector\\nequation 9.\\n\\nvij = NPMI (cid:0)wi, wj\\n\\n(cid:1) =\\n\\n\\uf8eb\\n\\n\\uf8ec\\n\\uf8ed\\n\\nlog\\n\\nP(wi,wj)+ϵ\\nP(wi)·P(wj)\\n\\n− log (cid:0)P (cid:0)wi, wj\\n\\n(cid:1) + ϵ(cid:1)\\n\\n\\uf8f6\\n\\n\\uf8f7\\n\\uf8f8 (9)\\n\\nThe NPMI metric measures how different in meaning the\\ndiscovered topics are. A higher score in diversity reduces\\nthe redundancy among the topics. A low score suggests that\\nthe topics are repetitive and the model’s ability to abstract the\\nthemes in the corpus is low [28].\\n\\nIt’s worth noting that the number of topics chosen for\\nthe model can impact topic diversity. Choosing too many\\ntopics can lead to similar topics with overlapping top words\\nwhile choosing too few can result\\nin broad topics that\\nare difficult to interpret. The most common measure for\\ntopic diversity is the ratio of unique words among the top\\n25 words [29].\\n\\nIV. EXPERIMENTAL SETUP\\nFor the experiment, we used the 20-group news and the BBC\\nnews datasets as benchmark datasets. Both these datasets\\n\\nVOLUME 11, 2023\\n\\n26557\\n\\n\\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models\\n\\nTABLE 1. Aggregate performance of topic models by dataset.\\n\\nFIGURE 3. Experiment workflow.\\n\\nwere available in the preprocessed form in OCTIS — an\\nopen-source framework for training, evaluating, and com-\\nparing Topic models. The Economic Times corpus was\\nour custom dataset. We used pretrained transformers for\\nextracting sentence embeddings. Specifically, we used the\\n’all- mpnet-base-v2, which provides higher and consistent\\nperformance [20]. The generic workflow of the experiment\\nis represented in Fig 3. For evaluating and configuring\\nhyperparameters, we used OCTIS. OCTIS provides a unified\\nplatform with several topic models, datasets, and evaluation\\nmetrics, enabling easy comparison of topic model perfor-\\nmance. It adopts a dataset-model-metric approach to pro-\\nvide optimal hyperparameters using a Bayesian Optimization\\nstrategy [21]. The OCTIS framework is represented in Fig 4.\\nFor the experiment we have chosen topic coherence and topic\\ndiversity measures as defined in the earlier section. Topic\\n\\nFIGURE 4. OCTIS workflow.\\n\\ncoherence metric is based on NPMI and topic diversity is\\nbased on unique words among top 10 words. The metrics are\\ncommon for all the three models and is available as part of\\nthe OCTIS library.\\n\\nA. DATA\\nWe have used three datasets in the articles - The primary\\ndataset actively curated by researchers and two other stan-\\ndard news datasets for benchmarking the performance of the\\nvarious topic models.\\n\\n1) THE ECONOMIC TIMES NEWS CORPUS\\nThe experiment dataset for the article comes from the dig-\\nital archives of the Economic Times, which is an estab-\\nlished English business daily in India. We curated the\\ndataset from news articles between 2008 and 2021. The\\nnews articles were extracted based on the keyword search\\n‘climate change’. After removing shot news stubs and news\\nitems with videos, the final dataset had 9774 documents. The\\ntotal number of terms in the corpus approximates 5 million.\\n\\n2) BENCHMARK DATASETS\\nWe used the 20 newsgroups and the B.B.C. news dataset\\nas benchmark datasets for evaluating and comparing the\\n\\n26558\\n\\nVOLUME 11, 2023\\n\\n\\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models\\n\\nFIGURE 5. Panel of three figures. Figures represent the NPMI score for across values of k.\\n\\nVOLUME 11, 2023\\n\\n26559\\n\\n\\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models\\n\\nFIGURE 5. (Continued.) Panel of three figures. Figures represent the NPMI score for across values of k.\\n\\nperformance of topic modes. The 20 newsgroups dataset\\nis popularly used in NLP experiments as a benchmarking\\ndataset consisting of 20000 news articles equally distributed\\nacross 20 categories. The categories include technology, pol-\\nitics, religion, auto, sports, etc. Similarly, the B.B.C. news\\ndataset has 2225 news articles reported between 2004-2005\\nacross five topical areas. The categories include Business,\\nSports, Technology, Entertainment, and Politics.\\n\\n3) RESULTS\\nEvery topic model chosen for the experiment was run on each\\ndataset three times. In each run was defined by the choice of a\\nrandom seed. In every run we selected five OCTIS values for\\nk in multiples of 10 until 50. In other words, each topic model\\nwas run 15 times on a dataset. The performance of the model\\ncan be analyzed in two ways. First, both NPMI. and topic\\ndiversity can be aggregated at a dataset level. We average the\\nNPMI and topic diversity scores over the three iterations and\\nsummarize it at a dataset level as shown in Table 1. BERTopic\\nperforms better than LDA and NMF across all three datasets\\nand on both the evaluation metrics. Between LDA and NMF,\\nwe also see that LDA performs better than NMF in terms of\\ntopic diversity, while NMF does better in terms of coherence.\\nSecond, we drill down into the performance of the three mod-\\nels at a dataset level where we calculate NPMI. with respect\\nto k. Figure 5 is a panel of three graphs that reports the NPMI.\\nperformance. From the display, we observe that BERtopic\\noutperforms the two other models across all three datasets.\\nWe find that NPMI. reached its maximum for different values\\n\\nof k for each of the dataset. In the case of the Economic\\nTimes dataset, we find that NPMI. value increases steadily till\\nk = 30, declines and increases again until it reaches its high\\nat k = 50. For the other two datasets, maximum NPMI.\\nis achieved at k = 20 and k = 40, respectively.\\n\\nBesides coherence, the experiment also yielded topic diver-\\nsity scores for each model. The results are displayed in\\nFigure 6. Once again, we observe that BERTopic performs\\nbetter than the other models for the experiment and bench-\\nmark datasets. For the Economic Times corpus we see that\\ntopic diversity is maximized at k = 20 and remains stable till\\nk = 50. For the benchmark datasets on the other hand, we see\\nthat topic diversity declines continuously for higher values\\nof k. Based on the results for the Economic Times dataset, we\\nset the k value at 50, where we achieved the highest NPMI.\\nand highest topic diversity as well.\\n\\nV. MAPPING THEMES USING BERTOPIC\\nBased on the model selection experiment in the last section,\\nwe chose BERTopic as the most suitable among all the three\\nmodels. We then applied it to the E.T. corpus with k = 50.\\nOne of the advantages of BERTopic is that it generates topic\\nlabels along with the topics. The labels are generated as a\\ncombination of the topic number and dominant keywords.\\nA partial list is provided below\\n\\n• 1_water_said_pollution_air\\n• 2_countries_climate_developing_agreement\\n• 4_energy_climate_india_coal\\n• 5_ice_climate_said_warming\\n\\n26560\\n\\nVOLUME 11, 2023\\n\\n\\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models\\n\\nFIGURE 6. Panel of three figures. Figures represent the diversity score for across values of k.\\n\\nVOLUME 11, 2023\\n\\n26561\\n\\n\\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models\\n\\nFIGURE 6. (Continued.) Panel of three figures. Figures represent the diversity score for across values of k.\\n\\n• 6_climate_emissions_countries_change’\\n• 8_energy_solar_renewable_power\\n• 12_monsoon_rainfall_climate_said\\n• 15_climate_trump_paris_said\\n• 17_species_tiger_conservation_wildlife\\n• 39_energy_india_renewable_clean\\n• 47_adani_mine_queensland_coal\\n• 49_Yoga_Resolution_day\\n\\nWhile topics might be coherent semantically, not all of them\\nmay be directly relevant to climate change. For example,\\nin topic 49 in the above-covered news about yoga day events,\\nspeakers also mentioned climate change. Alternatively, top-\\nics can be merged to represent a larger theme, such as in\\nthe case of topics 2 & 6. Both topics relate to climate\\nchange negotiations with obvious differences in their point\\nof view. Topic 2 covers news articles with a ‘developing\\ncountry’ perspective, while topic six focuses on news articles\\nwhich discuss country-specific emissions within the summit\\ncontext.\\n\\nTo further identify opportunities for merging similar top-\\nics we performed the following steps. First we examined\\nkeywords within each topic to identify topics that can be\\nmerged or need to be discarded. Second, we manually\\nsampled the full articles within each topic to infer the\\ncontext. Finally, we investigated the relationship between\\ntopics through clustering based on the cosine distance\\nbetween topic embeddings. The extracted themes, and their\\n\\nrelationship is presented in Fig 7. At the end of the process\\nwe were able to identify topics that were peripheral to climate\\nchange and discarded them. The other topics, we grouped\\nthem into eight overarching themes. The themes were ‘Cli-\\nmate cooperation’ and ‘Climate agreements & ’Energy &\\nEmissions’, ‘clean energy’, Resource management, Country\\nemissions and Business engagement ‘country emissions.’\\n‘Climate cooperation’ combines topics 2,13,21,27,35,42,\\n43,46, represents all news articles on bilateral and mul-\\ntrade relationships, and regional summits\\ntilateral visits,\\n(ex: SAARC, Quad), etc., where climate change is on the\\nagenda. ’Resource management and ecosystems’ consists of\\nnews articles about natural resources management, including\\nair, land, water, wildlife, food, etc. News stories often discuss\\nthe impact of climate change on natural resources. The theme\\ngroups topics 4,9,17,33 and ranks second in terms of overall\\ncontribution to the corpus. ‘Climate agreements & negoti-\\nations’ contains news articles that discuss COP summits,\\nprimarily including Paris, Glasgow, Copenhagen, and Lima.\\nFollowing this we have two themes on energy’ Energy &\\nEmissions’ and ’clean energy. While they cover energy at\\nlarge, the key distinctions come from the focus of the news\\nstories. The ‘Energy & emissions’ topic consists of articles\\nthat report the role of energy in emissions and emission reduc-\\ntion. On the other hand ‘clean energy’ exclusively focuses\\non domestic action in improving the share of renewables\\nin electricity regeneration. On a similar note, we find the\\n\\n26562\\n\\nVOLUME 11, 2023\\n\\n\\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models\\n\\nFIGURE 7. Hierarchial topic cluster of the corpus.\\n\\n‘country emissions’ focuses on new stories that present\\nnational emission targets and goals, and performance of dif-\\nferent countries. Specifically, several stories discuss India and\\nChina and their role as prominent contributors in terms of\\nabsolute emissions.\\n\\nFinally, we have the ‘‘Business and climate’’ themes,\\nwhich subsume three topic categories- 12,22 and 49. The\\nnews stories are a collection of company agendas for reducing\\nclimate footprints, improving sustainability practices, CEO.\\nspeeches on climate change, impact on the stock, product\\nmarkets, and new emerging industries such as Electric Vehi-\\ncles & Batteries. We further track the aggregated themes\\nover 2008 -2020, as shown in Fig 8. It is interesting to\\nnote that news across all categories spiked sharply in 2015\\n\\nand 2021. These spikes may be attributed to the greater atten-\\ntion given to climate change around the Paris COP and the\\nGlasgow COP.\\n\\nThere are also interesting insights that emerge from how\\ndifferent themes trend. Before 2014 news on climate sum-\\nmits and negotiations dominated other categories. Post 2014,\\nwe see that news on climate cooperation has the highest\\ncontribution to the overall corpus. In tandem with climate\\ncooperation, we also observe the increase in reporting on\\nthe impact of climate change on resources and ecosystems.\\nWe also observe that both the categories of energy news\\n(emissions & clean energy) move together. Finally we, see\\nthat news around business themes has gained traction since\\n2012 but has increased significantly from 2017 onwards.\\n\\nVOLUME 11, 2023\\n\\n26563\\n\\n\\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models\\n\\nFIGURE 8. Trends in climate change news within themes.\\n\\nVI. DISCUSSION AND CONCLUSION\\nThe scientific consensus on climate change is unassail-\\nable and there is an urgent need for stronger climate\\naction globally. However, countries continue to grapple with\\nsocio-political challenges which have emerged as a deterrent\\nto effective climate action. The social and political discourse\\naround climate change involves the public, the state, and the\\nscientific community, with media as the common platform.\\nUnderstanding how media reports on climate change gives a\\nglimpse into this diverse stakeholder system’s perspectives,\\ndebates, and responses.\\n\\nHowever, the tools used to understand the climate change\\ndiscourse have been derived mainly from traditional quan-\\ntitative and qualitative research. Natural Language based\\ntechniques can augment the existing toolbox to unfold and\\nextract insights from the climate change debate. This study is\\na step towards achieving integration of NLP in social science\\nresearch and, more specifically, within the domain of climate\\nchange discourse [17], [30]. Another important contribution\\nof the study is the demonstration and use of embeddings\\nbased topic models BERTopic as an alternative to LDA and\\nNMF. Additionally, it also furthers the use of experimental\\nframeworks such as OCTIS for evaluating the performance\\nof topic models. Finally, through the longitudinal analysis\\nof climate news the study provides a developing country\\nperspective to the literature on climate change discourse\\n\\nThe analysis in our study includes two components. First\\nan experimental study on topic models to determine relative\\nperformance.Second is the actual mapping of the climate\\nnews based on the model selected in the experiment Our\\nexperiments show that embedding models perform signif-\\nicantly better than the other topic models. The results are\\nconsistent with other recent comparative studies [31], [32].\\n\\nWe further demonstrate the application of BERTopic by\\ntraining it on the Economic Times corpus and discovering\\nnews frames and their evolution. Results suggest that news\\nframes around climate negotiation have been dominant before\\n2014 while climate cooperation gains importance post 2014.\\nWe also found that climate frames related to domestic action\\nalso has gained traction in recent years.\\n\\nStakeholder consensus is an essential aspect of climate\\naction. Given the multiple stakeholders in climate change\\ndiscourse, media analysis can play a crucial role in gauging\\nthe engagement of citizens, the state, and the commercial\\nsector. Using NLP techniques in this context can scale up\\ninformation processing and augment insights derived from\\nother research methods, thus helping make informed policy\\ndecisions for countering climate change.\\n\\nREFERENCES\\n\\n[1] D. Eckstein, Global Climate Risk Index 2020. Bonn, Germany: German-\\n\\nwatch, 2019.\\n\\n[2] T. R. Keller, V. Hase, J. Thaker, D. Mahl, and M. S. Schäfer, ‘‘News media\\ncoverage of climate change in India 1997–2016: Using automated content\\nanalysis to assess themes and topics,’’ Environ. Commun., vol. 14, no. 2,\\npp. 219–235, Feb. 2020.\\n\\n[3] J. Bohr, ‘‘Reporting on climate change: A computational analysis of U.S.\\nnewspapers and sources of bias, 1997–2017,’’ Global Environ. Change,\\nvol. 61, Mar. 2020, Art. no. 102038.\\n\\n[4] L. L. Benites-Lazaro, L. Giatti, and A. Giarolla, ‘‘Topic modeling method\\nfor analyzing social actor discourses on climate change, energy and food\\nsecurity,’’ Energy Res. Social Sci., vol. 45, pp. 318–330, Nov. 2018.\\n[5] S. Umamaheswaran, V. Dar, and J. Thaker, ‘‘The evolution of climate\\nchange reporting in business media: Longitudinal analysis of a business\\nnewspaper,’’ Sustainability, vol. 14, no. 22, Nov. 2022, Art. no. 15214.\\n[6] J. Foulds, ‘‘Mixed membership word embeddings for computational social\\n\\nscience,’’ in Proc. Int. Conf. Artif. Intell. Statist., 2018, pp. 86–95.\\n\\n[7] D. Hovy and S. L. Spruit, ‘‘The social impact of natural language pro-\\ncessing,’’ in Proc. 54th Annu. Meeting Assoc. Comput. Linguistics (Short\\nPapers), vol. 2, 2016, pp. 591–598.\\n\\n26564\\n\\nVOLUME 11, 2023\\n\\n\\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models\\n\\n[8] S. Mohammad, S. Kiritchenko, P. Sobhani, X. Zhu, and C. Cherry,\\n‘‘SemEval-2016 task 6: Detecting stance in tweets,’’ in Proc. 10th Int.\\nWorkshop Semantic Eval. (SemEval-), 2016, pp. 31–41.\\n\\n[9] K. Johnson, I.-T. Lee, and D. Goldwasser, ‘‘Ideological phrase indicators\\nfor classification of political discourse framing on Twitter,’’ in Proc. 2nd\\nWorkshop NLP Comput. Social Sci., 2017, pp. 90–99.\\n\\n[10] S. M. Mohammad, X. Zhu, S. Kiritchenko, and J. Martin, ‘‘Sentiment,\\nemotion, purpose, and style in electoral tweets,’’ Inf. Process. Manage.,\\nvol. 51, no. 4, pp. 480–499, 2015.\\n\\n[11] R. Kashyap and A. Nahapetian, ‘‘Tweet analysis for user health mon-\\nitoring,’’ in Proc. 4th Int. Conf. Wireless Mobile Commun. Healthcare\\nTransforming Healthcare Through Innov. Mobile Wireless Technolog.,\\nNov. 2014, pp. 348–351.\\n\\n[12] P. Lavanya and E. Sasikala, ‘‘Deep learning techniques on text classifica-\\ntion using natural language processing (NLP) in social healthcare network:\\nA comprehensive survey,’’ in Proc. 3rd Int. Conf. Signal Process. Commun.\\n(ICPSC), May 2021, pp. 603–609.\\n\\n[13] A. Farzindar and D. Inkpen, ‘‘Natural language processing for social\\nmedia,’’ Synth. Lect. Hum. Lang. Technol., vol. 8, no. 2, pp. 1–166, 2015.\\n[14] L. Hagen, O. Uzuner, C. Kotfila, T. M. Harrison, and D. Lamanna, ‘‘Under-\\nstanding Citizens’ direct policy suggestions to the federal government: A\\nnatural language processing and topic modeling approach,’’ in Proc. 48th\\nHawaii Int. Conf. Syst. Sci., Jan. 2015, pp. 2134–2143.\\n\\n[15] D. Elgesem and L. N. Steskal Diakopoulos, ‘‘Structure and content of\\nthe discourse on climate change in the blogosphere: The big picture,’’\\nin Climate Change Communication and the Internet. Evanston, IL, USA:\\nRoutledge, 2019, pp. 21–40.\\n\\n[16] H. Elfardy and M. Diab, ‘‘CU-GWU perspective at SemEval-2016 task 6:\\nIdeological stance detection in informal text,’’ in Proc. 10th Int. Workshop\\nSemantic Eval. (SemEval-), 2016, pp. 434–439.\\n\\n[17] P. Swarnakar and A. Modi, ‘‘NLP for climate policy: Creating a\\nknowledge platform for holistic and effective climate action,’’ 2021,\\narXiv:2105.05621.\\n\\n[18] B. Nerlich, R. Forsyth, and D. Clarke, ‘‘Climate in the news: How dif-\\nferences in media discourse between the U.S. and U.K. reflect national\\npriorities,’’ Environ. Commun., vol. 6, no. 1, pp. 44–63, Mar. 2012.\\n[19] P. DiMaggio, M. Nag, and D. Blei, ‘‘Exploiting affinities between topic\\nmodeling and the sociological perspective on culture: Application to news-\\npaper coverage of U.S. government arts funding,’’ Poetics, vol. 41, no. 6,\\npp. 570–606, Dec. 2013.\\n\\n[20] Y. Bengio, R. Ducharme, and P. Vincent, ‘‘A neural probabilistic lan-\\nguage model,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 13, 2000,\\npp. 1137–1155.\\n\\n[21] A. B. Dieng, F. J. R. Ruiz, and D. M. Blei, ‘‘Topic modeling in embed-\\nding spaces,’’ Trans. Assoc. Comput. Linguistics, vol. 8, pp. 439–453,\\nDec. 2020.\\n\\n[22] P. Ghasiya and K. Okamura, ‘‘Investigating COVID-19 news across four\\nnations: A topic modeling and sentiment analysis approach,’’ IEEE Access,\\nvol. 9, pp. 36645–36656, 2021.\\n\\n[23] J.-R. Lin, Z.-Z. Hu, J.-L. Li, and L.-M. Chen, ‘‘Understanding on-site\\ninspection of construction projects based on keyword extraction and topic\\nmodeling,’’ IEEE Access, vol. 8, pp. 198503–198517, 2020.\\n\\n[24] Y. Chen, H. Zhang, R. Liu, Z. Ye, and J. Lin, ‘‘Experimental explorations\\non short text topic mining between LDA and NMF based schemes,’’\\nKnowl.-Based Syst., vol. 163, pp. 1–13, Jan. 2019.\\n\\n[25] D. Lee and H. S. Seung, ‘‘Algorithms for non-negative matrix factoriza-\\ntion,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 13, 2000, pp. 1–7.\\n[26] M. Röder, A. Both, and A. Hinneburg, ‘‘Exploring the space of topic\\ncoherence measures,’’ in Proc. 8th ACM Int. Conf. Web Search Data\\nMining, Feb. 2015, pp. 399–408.\\n\\n[27] D. Mimno, ‘‘Optimizing semantic coherence in topic models,’’ in Proc.\\nConf. Empirical Methods Natural Language Process., 2011, pp. 1–11.\\n[28] A. Abdelrazek, Y. Eid, E. Gawish, W. Medhat, and A. Hassan, ‘‘Topic\\nmodeling algorithms and applications: A survey,’’ Inf. Syst., vol. 112,\\nFeb. 2023, Art. no. 102131.\\n\\n[29] S. Limwattana and S. Prom-On, ‘‘Topic modeling enhancement using\\nword embeddings,’’ in Proc. 18th Int. Joint Conf. Comput. Sci. Softw. Eng.\\n(JCSSE), Jun. 2021, pp. 1–5.\\n\\n[30] M. Stede and R. Patz, ‘‘The climate change debate and natural language\\nprocessing,’’ in Proc. 1st Workshop NLP Positive Impact, Aug. 2021,\\npp. 8–18.\\n\\n[31] M. J. Sánchez-Franco and M. Rey-Moreno, ‘‘Do travelers’ reviews depend\\non the destination? An analysis in coastal and urban peer-to-peer lodg-\\nings,’’ Psychol. Marketing, vol. 39, no. 2, pp. 441–459, 2022.\\n\\n[32] R. Egger and J. Yu, ‘‘A topic modeling comparison between LDA, NMF,\\nTop2 Vec, and BERTopic to demystify Twitter posts,’’ Frontiers Sociol.,\\nvol. 7, May 2022, Art. no. 886498.\\n\\nSWARNALAKSHMI UMAMAHESWARAN rec-\\neived the Ph.D. degree in public policy from the\\nTERI School of Advanced Studies, New Delhi.\\nHer doctoral work was on policy analysis for\\nlow-carbon growth development in the Indian con-\\ntext. Prior to her doctoral studies, she was an\\nanalytics consultant across several MNCs in the\\nbanking and marketing sector. As part of her cor-\\nporate career, she has built credit scoring and\\nrisk-profiling models using machine learning and\\ndata science tools. She is an economist by education and a data scientist\\nby practice. She is currently a Business Analytics Faculty Member with\\nthe Symbiosis Institute of Business Management, Bengaluru. Her current\\nresearch interest includes computational social sciences, with a special focus\\non climate and sustainability.\\n\\nVANDITA DAR received the Ph.D. degree in eco-\\nnomics from Mumbai University. She is a pas-\\nsionate economist and a researcher working as a\\nFaculty Member with the Symbiosis Institute of\\nBusiness Management, Bengaluru. She has diverse\\nwork experience in academics, research, and cor-\\nporate for more than 20 years. In her last corporate\\nassignment, she held the profile of an economist\\nwith a global financial services company. She has\\nconducted capacity-building workshops for senior\\ngovernment officers spanning the areas of macroeconomic policy and busi-\\nness environment, and was the head of the training division with an apex state\\ntraining institute. Her research interests include sustainability, climate issues,\\nmacroeconomic policy, and development economics. She is a reviewer of\\nreputed international journals.\\n\\nELIZA SHARMA received the Ph.D. degree in\\nmanagement and finance from the Jaypee Institute\\nof Information Technology, Noida. She was with\\nthe Indian Institute of Management, Ahmedabad,\\nIndia, as a Research Assistant for the Government\\nof India project to develop the Integrity Index\\nfor public sector organizations. She is currently\\na Finance Faculty Member with the Symbiosis\\nInstitute of Business Management, Bengaluru. She\\nhas published many research papers in national\\nand international journals and conference proceedings. Her research interest\\nincludes exploring corporate social responsibility practices in the industry.\\n\\nJIKKU SUSAN KURIAN received the Ph.D.\\ndegree in the core area of organizational behav-\\nior. She has 17 years of experience, of which\\neight years were with leading corporates and the\\nrest of seven years with KL University Business\\nSchool, Vijayawada. As a faculty member, she\\nhandles organizational behavior and HR-related\\nsubjects. She is currently an Assistant Professor\\nwith the Symbiosis Institute of Business Manage-\\nment, Bengaluru (SIBM-B). Her current research\\ninterests include researching sustainable HRM and technological transitions.\\n\\nVOLUME 11, 2023\\n\\n26565\\n\\n\\x0c'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text=df.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Received 7 February 2023, accepted 2 March 2023, date of publication 13 March 2023, date of current version 21 March 2023.  Digital Object Identifier 10.1109/ACCESS.2023.3256530  Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  SWARNALAKSHMI UMAMAHESWARAN, VANDITA DAR , ELIZA SHARMA , AND JIKKU SUSAN KURIAN Symbiosis Institute of Business Management, Symbiosis International (Deemed University), Bengaluru 560100, India  Corresponding author: Vandita Dar (vandita.dar@sibm.edu.in)  ABSTRACT India and other developing economies are receiving more attention in the context of climate change due to their rapid rates of economic expansion and large populations. In terms of absolute emissions, India surpassed China and the U.S. in 2018 to become the third-largest emitter. Solving this wicked problem calls for climate action across the stakeholder spectrum involving governments, business communities, and citizens. While extant literature has focused significantly on the role of governments and individual perceptions, the business sector needs to be more represented. In this study, we consider business news media as a platform that reflects the industry engagement in climate change and as a source of information on climate change for business decision-makers. Hence, understanding the topic and themes in the nexus of climate and business is important to evaluate the business sector’s stance towards climate change and how it has evolved. This work explores business news related to climate change using natural language techniques. We first experiment with three topic-modeling techniques, such as LDA, NMF, and BERTopic, on the business news and two more benchmark news datasets. Our test data is derived from digital news archives of ’The Economic Times – India’s leading business news daily. We evaluate the performance based on quantitative metrics commonly used for topic models. We choose the algorithm that provides the highest precision for climate-specific information represented by the test dataset. We then apply the algorithm with the best performance, as evaluated by the experiment, to a large corpus of Indian climate news from E.T. spanning from 2008 -2021. We present how different themes, including industry engagement, evolved over the last two decades. The results suggest that climate cooperation has the highest contribution in the corpus, with other themes on resource management, energy and business gaining traction in recent years.  INDEX TERMS Climate change, media, topic models, NLP, computational social sciences, experiment.  I. INTRODUCTION Climate change is the most critical issue of this millennium. India has high stakes in the global debate since it’s the third largest emitter and also high on climate vulnerability [1]. In its most recent update to the Paris pledge, the country has further committed to reducing its emission intensity to 35% of 2005 levels by 2030. Emission reduction of this scale requires solid public support across the stakeholder spectrum, includ- ing citizens and enterprises. The role of the commercial sector is particularly relevant given its contribution to country-level emissions.  The associate editor coordinating the review of this manuscript and  approving it for publication was Arianna Dulizia  .  A practical approach toward understanding how different stakeholders engage in climate change can be through the analysis of news coverage. While all news media reflect contemporary public discourse, business newspapers tra- ditionally report news that is more relevant for business decision-makers. Therefore, mapping the topics pertinent to climate change in business media and how such issues have evolved can help us understand how the commercial sector engages with the issue of climate change.  Automated content analysis of large corpora is an estab- lished technique in the social sciences. Specifically, the use of topic models for extracting latent topics from newspapers, discussion forums, and other social media content has been gaining traction in extant literature. Among the different  26554  This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/  VOLUME 11, 2023  \\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  Natural Language Processing (NLP) techniques, topic mod- eling is a machine-learning task that groups documents and words with similar meanings. All topic models yield topics, each ranked based on the specific terms and their relevance to the topic. The topic emerges automatically without prior annotation or labeling of the raw data. In this sense, topic models form an important category of unsupervised tech- niques within the larger field of Natural Language Process- ing. The are many mathematical approaches to topic models and an even higher spread of algorithms that learn the param- eters of such model approaches. Conventional topic models range from linear algebraic-based techniques such as LSA to more popular probabilistic models like LDA and PLSA. Metrics decomposition-based methods such as NMF have also been a mainstay of NLP applications. The underlying Bag Of Words (BOW) approach is a distinguishing factor among all these approaches. However, the BoW-based topic model is based on the unrealistic assumption that words are independent of each other and relies on frequencies of word occurrences. A more recent development in NLP, is the representation of text using embeddings. Embeddings in the context of NLP are dense representations of units of the text in the vector space in a way that the semantic similarity between the units of text is captured. Although automated content analysis is gaining importance within the climate discourse literature, there is an overreliance on probabilistic topic models [2], [3], [4], [5]. More importantly, the choice of topic models such as LDA is motivated by popular use and is not backed by experiments to evaluate performance metrics especially in the social sciences [6]. Given this background, the objective of this article is to evaluate the performance of conventional BOW-based models with embeddings-based models. Specifically, we run experiments using LDA, NMF, and BERTopic. We choose the most appropriate model based on the evaluation metrics and use it to map themes within the Economic Times corpus.  The article is organized as follows. Section II introduces the literature on automated content analysis of climate news and discusses how topic models are used in the literature. Section III presents the data and methods. Section IV offers the details of the experiment and its results. Section V presents the results of the final implantation of the chosen model.  II. LITERATURE REVIEW The adoption of NLP techniques in social sciences has seen significant growth in the recent decade. Language is a social construct and can be considered a proxy for behav- ior [7]. As expressed by people, language may contain rich latent information that can help identify several dimen- sions and traits of behavior. Therefore, the rapid growth of NLP methods within social sciences is not surprising. The social science applications of NLP range from understand- ing political affiliations and voter intentions [8], [9], [10], health care delivery [11], [12], media monitoring [13], and  improving public policy implementation [14]. The use of NLP in mining social discourse around climate change is still nascent and broadly categorized based on the analy- sis of social media platforms, online news, and scientific or technical documents. Several studies analyze societal stance on climate change through the study of Twitter and microblogs. For example, Elgasem et al. combine sentiment and networking analysis to identify climate skeptics and accept communities [15]. Stance detection in climate change tweets emerged as a separate subdomain with the release of the SemEval dataset, which defined detecting climate concerns as a task [8]. Several researchers have employed advanced NLP and machine learning techniques for improv- ing stance detection in climate tweets with the Semeval dataset [16].  While content analysis of climate news is an established norm, the use of NLP in analyzing climate news is relatively new. Compared with the studies on social media, news analy- sis has received less attention [17]. Keller et al. in [2] applied LDA to ∼18000 climate change articles sourced from two Indian dailies. They extracted 29 themes ranging from cli- mate change impact and politics to climate science. Similarly, Bohr [3] segments 52 US newspapers based on geography, partisan bias, circulation, etc. He then uses structural topic models to discover topics and further analyses the influence of these attributes on the topics [3]. Benites-Lazaro et al. [4] use a corpus of news articles between 2007 -2017 to analyze the climate, food, and energy nexus using topic models. From a methodological perspective, most authors have used topic models, especially the popular Latent Dirichlet Allocation (LDA) [2], [3], [4]. Despite the popularity of LDA, it suf- fers from certain shortcomings. Notably, the foundational assumption of LDA is that documents are an unordered set, i.e., the words that appear in the text are independent of each other [4]. This assumption is common to all models based on the Bag of Words text representation. This assumption conflicts with linguistic theory, which suggests that words in any human language are always connected and sequen- tial [18]. Another aspect of LDA is that it needs the number of topics extracted as input right at the beginning. Know- ing the optimal number of issues at the beginning is only feasible in some contexts and hence introduces an element of subjectivity [17]. Finally, labeling the extracted topics is also left to the users’ discretion and can vary considerably between coders. Therefore for credible topic extraction as well as validation, additional information from subject matter experts becomes crucial [19]. In this sense, LDA models do not lend to automation. While some of the shortcomings, such as having to specify a specific number of topics, can be countered using matrix factorization methods such as NMF, the underlying issues related to BoW representation remain the same. The dependency on classical topic models is partic- ularly conspicuous given several algorithmic developments in the domain. For example Topic models based on embeddings have gained traction in recent years. Embeddings are dense text representations such that words with semantic similarity  VOLUME 11, 2023  26555  \\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 1. Diagrammatic representation of LDA.  are closer in a low dimensional space [20]. Word embeddings capture the relationship between words and convey context better than the word count-based BoW representation of text. In general, embeddings-based models have reported supe- rior performance. However, the application of embeddings based topic models in social sciences has been sparse [21]. Another significant gap in social science based applications is the lack of robust experiments for model selection. Stud- ies typically choose a particular model on a apriori basis based on theoretical assumptions or prior literature contribu- tions [14], [22], [23]  III. METHODS A. LATENT DIRICHLET ALLOCATION LDA is the most basic model in probabilistic topic models and is popular in several social science disciplines [5]. LDA typically works on a Bag of Words vector representation of documents of a specific length. The learning algorithm in LDA is unsupervised and is used to discover latent semantic patterns in unstructured documents. The technique assumes that each document is generated by a complex probabilistic generative mechanism.LDA assumes that each document is created one word at a time by selecting a topic from the document’s topic distribution and then picking a word from that topic. Therefore, each document is modeled as a mixture of latent topics and each topic is modeled as a multinomial distribution of words. Thus, the learning in LDA is to estimate the parameters of the underlying mechanism that most likely generated the corpus as precisely as possible. Fig 1 provides the outline of the mathematical model  Were N — Number of words in each document. k  — Number of topics a document belongs to (a fixed number).  — Single document.  D — Corpus, a collection of M documents. d zd,n —topic for the nth word in the dth document. θ — Distribution of topics for each document.  βk — Distribution of words within each topic up  α, η — parameters of prior distributions over θ  to k.  and β.  The distribution of topics θ derives from a Dirichlet distri- bution. The use of Dirichlet allows each document to embed topic sparsity, thus mimicking real-word documents. The generative process then can be defined as the joint probability of the documents and topics as represented in  p(θ, z, w | α, β)  (1)  where α, and β refer to the hyperparameter related to the Dirichlet distribution. The joint distributions are further com- puted as the conditional distribution of the hidden topics given the set of documents. The mathematical formulation is given in Eqn 2.  p(θ, z | w, α, β) =  p(θ, z, w | α, β) p(w | α, β)  (2)  However, computing the above expression can be difficult since the denominator requires the summation of all possible combinations of topics. The use of approximation methods such as Markov Chain Monte Carlo and variational inference is standard in this context.  B. NON-NEGATIVE MATRIX FACTORIZATION NMF is a linear algebra-based multivariate technique based on matrix decompositions of a high dimensional vector space. The decomposed non-negative matrices represent hidden structures considered coordinate axes in a transformed low- dimensional vector space [24]. NMF considers every individ- ual document d as a vector of its terms. We then represent the term-document matrix D as follows  D = [d 1, d 2, . . . , d n] ∈ RMN  (3)  D can be decomposed into low dimensional vector spaces as represented by matrices U & V shown in equation 6. Elements of U and V are non-negative real numbers indicated by the  26556  VOLUME 11, 2023  \\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 2. Diagrammatic representation of NMF.  matrix R.U has dimensions M and K where M denotes topics and K denotes terms or words  distribution for every cluster, thus creating topic representa- tions from clustered embeddings  U = [u1, u2, . . . , uK ] ∈ RMN V = [v1, v2, . . . , vN ] ∈ RKN D ≈ UV  (4) (5) (6)  While V represents K topics and N documents. A more intuitive representation of NMF is given in Fig 2. The quality of the formulation in Equation 4 is measured as the squared difference between the document term matrix D and U.V. This can be formally represented below.  Min ||D − UV∥2  (7)  NMF learns U and V iteratively using multiplicative updates such that the error is minimized [25]  C. BERTopic BERTopic [19] is a combination of several modules. At the top it involves representing text as Embeddings using a pre-trained language model-specifically the Bidirectional Encoder Representations from Transformers(BERT). Such a representation generates a dense matrix where text either in sentences or paragraphs is presented as numeric vectors that capture the semantic similarity. The second layer involves using a dimensionality reduction algorithm such as UMAP. to convert the embeddings into a low-dimensional space. Finally, the reduced embeddings are clustered by HDB- SCAN. or K-means algorithms. At the final layer the topic representations are done such that each cluster is assigned a topic. The topics are then represented using the TF-IDF mea- sure, which combines term frequency and inverse document frequency for rank ordering the importance of words in the text. We define the classical TF-IDF process below  Dt,d = tft,d · log  (cid:19)  (cid:18) N dft  (8)  where  D - Term document matrix Tf - Term frequency Df - Document frequency BERTopic alters the application of the TF-IDF procedure using a cluster of documents rather than an individual doc- ument. This enables the BERTopic to output the topic word  D. EVALUATION METRICS When measuring topic quality, human evaluation of a topic is considered a gold standard. However, it is a resource intensive and expensive strategy to implement. As an alter- native, recent literature has focused on the automatic mea- surement of coherence measures that closely mimic human judgment. Topic coherence generally measures the degree to which the words in a topic are semantically related and make sense together [26] Alternatively, coherence can also be defined as a measure of the internal consistency of a topic and how well it represents the documents it is gen- erated from [27].. For measuring semantic similarity, the most common methods are the Normalized Pointwise Mutual Information (NPMI.). The NPMI. method is based on context vectors j. It is constructed by extracting co-occurrences and counts within a context window of ±n tokens around [26]. Therefore mathematically, the jth element of the i has an NPMI as represented in  i of word  j.  context vector equation 9.  vij = NPMI (cid:0)wi, wj  (cid:1) =  \\uf8eb  \\uf8ec \\uf8ed  log  P(wi,wj)+ϵ P(wi)·P(wj)  − log (cid:0)P (cid:0)wi, wj  (cid:1) + ϵ(cid:1)  \\uf8f6  \\uf8f7 \\uf8f8 (9)  The NPMI metric measures how different in meaning the discovered topics are. A higher score in diversity reduces the redundancy among the topics. A low score suggests that the topics are repetitive and the model’s ability to abstract the themes in the corpus is low [28].  It’s worth noting that the number of topics chosen for the model can impact topic diversity. Choosing too many topics can lead to similar topics with overlapping top words while choosing too few can result in broad topics that are difficult to interpret. The most common measure for topic diversity is the ratio of unique words among the top 25 words [29].  IV. EXPERIMENTAL SETUP For the experiment, we used the 20-group news and the BBC news datasets as benchmark datasets. Both these datasets  VOLUME 11, 2023  26557  \\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  TABLE 1. Aggregate performance of topic models by dataset.  FIGURE 3. Experiment workflow.  were available in the preprocessed form in OCTIS — an open-source framework for training, evaluating, and com- paring Topic models. The Economic Times corpus was our custom dataset. We used pretrained transformers for extracting sentence embeddings. Specifically, we used the ’all- mpnet-base-v2, which provides higher and consistent performance [20]. The generic workflow of the experiment is represented in Fig 3. For evaluating and configuring hyperparameters, we used OCTIS. OCTIS provides a unified platform with several topic models, datasets, and evaluation metrics, enabling easy comparison of topic model perfor- mance. It adopts a dataset-model-metric approach to pro- vide optimal hyperparameters using a Bayesian Optimization strategy [21]. The OCTIS framework is represented in Fig 4. For the experiment we have chosen topic coherence and topic diversity measures as defined in the earlier section. Topic  FIGURE 4. OCTIS workflow.  coherence metric is based on NPMI and topic diversity is based on unique words among top 10 words. The metrics are common for all the three models and is available as part of the OCTIS library.  A. DATA We have used three datasets in the articles - The primary dataset actively curated by researchers and two other stan- dard news datasets for benchmarking the performance of the various topic models.  1) THE ECONOMIC TIMES NEWS CORPUS The experiment dataset for the article comes from the dig- ital archives of the Economic Times, which is an estab- lished English business daily in India. We curated the dataset from news articles between 2008 and 2021. The news articles were extracted based on the keyword search ‘climate change’. After removing shot news stubs and news items with videos, the final dataset had 9774 documents. The total number of terms in the corpus approximates 5 million.  2) BENCHMARK DATASETS We used the 20 newsgroups and the B.B.C. news dataset as benchmark datasets for evaluating and comparing the  26558  VOLUME 11, 2023  \\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 5. Panel of three figures. Figures represent the NPMI score for across values of k.  VOLUME 11, 2023  26559  \\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 5. (Continued.) Panel of three figures. Figures represent the NPMI score for across values of k.  performance of topic modes. The 20 newsgroups dataset is popularly used in NLP experiments as a benchmarking dataset consisting of 20000 news articles equally distributed across 20 categories. The categories include technology, pol- itics, religion, auto, sports, etc. Similarly, the B.B.C. news dataset has 2225 news articles reported between 2004-2005 across five topical areas. The categories include Business, Sports, Technology, Entertainment, and Politics.  3) RESULTS Every topic model chosen for the experiment was run on each dataset three times. In each run was defined by the choice of a random seed. In every run we selected five OCTIS values for k in multiples of 10 until 50. In other words, each topic model was run 15 times on a dataset. The performance of the model can be analyzed in two ways. First, both NPMI. and topic diversity can be aggregated at a dataset level. We average the NPMI and topic diversity scores over the three iterations and summarize it at a dataset level as shown in Table 1. BERTopic performs better than LDA and NMF across all three datasets and on both the evaluation metrics. Between LDA and NMF, we also see that LDA performs better than NMF in terms of topic diversity, while NMF does better in terms of coherence. Second, we drill down into the performance of the three mod- els at a dataset level where we calculate NPMI. with respect to k. Figure 5 is a panel of three graphs that reports the NPMI. performance. From the display, we observe that BERtopic outperforms the two other models across all three datasets. We find that NPMI. reached its maximum for different values  of k for each of the dataset. In the case of the Economic Times dataset, we find that NPMI. value increases steadily till k = 30, declines and increases again until it reaches its high at k = 50. For the other two datasets, maximum NPMI. is achieved at k = 20 and k = 40, respectively.  Besides coherence, the experiment also yielded topic diver- sity scores for each model. The results are displayed in Figure 6. Once again, we observe that BERTopic performs better than the other models for the experiment and bench- mark datasets. For the Economic Times corpus we see that topic diversity is maximized at k = 20 and remains stable till k = 50. For the benchmark datasets on the other hand, we see that topic diversity declines continuously for higher values of k. Based on the results for the Economic Times dataset, we set the k value at 50, where we achieved the highest NPMI. and highest topic diversity as well.  V. MAPPING THEMES USING BERTOPIC Based on the model selection experiment in the last section, we chose BERTopic as the most suitable among all the three models. We then applied it to the E.T. corpus with k = 50. One of the advantages of BERTopic is that it generates topic labels along with the topics. The labels are generated as a combination of the topic number and dominant keywords. A partial list is provided below  • 1_water_said_pollution_air • 2_countries_climate_developing_agreement • 4_energy_climate_india_coal • 5_ice_climate_said_warming  26560  VOLUME 11, 2023  \\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 6. Panel of three figures. Figures represent the diversity score for across values of k.  VOLUME 11, 2023  26561  \\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 6. (Continued.) Panel of three figures. Figures represent the diversity score for across values of k.  • 6_climate_emissions_countries_change’ • 8_energy_solar_renewable_power • 12_monsoon_rainfall_climate_said • 15_climate_trump_paris_said • 17_species_tiger_conservation_wildlife • 39_energy_india_renewable_clean • 47_adani_mine_queensland_coal • 49_Yoga_Resolution_day  While topics might be coherent semantically, not all of them may be directly relevant to climate change. For example, in topic 49 in the above-covered news about yoga day events, speakers also mentioned climate change. Alternatively, top- ics can be merged to represent a larger theme, such as in the case of topics 2 & 6. Both topics relate to climate change negotiations with obvious differences in their point of view. Topic 2 covers news articles with a ‘developing country’ perspective, while topic six focuses on news articles which discuss country-specific emissions within the summit context.  To further identify opportunities for merging similar top- ics we performed the following steps. First we examined keywords within each topic to identify topics that can be merged or need to be discarded. Second, we manually sampled the full articles within each topic to infer the context. Finally, we investigated the relationship between topics through clustering based on the cosine distance between topic embeddings. The extracted themes, and their  relationship is presented in Fig 7. At the end of the process we were able to identify topics that were peripheral to climate change and discarded them. The other topics, we grouped them into eight overarching themes. The themes were ‘Cli- mate cooperation’ and ‘Climate agreements & ’Energy & Emissions’, ‘clean energy’, Resource management, Country emissions and Business engagement ‘country emissions.’ ‘Climate cooperation’ combines topics 2,13,21,27,35,42, 43,46, represents all news articles on bilateral and mul- trade relationships, and regional summits tilateral visits, (ex: SAARC, Quad), etc., where climate change is on the agenda. ’Resource management and ecosystems’ consists of news articles about natural resources management, including air, land, water, wildlife, food, etc. News stories often discuss the impact of climate change on natural resources. The theme groups topics 4,9,17,33 and ranks second in terms of overall contribution to the corpus. ‘Climate agreements & negoti- ations’ contains news articles that discuss COP summits, primarily including Paris, Glasgow, Copenhagen, and Lima. Following this we have two themes on energy’ Energy & Emissions’ and ’clean energy. While they cover energy at large, the key distinctions come from the focus of the news stories. The ‘Energy & emissions’ topic consists of articles that report the role of energy in emissions and emission reduc- tion. On the other hand ‘clean energy’ exclusively focuses on domestic action in improving the share of renewables in electricity regeneration. On a similar note, we find the  26562  VOLUME 11, 2023  \\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 7. Hierarchial topic cluster of the corpus.  ‘country emissions’ focuses on new stories that present national emission targets and goals, and performance of dif- ferent countries. Specifically, several stories discuss India and China and their role as prominent contributors in terms of absolute emissions.  Finally, we have the ‘‘Business and climate’’ themes, which subsume three topic categories- 12,22 and 49. The news stories are a collection of company agendas for reducing climate footprints, improving sustainability practices, CEO. speeches on climate change, impact on the stock, product markets, and new emerging industries such as Electric Vehi- cles & Batteries. We further track the aggregated themes over 2008 -2020, as shown in Fig 8. It is interesting to note that news across all categories spiked sharply in 2015  and 2021. These spikes may be attributed to the greater atten- tion given to climate change around the Paris COP and the Glasgow COP.  There are also interesting insights that emerge from how different themes trend. Before 2014 news on climate sum- mits and negotiations dominated other categories. Post 2014, we see that news on climate cooperation has the highest contribution to the overall corpus. In tandem with climate cooperation, we also observe the increase in reporting on the impact of climate change on resources and ecosystems. We also observe that both the categories of energy news (emissions & clean energy) move together. Finally we, see that news around business themes has gained traction since 2012 but has increased significantly from 2017 onwards.  VOLUME 11, 2023  26563  \\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 8. Trends in climate change news within themes.  VI. DISCUSSION AND CONCLUSION The scientific consensus on climate change is unassail- able and there is an urgent need for stronger climate action globally. However, countries continue to grapple with socio-political challenges which have emerged as a deterrent to effective climate action. The social and political discourse around climate change involves the public, the state, and the scientific community, with media as the common platform. Understanding how media reports on climate change gives a glimpse into this diverse stakeholder system’s perspectives, debates, and responses.  However, the tools used to understand the climate change discourse have been derived mainly from traditional quan- titative and qualitative research. Natural Language based techniques can augment the existing toolbox to unfold and extract insights from the climate change debate. This study is a step towards achieving integration of NLP in social science research and, more specifically, within the domain of climate change discourse [17], [30]. Another important contribution of the study is the demonstration and use of embeddings based topic models BERTopic as an alternative to LDA and NMF. Additionally, it also furthers the use of experimental frameworks such as OCTIS for evaluating the performance of topic models. Finally, through the longitudinal analysis of climate news the study provides a developing country perspective to the literature on climate change discourse  The analysis in our study includes two components. First an experimental study on topic models to determine relative performance.Second is the actual mapping of the climate news based on the model selected in the experiment Our experiments show that embedding models perform signif- icantly better than the other topic models. The results are consistent with other recent comparative studies [31], [32].  We further demonstrate the application of BERTopic by training it on the Economic Times corpus and discovering news frames and their evolution. Results suggest that news frames around climate negotiation have been dominant before 2014 while climate cooperation gains importance post 2014. We also found that climate frames related to domestic action also has gained traction in recent years.  Stakeholder consensus is an essential aspect of climate action. Given the multiple stakeholders in climate change discourse, media analysis can play a crucial role in gauging the engagement of citizens, the state, and the commercial sector. Using NLP techniques in this context can scale up information processing and augment insights derived from other research methods, thus helping make informed policy decisions for countering climate change.  REFERENCES  [1] D. Eckstein, Global Climate Risk Index 2020. Bonn, Germany: German-  watch, 2019.  [2] T. R. Keller, V. Hase, J. Thaker, D. Mahl, and M. S. Schäfer, ‘‘News media coverage of climate change in India 1997–2016: Using automated content analysis to assess themes and topics,’’ Environ. Commun., vol. 14, no. 2, pp. 219–235, Feb. 2020.  [3] J. Bohr, ‘‘Reporting on climate change: A computational analysis of U.S. newspapers and sources of bias, 1997–2017,’’ Global Environ. Change, vol. 61, Mar. 2020, Art. no. 102038.  [4] L. L. Benites-Lazaro, L. Giatti, and A. Giarolla, ‘‘Topic modeling method for analyzing social actor discourses on climate change, energy and food security,’’ Energy Res. Social Sci., vol. 45, pp. 318–330, Nov. 2018. [5] S. Umamaheswaran, V. Dar, and J. Thaker, ‘‘The evolution of climate change reporting in business media: Longitudinal analysis of a business newspaper,’’ Sustainability, vol. 14, no. 22, Nov. 2022, Art. no. 15214. [6] J. Foulds, ‘‘Mixed membership word embeddings for computational social  science,’’ in Proc. Int. Conf. Artif. Intell. Statist., 2018, pp. 86–95.  [7] D. Hovy and S. L. Spruit, ‘‘The social impact of natural language pro- cessing,’’ in Proc. 54th Annu. Meeting Assoc. Comput. Linguistics (Short Papers), vol. 2, 2016, pp. 591–598.  26564  VOLUME 11, 2023  \\x0cS. Umamaheswaran et al.: Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  [8] S. Mohammad, S. Kiritchenko, P. Sobhani, X. Zhu, and C. Cherry, ‘‘SemEval-2016 task 6: Detecting stance in tweets,’’ in Proc. 10th Int. Workshop Semantic Eval. (SemEval-), 2016, pp. 31–41.  [9] K. Johnson, I.-T. Lee, and D. Goldwasser, ‘‘Ideological phrase indicators for classification of political discourse framing on Twitter,’’ in Proc. 2nd Workshop NLP Comput. Social Sci., 2017, pp. 90–99.  [10] S. M. Mohammad, X. Zhu, S. Kiritchenko, and J. Martin, ‘‘Sentiment, emotion, purpose, and style in electoral tweets,’’ Inf. Process. Manage., vol. 51, no. 4, pp. 480–499, 2015.  [11] R. Kashyap and A. Nahapetian, ‘‘Tweet analysis for user health mon- itoring,’’ in Proc. 4th Int. Conf. Wireless Mobile Commun. Healthcare Transforming Healthcare Through Innov. Mobile Wireless Technolog., Nov. 2014, pp. 348–351.  [12] P. Lavanya and E. Sasikala, ‘‘Deep learning techniques on text classifica- tion using natural language processing (NLP) in social healthcare network: A comprehensive survey,’’ in Proc. 3rd Int. Conf. Signal Process. Commun. (ICPSC), May 2021, pp. 603–609.  [13] A. Farzindar and D. Inkpen, ‘‘Natural language processing for social media,’’ Synth. Lect. Hum. Lang. Technol., vol. 8, no. 2, pp. 1–166, 2015. [14] L. Hagen, O. Uzuner, C. Kotfila, T. M. Harrison, and D. Lamanna, ‘‘Under- standing Citizens’ direct policy suggestions to the federal government: A natural language processing and topic modeling approach,’’ in Proc. 48th Hawaii Int. Conf. Syst. Sci., Jan. 2015, pp. 2134–2143.  [15] D. Elgesem and L. N. Steskal Diakopoulos, ‘‘Structure and content of the discourse on climate change in the blogosphere: The big picture,’’ in Climate Change Communication and the Internet. Evanston, IL, USA: Routledge, 2019, pp. 21–40.  [16] H. Elfardy and M. Diab, ‘‘CU-GWU perspective at SemEval-2016 task 6: Ideological stance detection in informal text,’’ in Proc. 10th Int. Workshop Semantic Eval. (SemEval-), 2016, pp. 434–439.  [17] P. Swarnakar and A. Modi, ‘‘NLP for climate policy: Creating a knowledge platform for holistic and effective climate action,’’ 2021, arXiv:2105.05621.  [18] B. Nerlich, R. Forsyth, and D. Clarke, ‘‘Climate in the news: How dif- ferences in media discourse between the U.S. and U.K. reflect national priorities,’’ Environ. Commun., vol. 6, no. 1, pp. 44–63, Mar. 2012. [19] P. DiMaggio, M. Nag, and D. Blei, ‘‘Exploiting affinities between topic modeling and the sociological perspective on culture: Application to news- paper coverage of U.S. government arts funding,’’ Poetics, vol. 41, no. 6, pp. 570–606, Dec. 2013.  [20] Y. Bengio, R. Ducharme, and P. Vincent, ‘‘A neural probabilistic lan- guage model,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 13, 2000, pp. 1137–1155.  [21] A. B. Dieng, F. J. R. Ruiz, and D. M. Blei, ‘‘Topic modeling in embed- ding spaces,’’ Trans. Assoc. Comput. Linguistics, vol. 8, pp. 439–453, Dec. 2020.  [22] P. Ghasiya and K. Okamura, ‘‘Investigating COVID-19 news across four nations: A topic modeling and sentiment analysis approach,’’ IEEE Access, vol. 9, pp. 36645–36656, 2021.  [23] J.-R. Lin, Z.-Z. Hu, J.-L. Li, and L.-M. Chen, ‘‘Understanding on-site inspection of construction projects based on keyword extraction and topic modeling,’’ IEEE Access, vol. 8, pp. 198503–198517, 2020.  [24] Y. Chen, H. Zhang, R. Liu, Z. Ye, and J. Lin, ‘‘Experimental explorations on short text topic mining between LDA and NMF based schemes,’’ Knowl.-Based Syst., vol. 163, pp. 1–13, Jan. 2019.  [25] D. Lee and H. S. Seung, ‘‘Algorithms for non-negative matrix factoriza- tion,’’ in Proc. Adv. Neural Inf. Process. Syst., vol. 13, 2000, pp. 1–7. [26] M. Röder, A. Both, and A. Hinneburg, ‘‘Exploring the space of topic coherence measures,’’ in Proc. 8th ACM Int. Conf. Web Search Data Mining, Feb. 2015, pp. 399–408.  [27] D. Mimno, ‘‘Optimizing semantic coherence in topic models,’’ in Proc. Conf. Empirical Methods Natural Language Process., 2011, pp. 1–11. [28] A. Abdelrazek, Y. Eid, E. Gawish, W. Medhat, and A. Hassan, ‘‘Topic modeling algorithms and applications: A survey,’’ Inf. Syst., vol. 112, Feb. 2023, Art. no. 102131.  [29] S. Limwattana and S. Prom-On, ‘‘Topic modeling enhancement using word embeddings,’’ in Proc. 18th Int. Joint Conf. Comput. Sci. Softw. Eng. (JCSSE), Jun. 2021, pp. 1–5.  [30] M. Stede and R. Patz, ‘‘The climate change debate and natural language processing,’’ in Proc. 1st Workshop NLP Positive Impact, Aug. 2021, pp. 8–18.  [31] M. J. Sánchez-Franco and M. Rey-Moreno, ‘‘Do travelers’ reviews depend on the destination? An analysis in coastal and urban peer-to-peer lodg- ings,’’ Psychol. Marketing, vol. 39, no. 2, pp. 441–459, 2022.  [32] R. Egger and J. Yu, ‘‘A topic modeling comparison between LDA, NMF, Top2 Vec, and BERTopic to demystify Twitter posts,’’ Frontiers Sociol., vol. 7, May 2022, Art. no. 886498.  SWARNALAKSHMI UMAMAHESWARAN rec- eived the Ph.D. degree in public policy from the TERI School of Advanced Studies, New Delhi. Her doctoral work was on policy analysis for low-carbon growth development in the Indian con- text. Prior to her doctoral studies, she was an analytics consultant across several MNCs in the banking and marketing sector. As part of her cor- porate career, she has built credit scoring and risk-profiling models using machine learning and data science tools. She is an economist by education and a data scientist by practice. She is currently a Business Analytics Faculty Member with the Symbiosis Institute of Business Management, Bengaluru. Her current research interest includes computational social sciences, with a special focus on climate and sustainability.  VANDITA DAR received the Ph.D. degree in eco- nomics from Mumbai University. She is a pas- sionate economist and a researcher working as a Faculty Member with the Symbiosis Institute of Business Management, Bengaluru. She has diverse work experience in academics, research, and cor- porate for more than 20 years. In her last corporate assignment, she held the profile of an economist with a global financial services company. She has conducted capacity-building workshops for senior government officers spanning the areas of macroeconomic policy and busi- ness environment, and was the head of the training division with an apex state training institute. Her research interests include sustainability, climate issues, macroeconomic policy, and development economics. She is a reviewer of reputed international journals.  ELIZA SHARMA received the Ph.D. degree in management and finance from the Jaypee Institute of Information Technology, Noida. She was with the Indian Institute of Management, Ahmedabad, India, as a Research Assistant for the Government of India project to develop the Integrity Index for public sector organizations. She is currently a Finance Faculty Member with the Symbiosis Institute of Business Management, Bengaluru. She has published many research papers in national and international journals and conference proceedings. Her research interest includes exploring corporate social responsibility practices in the industry.  JIKKU SUSAN KURIAN received the Ph.D. degree in the core area of organizational behav- ior. She has 17 years of experience, of which eight years were with leading corporates and the rest of seven years with KL University Business School, Vijayawada. As a faculty member, she handles organizational behavior and HR-related subjects. She is currently an Assistant Professor with the Symbiosis Institute of Business Manage- ment, Bengaluru (SIBM-B). Her current research interests include researching sustainable HRM and technological transitions.  VOLUME 11, 2023  26565  \\x0c'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Received 7 February 2023, accepted 2 March 2023, date of publication 13 March 2023, date of current version 21 March 2023',\n",
       " '  Digital Object Identifier 10',\n",
       " '1109/ACCESS',\n",
       " '2023',\n",
       " '3256530  Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  SWARNALAKSHMI UMAMAHESWARAN, VANDITA DAR , ELIZA SHARMA , AND JIKKU SUSAN KURIAN Symbiosis Institute of Business Management, Symbiosis International (Deemed University), Bengaluru 560100, India  Corresponding author: Vandita Dar (vandita',\n",
       " 'dar@sibm',\n",
       " 'edu',\n",
       " 'in)  ABSTRACT India and other developing economies are receiving more attention in the context of climate change due to their rapid rates of economic expansion and large populations',\n",
       " ' In terms of absolute emissions, India surpassed China and the U',\n",
       " 'S',\n",
       " ' in 2018 to become the third-largest emitter',\n",
       " ' Solving this wicked problem calls for climate action across the stakeholder spectrum involving governments, business communities, and citizens',\n",
       " ' While extant literature has focused significantly on the role of governments and individual perceptions, the business sector needs to be more represented',\n",
       " ' In this study, we consider business news media as a platform that reflects the industry engagement in climate change and as a source of information on climate change for business decision-makers',\n",
       " ' Hence, understanding the topic and themes in the nexus of climate and business is important to evaluate the business sector’s stance towards climate change and how it has evolved',\n",
       " ' This work explores business news related to climate change using natural language techniques',\n",
       " ' We first experiment with three topic-modeling techniques, such as LDA, NMF, and BERTopic, on the business news and two more benchmark news datasets',\n",
       " ' Our test data is derived from digital news archives of ’The Economic Times – India’s leading business news daily',\n",
       " ' We evaluate the performance based on quantitative metrics commonly used for topic models',\n",
       " ' We choose the algorithm that provides the highest precision for climate-specific information represented by the test dataset',\n",
       " ' We then apply the algorithm with the best performance, as evaluated by the experiment, to a large corpus of Indian climate news from E',\n",
       " 'T',\n",
       " ' spanning from 2008 -2021',\n",
       " ' We present how different themes, including industry engagement, evolved over the last two decades',\n",
       " ' The results suggest that climate cooperation has the highest contribution in the corpus, with other themes on resource management, energy and business gaining traction in recent years',\n",
       " '  INDEX TERMS Climate change, media, topic models, NLP, computational social sciences, experiment',\n",
       " '  I',\n",
       " ' INTRODUCTION Climate change is the most critical issue of this millennium',\n",
       " ' India has high stakes in the global debate since it’s the third largest emitter and also high on climate vulnerability [1]',\n",
       " ' In its most recent update to the Paris pledge, the country has further committed to reducing its emission intensity to 35% of 2005 levels by 2030',\n",
       " ' Emission reduction of this scale requires solid public support across the stakeholder spectrum, includ- ing citizens and enterprises',\n",
       " ' The role of the commercial sector is particularly relevant given its contribution to country-level emissions',\n",
       " '  The associate editor coordinating the review of this manuscript and  approving it for publication was Arianna Dulizia  ',\n",
       " '  A practical approach toward understanding how different stakeholders engage in climate change can be through the analysis of news coverage',\n",
       " ' While all news media reflect contemporary public discourse, business newspapers tra- ditionally report news that is more relevant for business decision-makers',\n",
       " ' Therefore, mapping the topics pertinent to climate change in business media and how such issues have evolved can help us understand how the commercial sector engages with the issue of climate change',\n",
       " '  Automated content analysis of large corpora is an estab- lished technique in the social sciences',\n",
       " ' Specifically, the use of topic models for extracting latent topics from newspapers, discussion forums, and other social media content has been gaining traction in extant literature',\n",
       " ' Among the different  26554  This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4',\n",
       " '0 License',\n",
       " ' For more information, see https://creativecommons',\n",
       " 'org/licenses/by-nc-nd/4',\n",
       " '0/  VOLUME 11, 2023  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  Natural Language Processing (NLP) techniques, topic mod- eling is a machine-learning task that groups documents and words with similar meanings',\n",
       " ' All topic models yield topics, each ranked based on the specific terms and their relevance to the topic',\n",
       " ' The topic emerges automatically without prior annotation or labeling of the raw data',\n",
       " ' In this sense, topic models form an important category of unsupervised tech- niques within the larger field of Natural Language Process- ing',\n",
       " ' The are many mathematical approaches to topic models and an even higher spread of algorithms that learn the param- eters of such model approaches',\n",
       " ' Conventional topic models range from linear algebraic-based techniques such as LSA to more popular probabilistic models like LDA and PLSA',\n",
       " ' Metrics decomposition-based methods such as NMF have also been a mainstay of NLP applications',\n",
       " ' The underlying Bag Of Words (BOW) approach is a distinguishing factor among all these approaches',\n",
       " ' However, the BoW-based topic model is based on the unrealistic assumption that words are independent of each other and relies on frequencies of word occurrences',\n",
       " ' A more recent development in NLP, is the representation of text using embeddings',\n",
       " ' Embeddings in the context of NLP are dense representations of units of the text in the vector space in a way that the semantic similarity between the units of text is captured',\n",
       " ' Although automated content analysis is gaining importance within the climate discourse literature, there is an overreliance on probabilistic topic models [2], [3], [4], [5]',\n",
       " ' More importantly, the choice of topic models such as LDA is motivated by popular use and is not backed by experiments to evaluate performance metrics especially in the social sciences [6]',\n",
       " ' Given this background, the objective of this article is to evaluate the performance of conventional BOW-based models with embeddings-based models',\n",
       " ' Specifically, we run experiments using LDA, NMF, and BERTopic',\n",
       " ' We choose the most appropriate model based on the evaluation metrics and use it to map themes within the Economic Times corpus',\n",
       " '  The article is organized as follows',\n",
       " ' Section II introduces the literature on automated content analysis of climate news and discusses how topic models are used in the literature',\n",
       " ' Section III presents the data and methods',\n",
       " ' Section IV offers the details of the experiment and its results',\n",
       " ' Section V presents the results of the final implantation of the chosen model',\n",
       " '  II',\n",
       " ' LITERATURE REVIEW The adoption of NLP techniques in social sciences has seen significant growth in the recent decade',\n",
       " ' Language is a social construct and can be considered a proxy for behav- ior [7]',\n",
       " ' As expressed by people, language may contain rich latent information that can help identify several dimen- sions and traits of behavior',\n",
       " ' Therefore, the rapid growth of NLP methods within social sciences is not surprising',\n",
       " ' The social science applications of NLP range from understand- ing political affiliations and voter intentions [8], [9], [10], health care delivery [11], [12], media monitoring [13], and  improving public policy implementation [14]',\n",
       " ' The use of NLP in mining social discourse around climate change is still nascent and broadly categorized based on the analy- sis of social media platforms, online news, and scientific or technical documents',\n",
       " ' Several studies analyze societal stance on climate change through the study of Twitter and microblogs',\n",
       " ' For example, Elgasem et al',\n",
       " ' combine sentiment and networking analysis to identify climate skeptics and accept communities [15]',\n",
       " ' Stance detection in climate change tweets emerged as a separate subdomain with the release of the SemEval dataset, which defined detecting climate concerns as a task [8]',\n",
       " ' Several researchers have employed advanced NLP and machine learning techniques for improv- ing stance detection in climate tweets with the Semeval dataset [16]',\n",
       " '  While content analysis of climate news is an established norm, the use of NLP in analyzing climate news is relatively new',\n",
       " ' Compared with the studies on social media, news analy- sis has received less attention [17]',\n",
       " ' Keller et al',\n",
       " ' in [2] applied LDA to ∼18000 climate change articles sourced from two Indian dailies',\n",
       " ' They extracted 29 themes ranging from cli- mate change impact and politics to climate science',\n",
       " ' Similarly, Bohr [3] segments 52 US newspapers based on geography, partisan bias, circulation, etc',\n",
       " ' He then uses structural topic models to discover topics and further analyses the influence of these attributes on the topics [3]',\n",
       " ' Benites-Lazaro et al',\n",
       " ' [4] use a corpus of news articles between 2007 -2017 to analyze the climate, food, and energy nexus using topic models',\n",
       " ' From a methodological perspective, most authors have used topic models, especially the popular Latent Dirichlet Allocation (LDA) [2], [3], [4]',\n",
       " ' Despite the popularity of LDA, it suf- fers from certain shortcomings',\n",
       " ' Notably, the foundational assumption of LDA is that documents are an unordered set, i',\n",
       " 'e',\n",
       " ', the words that appear in the text are independent of each other [4]',\n",
       " ' This assumption is common to all models based on the Bag of Words text representation',\n",
       " ' This assumption conflicts with linguistic theory, which suggests that words in any human language are always connected and sequen- tial [18]',\n",
       " ' Another aspect of LDA is that it needs the number of topics extracted as input right at the beginning',\n",
       " ' Know- ing the optimal number of issues at the beginning is only feasible in some contexts and hence introduces an element of subjectivity [17]',\n",
       " ' Finally, labeling the extracted topics is also left to the users’ discretion and can vary considerably between coders',\n",
       " ' Therefore for credible topic extraction as well as validation, additional information from subject matter experts becomes crucial [19]',\n",
       " ' In this sense, LDA models do not lend to automation',\n",
       " ' While some of the shortcomings, such as having to specify a specific number of topics, can be countered using matrix factorization methods such as NMF, the underlying issues related to BoW representation remain the same',\n",
       " ' The dependency on classical topic models is partic- ularly conspicuous given several algorithmic developments in the domain',\n",
       " ' For example Topic models based on embeddings have gained traction in recent years',\n",
       " ' Embeddings are dense text representations such that words with semantic similarity  VOLUME 11, 2023  26555  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 1',\n",
       " ' Diagrammatic representation of LDA',\n",
       " '  are closer in a low dimensional space [20]',\n",
       " ' Word embeddings capture the relationship between words and convey context better than the word count-based BoW representation of text',\n",
       " ' In general, embeddings-based models have reported supe- rior performance',\n",
       " ' However, the application of embeddings based topic models in social sciences has been sparse [21]',\n",
       " ' Another significant gap in social science based applications is the lack of robust experiments for model selection',\n",
       " ' Stud- ies typically choose a particular model on a apriori basis based on theoretical assumptions or prior literature contribu- tions [14], [22], [23]  III',\n",
       " ' METHODS A',\n",
       " ' LATENT DIRICHLET ALLOCATION LDA is the most basic model in probabilistic topic models and is popular in several social science disciplines [5]',\n",
       " ' LDA typically works on a Bag of Words vector representation of documents of a specific length',\n",
       " ' The learning algorithm in LDA is unsupervised and is used to discover latent semantic patterns in unstructured documents',\n",
       " ' The technique assumes that each document is generated by a complex probabilistic generative mechanism',\n",
       " 'LDA assumes that each document is created one word at a time by selecting a topic from the document’s topic distribution and then picking a word from that topic',\n",
       " ' Therefore, each document is modeled as a mixture of latent topics and each topic is modeled as a multinomial distribution of words',\n",
       " ' Thus, the learning in LDA is to estimate the parameters of the underlying mechanism that most likely generated the corpus as precisely as possible',\n",
       " ' Fig 1 provides the outline of the mathematical model  Were N — Number of words in each document',\n",
       " ' k  — Number of topics a document belongs to (a fixed number)',\n",
       " '  — Single document',\n",
       " '  D — Corpus, a collection of M documents',\n",
       " ' d zd,n —topic for the nth word in the dth document',\n",
       " ' θ — Distribution of topics for each document',\n",
       " '  βk — Distribution of words within each topic up  α, η — parameters of prior distributions over θ  to k',\n",
       " '  and β',\n",
       " '  The distribution of topics θ derives from a Dirichlet distri- bution',\n",
       " ' The use of Dirichlet allows each document to embed topic sparsity, thus mimicking real-word documents',\n",
       " ' The generative process then can be defined as the joint probability of the documents and topics as represented in  p(θ, z, w | α, β)  (1)  where α, and β refer to the hyperparameter related to the Dirichlet distribution',\n",
       " ' The joint distributions are further com- puted as the conditional distribution of the hidden topics given the set of documents',\n",
       " ' The mathematical formulation is given in Eqn 2',\n",
       " '  p(θ, z | w, α, β) =  p(θ, z, w | α, β) p(w | α, β)  (2)  However, computing the above expression can be difficult since the denominator requires the summation of all possible combinations of topics',\n",
       " ' The use of approximation methods such as Markov Chain Monte Carlo and variational inference is standard in this context',\n",
       " '  B',\n",
       " ' NON-NEGATIVE MATRIX FACTORIZATION NMF is a linear algebra-based multivariate technique based on matrix decompositions of a high dimensional vector space',\n",
       " ' The decomposed non-negative matrices represent hidden structures considered coordinate axes in a transformed low- dimensional vector space [24]',\n",
       " ' NMF considers every individ- ual document d as a vector of its terms',\n",
       " ' We then represent the term-document matrix D as follows  D = [d 1, d 2, ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' , d n] ∈ RMN  (3)  D can be decomposed into low dimensional vector spaces as represented by matrices U & V shown in equation 6',\n",
       " ' Elements of U and V are non-negative real numbers indicated by the  26556  VOLUME 11, 2023  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 2',\n",
       " ' Diagrammatic representation of NMF',\n",
       " '  matrix R',\n",
       " 'U has dimensions M and K where M denotes topics and K denotes terms or words  distribution for every cluster, thus creating topic representa- tions from clustered embeddings  U = [u1, u2, ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' , uK ] ∈ RMN V = [v1, v2, ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' , vN ] ∈ RKN D ≈ UV  (4) (5) (6)  While V represents K topics and N documents',\n",
       " ' A more intuitive representation of NMF is given in Fig 2',\n",
       " ' The quality of the formulation in Equation 4 is measured as the squared difference between the document term matrix D and U',\n",
       " 'V',\n",
       " ' This can be formally represented below',\n",
       " '  Min ||D − UV∥2  (7)  NMF learns U and V iteratively using multiplicative updates such that the error is minimized [25]  C',\n",
       " ' BERTopic BERTopic [19] is a combination of several modules',\n",
       " ' At the top it involves representing text as Embeddings using a pre-trained language model-specifically the Bidirectional Encoder Representations from Transformers(BERT)',\n",
       " ' Such a representation generates a dense matrix where text either in sentences or paragraphs is presented as numeric vectors that capture the semantic similarity',\n",
       " ' The second layer involves using a dimensionality reduction algorithm such as UMAP',\n",
       " ' to convert the embeddings into a low-dimensional space',\n",
       " ' Finally, the reduced embeddings are clustered by HDB- SCAN',\n",
       " ' or K-means algorithms',\n",
       " ' At the final layer the topic representations are done such that each cluster is assigned a topic',\n",
       " ' The topics are then represented using the TF-IDF mea- sure, which combines term frequency and inverse document frequency for rank ordering the importance of words in the text',\n",
       " ' We define the classical TF-IDF process below  Dt,d = tft,d · log  (cid:19)  (cid:18) N dft  (8)  where  D - Term document matrix Tf - Term frequency Df - Document frequency BERTopic alters the application of the TF-IDF procedure using a cluster of documents rather than an individual doc- ument',\n",
       " ' This enables the BERTopic to output the topic word  D',\n",
       " ' EVALUATION METRICS When measuring topic quality, human evaluation of a topic is considered a gold standard',\n",
       " ' However, it is a resource intensive and expensive strategy to implement',\n",
       " ' As an alter- native, recent literature has focused on the automatic mea- surement of coherence measures that closely mimic human judgment',\n",
       " ' Topic coherence generally measures the degree to which the words in a topic are semantically related and make sense together [26] Alternatively, coherence can also be defined as a measure of the internal consistency of a topic and how well it represents the documents it is gen- erated from [27]',\n",
       " '',\n",
       " ' For measuring semantic similarity, the most common methods are the Normalized Pointwise Mutual Information (NPMI',\n",
       " ')',\n",
       " ' The NPMI',\n",
       " ' method is based on context vectors j',\n",
       " ' It is constructed by extracting co-occurrences and counts within a context window of ±n tokens around [26]',\n",
       " ' Therefore mathematically, the jth element of the i has an NPMI as represented in  i of word  j',\n",
       " '  context vector equation 9',\n",
       " '  vij = NPMI (cid:0)wi, wj  (cid:1) =  \\uf8eb  \\uf8ec \\uf8ed  log  P(wi,wj)+ϵ P(wi)·P(wj)  − log (cid:0)P (cid:0)wi, wj  (cid:1) + ϵ(cid:1)  \\uf8f6  \\uf8f7 \\uf8f8 (9)  The NPMI metric measures how different in meaning the discovered topics are',\n",
       " ' A higher score in diversity reduces the redundancy among the topics',\n",
       " ' A low score suggests that the topics are repetitive and the model’s ability to abstract the themes in the corpus is low [28]',\n",
       " '  It’s worth noting that the number of topics chosen for the model can impact topic diversity',\n",
       " ' Choosing too many topics can lead to similar topics with overlapping top words while choosing too few can result in broad topics that are difficult to interpret',\n",
       " ' The most common measure for topic diversity is the ratio of unique words among the top 25 words [29]',\n",
       " '  IV',\n",
       " ' EXPERIMENTAL SETUP For the experiment, we used the 20-group news and the BBC news datasets as benchmark datasets',\n",
       " ' Both these datasets  VOLUME 11, 2023  26557  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  TABLE 1',\n",
       " ' Aggregate performance of topic models by dataset',\n",
       " '  FIGURE 3',\n",
       " ' Experiment workflow',\n",
       " '  were available in the preprocessed form in OCTIS — an open-source framework for training, evaluating, and com- paring Topic models',\n",
       " ' The Economic Times corpus was our custom dataset',\n",
       " ' We used pretrained transformers for extracting sentence embeddings',\n",
       " ' Specifically, we used the ’all- mpnet-base-v2, which provides higher and consistent performance [20]',\n",
       " ' The generic workflow of the experiment is represented in Fig 3',\n",
       " ' For evaluating and configuring hyperparameters, we used OCTIS',\n",
       " ' OCTIS provides a unified platform with several topic models, datasets, and evaluation metrics, enabling easy comparison of topic model perfor- mance',\n",
       " ' It adopts a dataset-model-metric approach to pro- vide optimal hyperparameters using a Bayesian Optimization strategy [21]',\n",
       " ' The OCTIS framework is represented in Fig 4',\n",
       " ' For the experiment we have chosen topic coherence and topic diversity measures as defined in the earlier section',\n",
       " ' Topic  FIGURE 4',\n",
       " ' OCTIS workflow',\n",
       " '  coherence metric is based on NPMI and topic diversity is based on unique words among top 10 words',\n",
       " ' The metrics are common for all the three models and is available as part of the OCTIS library',\n",
       " '  A',\n",
       " ' DATA We have used three datasets in the articles - The primary dataset actively curated by researchers and two other stan- dard news datasets for benchmarking the performance of the various topic models',\n",
       " '  1) THE ECONOMIC TIMES NEWS CORPUS The experiment dataset for the article comes from the dig- ital archives of the Economic Times, which is an estab- lished English business daily in India',\n",
       " ' We curated the dataset from news articles between 2008 and 2021',\n",
       " ' The news articles were extracted based on the keyword search ‘climate change’',\n",
       " ' After removing shot news stubs and news items with videos, the final dataset had 9774 documents',\n",
       " ' The total number of terms in the corpus approximates 5 million',\n",
       " '  2) BENCHMARK DATASETS We used the 20 newsgroups and the B',\n",
       " 'B',\n",
       " 'C',\n",
       " ' news dataset as benchmark datasets for evaluating and comparing the  26558  VOLUME 11, 2023  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 5',\n",
       " ' Panel of three figures',\n",
       " ' Figures represent the NPMI score for across values of k',\n",
       " '  VOLUME 11, 2023  26559  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 5',\n",
       " ' (Continued',\n",
       " ') Panel of three figures',\n",
       " ' Figures represent the NPMI score for across values of k',\n",
       " '  performance of topic modes',\n",
       " ' The 20 newsgroups dataset is popularly used in NLP experiments as a benchmarking dataset consisting of 20000 news articles equally distributed across 20 categories',\n",
       " ' The categories include technology, pol- itics, religion, auto, sports, etc',\n",
       " ' Similarly, the B',\n",
       " 'B',\n",
       " 'C',\n",
       " ' news dataset has 2225 news articles reported between 2004-2005 across five topical areas',\n",
       " ' The categories include Business, Sports, Technology, Entertainment, and Politics',\n",
       " '  3) RESULTS Every topic model chosen for the experiment was run on each dataset three times',\n",
       " ' In each run was defined by the choice of a random seed',\n",
       " ' In every run we selected five OCTIS values for k in multiples of 10 until 50',\n",
       " ' In other words, each topic model was run 15 times on a dataset',\n",
       " ' The performance of the model can be analyzed in two ways',\n",
       " ' First, both NPMI',\n",
       " ' and topic diversity can be aggregated at a dataset level',\n",
       " ' We average the NPMI and topic diversity scores over the three iterations and summarize it at a dataset level as shown in Table 1',\n",
       " ' BERTopic performs better than LDA and NMF across all three datasets and on both the evaluation metrics',\n",
       " ' Between LDA and NMF, we also see that LDA performs better than NMF in terms of topic diversity, while NMF does better in terms of coherence',\n",
       " ' Second, we drill down into the performance of the three mod- els at a dataset level where we calculate NPMI',\n",
       " ' with respect to k',\n",
       " ' Figure 5 is a panel of three graphs that reports the NPMI',\n",
       " ' performance',\n",
       " ' From the display, we observe that BERtopic outperforms the two other models across all three datasets',\n",
       " ' We find that NPMI',\n",
       " ' reached its maximum for different values  of k for each of the dataset',\n",
       " ' In the case of the Economic Times dataset, we find that NPMI',\n",
       " ' value increases steadily till k = 30, declines and increases again until it reaches its high at k = 50',\n",
       " ' For the other two datasets, maximum NPMI',\n",
       " ' is achieved at k = 20 and k = 40, respectively',\n",
       " '  Besides coherence, the experiment also yielded topic diver- sity scores for each model',\n",
       " ' The results are displayed in Figure 6',\n",
       " ' Once again, we observe that BERTopic performs better than the other models for the experiment and bench- mark datasets',\n",
       " ' For the Economic Times corpus we see that topic diversity is maximized at k = 20 and remains stable till k = 50',\n",
       " ' For the benchmark datasets on the other hand, we see that topic diversity declines continuously for higher values of k',\n",
       " ' Based on the results for the Economic Times dataset, we set the k value at 50, where we achieved the highest NPMI',\n",
       " ' and highest topic diversity as well',\n",
       " '  V',\n",
       " ' MAPPING THEMES USING BERTOPIC Based on the model selection experiment in the last section, we chose BERTopic as the most suitable among all the three models',\n",
       " ' We then applied it to the E',\n",
       " 'T',\n",
       " ' corpus with k = 50',\n",
       " ' One of the advantages of BERTopic is that it generates topic labels along with the topics',\n",
       " ' The labels are generated as a combination of the topic number and dominant keywords',\n",
       " ' A partial list is provided below  • 1_water_said_pollution_air • 2_countries_climate_developing_agreement • 4_energy_climate_india_coal • 5_ice_climate_said_warming  26560  VOLUME 11, 2023  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 6',\n",
       " ' Panel of three figures',\n",
       " ' Figures represent the diversity score for across values of k',\n",
       " '  VOLUME 11, 2023  26561  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 6',\n",
       " ' (Continued',\n",
       " ') Panel of three figures',\n",
       " ' Figures represent the diversity score for across values of k',\n",
       " '  • 6_climate_emissions_countries_change’ • 8_energy_solar_renewable_power • 12_monsoon_rainfall_climate_said • 15_climate_trump_paris_said • 17_species_tiger_conservation_wildlife • 39_energy_india_renewable_clean • 47_adani_mine_queensland_coal • 49_Yoga_Resolution_day  While topics might be coherent semantically, not all of them may be directly relevant to climate change',\n",
       " ' For example, in topic 49 in the above-covered news about yoga day events, speakers also mentioned climate change',\n",
       " ' Alternatively, top- ics can be merged to represent a larger theme, such as in the case of topics 2 & 6',\n",
       " ' Both topics relate to climate change negotiations with obvious differences in their point of view',\n",
       " ' Topic 2 covers news articles with a ‘developing country’ perspective, while topic six focuses on news articles which discuss country-specific emissions within the summit context',\n",
       " '  To further identify opportunities for merging similar top- ics we performed the following steps',\n",
       " ' First we examined keywords within each topic to identify topics that can be merged or need to be discarded',\n",
       " ' Second, we manually sampled the full articles within each topic to infer the context',\n",
       " ' Finally, we investigated the relationship between topics through clustering based on the cosine distance between topic embeddings',\n",
       " ' The extracted themes, and their  relationship is presented in Fig 7',\n",
       " ' At the end of the process we were able to identify topics that were peripheral to climate change and discarded them',\n",
       " ' The other topics, we grouped them into eight overarching themes',\n",
       " ' The themes were ‘Cli- mate cooperation’ and ‘Climate agreements & ’Energy & Emissions’, ‘clean energy’, Resource management, Country emissions and Business engagement ‘country emissions',\n",
       " '’ ‘Climate cooperation’ combines topics 2,13,21,27,35,42, 43,46, represents all news articles on bilateral and mul- trade relationships, and regional summits tilateral visits, (ex: SAARC, Quad), etc',\n",
       " ', where climate change is on the agenda',\n",
       " ' ’Resource management and ecosystems’ consists of news articles about natural resources management, including air, land, water, wildlife, food, etc',\n",
       " ' News stories often discuss the impact of climate change on natural resources',\n",
       " ' The theme groups topics 4,9,17,33 and ranks second in terms of overall contribution to the corpus',\n",
       " ' ‘Climate agreements & negoti- ations’ contains news articles that discuss COP summits, primarily including Paris, Glasgow, Copenhagen, and Lima',\n",
       " ' Following this we have two themes on energy’ Energy & Emissions’ and ’clean energy',\n",
       " ' While they cover energy at large, the key distinctions come from the focus of the news stories',\n",
       " ' The ‘Energy & emissions’ topic consists of articles that report the role of energy in emissions and emission reduc- tion',\n",
       " ' On the other hand ‘clean energy’ exclusively focuses on domestic action in improving the share of renewables in electricity regeneration',\n",
       " ' On a similar note, we find the  26562  VOLUME 11, 2023  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 7',\n",
       " ' Hierarchial topic cluster of the corpus',\n",
       " '  ‘country emissions’ focuses on new stories that present national emission targets and goals, and performance of dif- ferent countries',\n",
       " ' Specifically, several stories discuss India and China and their role as prominent contributors in terms of absolute emissions',\n",
       " '  Finally, we have the ‘‘Business and climate’’ themes, which subsume three topic categories- 12,22 and 49',\n",
       " ' The news stories are a collection of company agendas for reducing climate footprints, improving sustainability practices, CEO',\n",
       " ' speeches on climate change, impact on the stock, product markets, and new emerging industries such as Electric Vehi- cles & Batteries',\n",
       " ' We further track the aggregated themes over 2008 -2020, as shown in Fig 8',\n",
       " ' It is interesting to note that news across all categories spiked sharply in 2015  and 2021',\n",
       " ' These spikes may be attributed to the greater atten- tion given to climate change around the Paris COP and the Glasgow COP',\n",
       " '  There are also interesting insights that emerge from how different themes trend',\n",
       " ' Before 2014 news on climate sum- mits and negotiations dominated other categories',\n",
       " ' Post 2014, we see that news on climate cooperation has the highest contribution to the overall corpus',\n",
       " ' In tandem with climate cooperation, we also observe the increase in reporting on the impact of climate change on resources and ecosystems',\n",
       " ' We also observe that both the categories of energy news (emissions & clean energy) move together',\n",
       " ' Finally we, see that news around business themes has gained traction since 2012 but has increased significantly from 2017 onwards',\n",
       " '  VOLUME 11, 2023  26563  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 8',\n",
       " ' Trends in climate change news within themes',\n",
       " '  VI',\n",
       " ' DISCUSSION AND CONCLUSION The scientific consensus on climate change is unassail- able and there is an urgent need for stronger climate action globally',\n",
       " ' However, countries continue to grapple with socio-political challenges which have emerged as a deterrent to effective climate action',\n",
       " ' The social and political discourse around climate change involves the public, the state, and the scientific community, with media as the common platform',\n",
       " ' Understanding how media reports on climate change gives a glimpse into this diverse stakeholder system’s perspectives, debates, and responses',\n",
       " '  However, the tools used to understand the climate change discourse have been derived mainly from traditional quan- titative and qualitative research',\n",
       " ' Natural Language based techniques can augment the existing toolbox to unfold and extract insights from the climate change debate',\n",
       " ' This study is a step towards achieving integration of NLP in social science research and, more specifically, within the domain of climate change discourse [17], [30]',\n",
       " ' Another important contribution of the study is the demonstration and use of embeddings based topic models BERTopic as an alternative to LDA and NMF',\n",
       " ' Additionally, it also furthers the use of experimental frameworks such as OCTIS for evaluating the performance of topic models',\n",
       " ' Finally, through the longitudinal analysis of climate news the study provides a developing country perspective to the literature on climate change discourse  The analysis in our study includes two components',\n",
       " ' First an experimental study on topic models to determine relative performance',\n",
       " 'Second is the actual mapping of the climate news based on the model selected in the experiment Our experiments show that embedding models perform signif- icantly better than the other topic models',\n",
       " ' The results are consistent with other recent comparative studies [31], [32]',\n",
       " '  We further demonstrate the application of BERTopic by training it on the Economic Times corpus and discovering news frames and their evolution',\n",
       " ' Results suggest that news frames around climate negotiation have been dominant before 2014 while climate cooperation gains importance post 2014',\n",
       " ' We also found that climate frames related to domestic action also has gained traction in recent years',\n",
       " '  Stakeholder consensus is an essential aspect of climate action',\n",
       " ' Given the multiple stakeholders in climate change discourse, media analysis can play a crucial role in gauging the engagement of citizens, the state, and the commercial sector',\n",
       " ' Using NLP techniques in this context can scale up information processing and augment insights derived from other research methods, thus helping make informed policy decisions for countering climate change',\n",
       " '  REFERENCES  [1] D',\n",
       " ' Eckstein, Global Climate Risk Index 2020',\n",
       " ' Bonn, Germany: German-  watch, 2019',\n",
       " '  [2] T',\n",
       " ' R',\n",
       " ' Keller, V',\n",
       " ' Hase, J',\n",
       " ' Thaker, D',\n",
       " ' Mahl, and M',\n",
       " ' S',\n",
       " ' Schäfer, ‘‘News media coverage of climate change in India 1997–2016: Using automated content analysis to assess themes and topics,’’ Environ',\n",
       " ' Commun',\n",
       " ', vol',\n",
       " ' 14, no',\n",
       " ' 2, pp',\n",
       " ' 219–235, Feb',\n",
       " ' 2020',\n",
       " '  [3] J',\n",
       " ' Bohr, ‘‘Reporting on climate change: A computational analysis of U',\n",
       " 'S',\n",
       " ' newspapers and sources of bias, 1997–2017,’’ Global Environ',\n",
       " ' Change, vol',\n",
       " ' 61, Mar',\n",
       " ' 2020, Art',\n",
       " ' no',\n",
       " ' 102038',\n",
       " '  [4] L',\n",
       " ' L',\n",
       " ' Benites-Lazaro, L',\n",
       " ' Giatti, and A',\n",
       " ' Giarolla, ‘‘Topic modeling method for analyzing social actor discourses on climate change, energy and food security,’’ Energy Res',\n",
       " ' Social Sci',\n",
       " ', vol',\n",
       " ' 45, pp',\n",
       " ' 318–330, Nov',\n",
       " ' 2018',\n",
       " ' [5] S',\n",
       " ' Umamaheswaran, V',\n",
       " ' Dar, and J',\n",
       " ' Thaker, ‘‘The evolution of climate change reporting in business media: Longitudinal analysis of a business newspaper,’’ Sustainability, vol',\n",
       " ' 14, no',\n",
       " ' 22, Nov',\n",
       " ' 2022, Art',\n",
       " ' no',\n",
       " ' 15214',\n",
       " ' [6] J',\n",
       " ' Foulds, ‘‘Mixed membership word embeddings for computational social  science,’’ in Proc',\n",
       " ' Int',\n",
       " ' Conf',\n",
       " ' Artif',\n",
       " ' Intell',\n",
       " ' Statist',\n",
       " ', 2018, pp',\n",
       " ' 86–95',\n",
       " '  [7] D',\n",
       " ' Hovy and S',\n",
       " ' L',\n",
       " ' Spruit, ‘‘The social impact of natural language pro- cessing,’’ in Proc',\n",
       " ' 54th Annu',\n",
       " ' Meeting Assoc',\n",
       " ' Comput',\n",
       " ' Linguistics (Short Papers), vol',\n",
       " ' 2, 2016, pp',\n",
       " ' 591–598',\n",
       " '  26564  VOLUME 11, 2023  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  [8] S',\n",
       " ' Mohammad, S',\n",
       " ' Kiritchenko, P',\n",
       " ' Sobhani, X',\n",
       " ' Zhu, and C',\n",
       " ' Cherry, ‘‘SemEval-2016 task 6: Detecting stance in tweets,’’ in Proc',\n",
       " ' 10th Int',\n",
       " ' Workshop Semantic Eval',\n",
       " ' (SemEval-), 2016, pp',\n",
       " ' 31–41',\n",
       " '  [9] K',\n",
       " ' Johnson, I',\n",
       " '-T',\n",
       " ' Lee, and D',\n",
       " ' Goldwasser, ‘‘Ideological phrase indicators for classification of political discourse framing on Twitter,’’ in Proc',\n",
       " ' 2nd Workshop NLP Comput',\n",
       " ' Social Sci',\n",
       " ', 2017, pp',\n",
       " ' 90–99',\n",
       " '  [10] S',\n",
       " ' M',\n",
       " ' Mohammad, X',\n",
       " ' Zhu, S',\n",
       " ' Kiritchenko, and J',\n",
       " ' Martin, ‘‘Sentiment, emotion, purpose, and style in electoral tweets,’’ Inf',\n",
       " ' Process',\n",
       " ' Manage',\n",
       " ', vol',\n",
       " ' 51, no',\n",
       " ' 4, pp',\n",
       " ' 480–499, 2015',\n",
       " '  [11] R',\n",
       " ' Kashyap and A',\n",
       " ' Nahapetian, ‘‘Tweet analysis for user health mon- itoring,’’ in Proc',\n",
       " ' 4th Int',\n",
       " ' Conf',\n",
       " ' Wireless Mobile Commun',\n",
       " ' Healthcare Transforming Healthcare Through Innov',\n",
       " ' Mobile Wireless Technolog',\n",
       " ', Nov',\n",
       " ' 2014, pp',\n",
       " ' 348–351',\n",
       " '  [12] P',\n",
       " ' Lavanya and E',\n",
       " ' Sasikala, ‘‘Deep learning techniques on text classifica- tion using natural language processing (NLP) in social healthcare network: A comprehensive survey,’’ in Proc',\n",
       " ' 3rd Int',\n",
       " ' Conf',\n",
       " ' Signal Process',\n",
       " ' Commun',\n",
       " ' (ICPSC), May 2021, pp',\n",
       " ' 603–609',\n",
       " '  [13] A',\n",
       " ' Farzindar and D',\n",
       " ' Inkpen, ‘‘Natural language processing for social media,’’ Synth',\n",
       " ' Lect',\n",
       " ' Hum',\n",
       " ' Lang',\n",
       " ' Technol',\n",
       " ', vol',\n",
       " ' 8, no',\n",
       " ' 2, pp',\n",
       " ' 1–166, 2015',\n",
       " ' [14] L',\n",
       " ' Hagen, O',\n",
       " ' Uzuner, C',\n",
       " ' Kotfila, T',\n",
       " ' M',\n",
       " ' Harrison, and D',\n",
       " ' Lamanna, ‘‘Under- standing Citizens’ direct policy suggestions to the federal government: A natural language processing and topic modeling approach,’’ in Proc',\n",
       " ' 48th Hawaii Int',\n",
       " ' Conf',\n",
       " ' Syst',\n",
       " ' Sci',\n",
       " ', Jan',\n",
       " ' 2015, pp',\n",
       " ' 2134–2143',\n",
       " '  [15] D',\n",
       " ' Elgesem and L',\n",
       " ' N',\n",
       " ' Steskal Diakopoulos, ‘‘Structure and content of the discourse on climate change in the blogosphere: The big picture,’’ in Climate Change Communication and the Internet',\n",
       " ' Evanston, IL, USA: Routledge, 2019, pp',\n",
       " ' 21–40',\n",
       " '  [16] H',\n",
       " ' Elfardy and M',\n",
       " ' Diab, ‘‘CU-GWU perspective at SemEval-2016 task 6: Ideological stance detection in informal text,’’ in Proc',\n",
       " ' 10th Int',\n",
       " ' Workshop Semantic Eval',\n",
       " ' (SemEval-), 2016, pp',\n",
       " ' 434–439',\n",
       " '  [17] P',\n",
       " ' Swarnakar and A',\n",
       " ' Modi, ‘‘NLP for climate policy: Creating a knowledge platform for holistic and effective climate action,’’ 2021, arXiv:2105',\n",
       " '05621',\n",
       " '  [18] B',\n",
       " ' Nerlich, R',\n",
       " ' Forsyth, and D',\n",
       " ' Clarke, ‘‘Climate in the news: How dif- ferences in media discourse between the U',\n",
       " 'S',\n",
       " ' and U',\n",
       " 'K',\n",
       " ' reflect national priorities,’’ Environ',\n",
       " ' Commun',\n",
       " ', vol',\n",
       " ' 6, no',\n",
       " ' 1, pp',\n",
       " ' 44–63, Mar',\n",
       " ' 2012',\n",
       " ' [19] P',\n",
       " ' DiMaggio, M',\n",
       " ' Nag, and D',\n",
       " ' Blei, ‘‘Exploiting affinities between topic modeling and the sociological perspective on culture: Application to news- paper coverage of U',\n",
       " 'S',\n",
       " ' government arts funding,’’ Poetics, vol',\n",
       " ' 41, no',\n",
       " ' 6, pp',\n",
       " ' 570–606, Dec',\n",
       " ' 2013',\n",
       " '  [20] Y',\n",
       " ' Bengio, R',\n",
       " ' Ducharme, and P',\n",
       " ' Vincent, ‘‘A neural probabilistic lan- guage model,’’ in Proc',\n",
       " ' Adv',\n",
       " ' Neural Inf',\n",
       " ' Process',\n",
       " ' Syst',\n",
       " ', vol',\n",
       " ' 13, 2000, pp',\n",
       " ' 1137–1155',\n",
       " '  [21] A',\n",
       " ' B',\n",
       " ' Dieng, F',\n",
       " ' J',\n",
       " ' R',\n",
       " ' Ruiz, and D',\n",
       " ' M',\n",
       " ' Blei, ‘‘Topic modeling in embed- ding spaces,’’ Trans',\n",
       " ' Assoc',\n",
       " ' Comput',\n",
       " ' Linguistics, vol',\n",
       " ' 8, pp',\n",
       " ' 439–453, Dec',\n",
       " ' 2020',\n",
       " '  [22] P',\n",
       " ' Ghasiya and K',\n",
       " ' Okamura, ‘‘Investigating COVID-19 news across four nations: A topic modeling and sentiment analysis approach,’’ IEEE Access, vol',\n",
       " ' 9, pp',\n",
       " ' 36645–36656, 2021',\n",
       " '  [23] J',\n",
       " '-R',\n",
       " ' Lin, Z',\n",
       " '-Z',\n",
       " ' Hu, J',\n",
       " '-L',\n",
       " ' Li, and L',\n",
       " '-M',\n",
       " ' Chen, ‘‘Understanding on-site inspection of construction projects based on keyword extraction and topic modeling,’’ IEEE Access, vol',\n",
       " ' 8, pp',\n",
       " ' 198503–198517, 2020',\n",
       " '  [24] Y',\n",
       " ' Chen, H',\n",
       " ' Zhang, R',\n",
       " ' Liu, Z',\n",
       " ' Ye, and J',\n",
       " ' Lin, ‘‘Experimental explorations on short text topic mining between LDA and NMF based schemes,’’ Knowl',\n",
       " '-Based Syst',\n",
       " ', vol',\n",
       " ' 163, pp',\n",
       " ' 1–13, Jan',\n",
       " ' 2019',\n",
       " '  [25] D',\n",
       " ' Lee and H',\n",
       " ' S',\n",
       " ' Seung, ‘‘Algorithms for non-negative matrix factoriza- tion,’’ in Proc',\n",
       " ' Adv',\n",
       " ' Neural Inf',\n",
       " ' Process',\n",
       " ' Syst',\n",
       " ', vol',\n",
       " ' 13, 2000, pp',\n",
       " ' 1–7',\n",
       " ' [26] M',\n",
       " ' Röder, A',\n",
       " ' Both, and A',\n",
       " ' Hinneburg, ‘‘Exploring the space of topic coherence measures,’’ in Proc',\n",
       " ' 8th ACM Int',\n",
       " ' Conf',\n",
       " ' Web Search Data Mining, Feb',\n",
       " ' 2015, pp',\n",
       " ' 399–408',\n",
       " '  [27] D',\n",
       " ' Mimno, ‘‘Optimizing semantic coherence in topic models,’’ in Proc',\n",
       " ' Conf',\n",
       " ' Empirical Methods Natural Language Process',\n",
       " ', 2011, pp',\n",
       " ' 1–11',\n",
       " ' [28] A',\n",
       " ' Abdelrazek, Y',\n",
       " ' Eid, E',\n",
       " ' Gawish, W',\n",
       " ' Medhat, and A',\n",
       " ' Hassan, ‘‘Topic modeling algorithms and applications: A survey,’’ Inf',\n",
       " ' Syst',\n",
       " ', vol',\n",
       " ' 112, Feb',\n",
       " ' 2023, Art',\n",
       " ' no',\n",
       " ' 102131',\n",
       " '  [29] S',\n",
       " ' Limwattana and S',\n",
       " ' Prom-On, ‘‘Topic modeling enhancement using word embeddings,’’ in Proc',\n",
       " ' 18th Int',\n",
       " ' Joint Conf',\n",
       " ' Comput',\n",
       " ' Sci',\n",
       " ' Softw',\n",
       " ' Eng',\n",
       " ' (JCSSE), Jun',\n",
       " ' 2021, pp',\n",
       " ' 1–5',\n",
       " '  [30] M',\n",
       " ' Stede and R',\n",
       " ' Patz, ‘‘The climate change debate and natural language processing,’’ in Proc',\n",
       " ' 1st Workshop NLP Positive Impact, Aug',\n",
       " ' 2021, pp',\n",
       " ' 8–18',\n",
       " '  [31] M',\n",
       " ' J',\n",
       " ' Sánchez-Franco and M',\n",
       " ' Rey-Moreno, ‘‘Do travelers’ reviews depend on the destination? An analysis in coastal and urban peer-to-peer lodg- ings,’’ Psychol',\n",
       " ' Marketing, vol',\n",
       " ' 39, no',\n",
       " ' 2, pp',\n",
       " ' 441–459, 2022',\n",
       " '  [32] R',\n",
       " ' Egger and J',\n",
       " ' Yu, ‘‘A topic modeling comparison between LDA, NMF, Top2 Vec, and BERTopic to demystify Twitter posts,’’ Frontiers Sociol',\n",
       " ', vol',\n",
       " ' 7, May 2022, Art',\n",
       " ' no',\n",
       " ' 886498',\n",
       " '  SWARNALAKSHMI UMAMAHESWARAN rec- eived the Ph',\n",
       " 'D',\n",
       " ' degree in public policy from the TERI School of Advanced Studies, New Delhi',\n",
       " ' Her doctoral work was on policy analysis for low-carbon growth development in the Indian con- text',\n",
       " ' Prior to her doctoral studies, she was an analytics consultant across several MNCs in the banking and marketing sector',\n",
       " ' As part of her cor- porate career, she has built credit scoring and risk-profiling models using machine learning and data science tools',\n",
       " ' She is an economist by education and a data scientist by practice',\n",
       " ' She is currently a Business Analytics Faculty Member with the Symbiosis Institute of Business Management, Bengaluru',\n",
       " ' Her current research interest includes computational social sciences, with a special focus on climate and sustainability',\n",
       " '  VANDITA DAR received the Ph',\n",
       " 'D',\n",
       " ' degree in eco- nomics from Mumbai University',\n",
       " ' She is a pas- sionate economist and a researcher working as a Faculty Member with the Symbiosis Institute of Business Management, Bengaluru',\n",
       " ' She has diverse work experience in academics, research, and cor- porate for more than 20 years',\n",
       " ' In her last corporate assignment, she held the profile of an economist with a global financial services company',\n",
       " ' She has conducted capacity-building workshops for senior government officers spanning the areas of macroeconomic policy and busi- ness environment, and was the head of the training division with an apex state training institute',\n",
       " ' Her research interests include sustainability, climate issues, macroeconomic policy, and development economics',\n",
       " ' She is a reviewer of reputed international journals',\n",
       " '  ELIZA SHARMA received the Ph',\n",
       " 'D',\n",
       " ' degree in management and finance from the Jaypee Institute of Information Technology, Noida',\n",
       " ' She was with the Indian Institute of Management, Ahmedabad, India, as a Research Assistant for the Government of India project to develop the Integrity Index for public sector organizations',\n",
       " ' She is currently a Finance Faculty Member with the Symbiosis Institute of Business Management, Bengaluru',\n",
       " ' She has published many research papers in national and international journals and conference proceedings',\n",
       " ' Her research interest includes exploring corporate social responsibility practices in the industry',\n",
       " '  JIKKU SUSAN KURIAN received the Ph',\n",
       " 'D',\n",
       " ' degree in the core area of organizational behav- ior',\n",
       " ' She has 17 years of experience, of which eight years were with leading corporates and the rest of seven years with KL University Business School, Vijayawada',\n",
       " ' As a faculty member, she handles organizational behavior and HR-related subjects',\n",
       " ' She is currently an Assistant Professor with the Symbiosis Institute of Business Manage- ment, Bengaluru (SIBM-B)',\n",
       " ' Her current research interests include researching sustainable HRM and technological transitions',\n",
       " '  VOLUME 11, 2023  26565  \\x0c']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_text=final_text.split('.')\n",
    "final_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=pd.DataFrame(final_text)\n",
    "# type(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Received 7 February 2023, accepted 2 March 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Digital Object Identifier 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1109/ACCESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3256530  Mapping Climate Themes From 2008-2021...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>684</th>\n",
       "      <td>She has 17 years of experience, of which eigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>As a faculty member, she handles organization...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>She is currently an Assistant Professor with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>Her current research interests include resear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>VOLUME 11, 2023  26565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>611 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0    Received 7 February 2023, accepted 2 March 202...\n",
       "1                         Digital Object Identifier 10\n",
       "2                                          1109/ACCESS\n",
       "3                                                 2023\n",
       "4    3256530  Mapping Climate Themes From 2008-2021...\n",
       "..                                                 ...\n",
       "684   She has 17 years of experience, of which eigh...\n",
       "685   As a faculty member, she handles organization...\n",
       "686   She is currently an Assistant Professor with ...\n",
       "687   Her current research interests include resear...\n",
       "688                          VOLUME 11, 2023  26565  \n",
       "\n",
       "\n",
       "[611 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.drop_duplicates(inplace=True)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage import io, color\n",
    "\n",
    "def image_similarity(image_path1, image_path2):\n",
    "    # Load the images\n",
    "    img1 = io.imread(image_path1)\n",
    "    img2 = io.imread(image_path2)\n",
    "\n",
    "    # Convert images to grayscale if they are in color\n",
    "    if img1.shape[-1] == 3:\n",
    "        img1 = color.rgb2gray(img1)\n",
    "    if img2.shape[-1] == 3:\n",
    "        img2 = color.rgb2gray(img2)\n",
    "\n",
    "    # Compute the Structural Similarity Index (SSI)\n",
    "    similarity_index, _ = ssim(img1, img2, full=True,data_range=img1.max() - img1.min())\n",
    "\n",
    "    return similarity_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compare_and_resize(image1,image2):\n",
    "#     img1=Image.open(image1)\n",
    "#     img2=Image.open(image2)\n",
    "\n",
    "#     if(img1.size[0]==img2.size[0]): return True\n",
    "\n",
    "#     elif (img1.size[0]>img2.size[0]):\n",
    "#         img1=img1.resize(img2.size)\n",
    "#         img1.save(image1)\n",
    "#         return True\n",
    "#     else:\n",
    "#         img2=img2.resize(img1.size)\n",
    "#         img2.save(image2)\n",
    "#         return True\n",
    "\n",
    "def resize_image(image, target_size):\n",
    "    resized_image = cv2.resize(image, target_size)\n",
    "    return resized_image\n",
    "\n",
    "def compare_images(image1, image2):\n",
    "    img1=cv2.imread(image1)\n",
    "    img2=cv2.imread(image2)\n",
    "    # Resize images to a common size\n",
    "    target_size = (width, height) = (300, 200)\n",
    "    resized_image1 = resize_image(img1, target_size)\n",
    "    resized_image2 = resize_image(img2, target_size)\n",
    "\n",
    "    # Convert resized images to grayscale\n",
    "    gray_image1 = cv2.cvtColor(resized_image1, cv2.COLOR_BGR2GRAY)\n",
    "    gray_image2 = cv2.cvtColor(resized_image2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Compute Structural Similarity Index (SSI)\n",
    "    similarity_index, _ = ssim(gray_image1, gray_image2, full=True)\n",
    "\n",
    "    return similarity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image1.png',\n",
       " 'image2.png',\n",
       " 'image4.jpeg',\n",
       " 'image6.png',\n",
       " 'image7.jpeg',\n",
       " 'image8.jpeg',\n",
       " 'image10.jpeg',\n",
       " 'image12.jpeg',\n",
       " 'image13.jpeg',\n",
       " 'image15.jpeg',\n",
       " 'image17.jpeg',\n",
       " 'image18.jpeg',\n",
       " 'image20.jpeg',\n",
       " 'image22.jpeg',\n",
       " 'image24.jpeg',\n",
       " 'image25.jpeg',\n",
       " 'image26.jpeg',\n",
       " 'image27.jpeg',\n",
       " 'image29.jpeg']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# x=len(image_names_arr)\n",
    "# for i in range(x):\n",
    "#     for j in range(i+1,x):\n",
    "#         if compare_images(image_names_arr[i],image_names_arr[j])>0.8:\n",
    "#             os.remove(image_names_arr[j])\n",
    "#             image_names_arr.remove(image_names_arr[j])\n",
    "#             x=len(image_names_arr)\n",
    "\n",
    "# print(image_names_arr)\n",
    "def remove_similar_images(image_names_arr, similarity_threshold=0.8):\n",
    "    i = 0\n",
    "    while i < len(image_names_arr):\n",
    "        j = i + 1\n",
    "        while j < len(image_names_arr):\n",
    "            if compare_images(image_names_arr[i], image_names_arr[j]) > similarity_threshold:\n",
    "                # Remove similar image and update the list\n",
    "                os.remove(image_names_arr[j])\n",
    "                image_names_arr.pop(j)\n",
    "            else:\n",
    "                j += 1\n",
    "        i += 1\n",
    "\n",
    "    return image_names_arr\n",
    "\n",
    "remove_similar_images(image_names_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abstract_and_references(pdf_list):\n",
    "    abstract_index = next((i for i, s in enumerate(pdf_list) if \"ABSTRACT\" in s), None)\n",
    "    references_index = next((i for i, s in enumerate(pdf_list) if \"REFERENCES\" in s), None)\n",
    "\n",
    "    if abstract_index is not None and references_index is not None:\n",
    "        abstract = pdf_list[abstract_index + 1:references_index]\n",
    "        return abstract\n",
    "    else:\n",
    "        print(\"ABSTRACT or REFERENCES not found in the list.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list=extract_abstract_and_references(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' In terms of absolute emissions, India surpassed China and the U',\n",
       " 'S',\n",
       " ' in 2018 to become the third-largest emitter',\n",
       " ' Solving this wicked problem calls for climate action across the stakeholder spectrum involving governments, business communities, and citizens',\n",
       " ' While extant literature has focused significantly on the role of governments and individual perceptions, the business sector needs to be more represented',\n",
       " ' In this study, we consider business news media as a platform that reflects the industry engagement in climate change and as a source of information on climate change for business decision-makers',\n",
       " ' Hence, understanding the topic and themes in the nexus of climate and business is important to evaluate the business sector’s stance towards climate change and how it has evolved',\n",
       " ' This work explores business news related to climate change using natural language techniques',\n",
       " ' We first experiment with three topic-modeling techniques, such as LDA, NMF, and BERTopic, on the business news and two more benchmark news datasets',\n",
       " ' Our test data is derived from digital news archives of ’The Economic Times – India’s leading business news daily',\n",
       " ' We evaluate the performance based on quantitative metrics commonly used for topic models',\n",
       " ' We choose the algorithm that provides the highest precision for climate-specific information represented by the test dataset',\n",
       " ' We then apply the algorithm with the best performance, as evaluated by the experiment, to a large corpus of Indian climate news from E',\n",
       " 'T',\n",
       " ' spanning from 2008 -2021',\n",
       " ' We present how different themes, including industry engagement, evolved over the last two decades',\n",
       " ' The results suggest that climate cooperation has the highest contribution in the corpus, with other themes on resource management, energy and business gaining traction in recent years',\n",
       " '  INDEX TERMS Climate change, media, topic models, NLP, computational social sciences, experiment',\n",
       " '  I',\n",
       " ' INTRODUCTION Climate change is the most critical issue of this millennium',\n",
       " ' India has high stakes in the global debate since it’s the third largest emitter and also high on climate vulnerability [1]',\n",
       " ' In its most recent update to the Paris pledge, the country has further committed to reducing its emission intensity to 35% of 2005 levels by 2030',\n",
       " ' Emission reduction of this scale requires solid public support across the stakeholder spectrum, includ- ing citizens and enterprises',\n",
       " ' The role of the commercial sector is particularly relevant given its contribution to country-level emissions',\n",
       " '  The associate editor coordinating the review of this manuscript and  approving it for publication was Arianna Dulizia  ',\n",
       " '  A practical approach toward understanding how different stakeholders engage in climate change can be through the analysis of news coverage',\n",
       " ' While all news media reflect contemporary public discourse, business newspapers tra- ditionally report news that is more relevant for business decision-makers',\n",
       " ' Therefore, mapping the topics pertinent to climate change in business media and how such issues have evolved can help us understand how the commercial sector engages with the issue of climate change',\n",
       " '  Automated content analysis of large corpora is an estab- lished technique in the social sciences',\n",
       " ' Specifically, the use of topic models for extracting latent topics from newspapers, discussion forums, and other social media content has been gaining traction in extant literature',\n",
       " ' Among the different  26554  This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4',\n",
       " '0 License',\n",
       " ' For more information, see https://creativecommons',\n",
       " 'org/licenses/by-nc-nd/4',\n",
       " '0/  VOLUME 11, 2023  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  Natural Language Processing (NLP) techniques, topic mod- eling is a machine-learning task that groups documents and words with similar meanings',\n",
       " ' All topic models yield topics, each ranked based on the specific terms and their relevance to the topic',\n",
       " ' The topic emerges automatically without prior annotation or labeling of the raw data',\n",
       " ' In this sense, topic models form an important category of unsupervised tech- niques within the larger field of Natural Language Process- ing',\n",
       " ' The are many mathematical approaches to topic models and an even higher spread of algorithms that learn the param- eters of such model approaches',\n",
       " ' Conventional topic models range from linear algebraic-based techniques such as LSA to more popular probabilistic models like LDA and PLSA',\n",
       " ' Metrics decomposition-based methods such as NMF have also been a mainstay of NLP applications',\n",
       " ' The underlying Bag Of Words (BOW) approach is a distinguishing factor among all these approaches',\n",
       " ' However, the BoW-based topic model is based on the unrealistic assumption that words are independent of each other and relies on frequencies of word occurrences',\n",
       " ' A more recent development in NLP, is the representation of text using embeddings',\n",
       " ' Embeddings in the context of NLP are dense representations of units of the text in the vector space in a way that the semantic similarity between the units of text is captured',\n",
       " ' Although automated content analysis is gaining importance within the climate discourse literature, there is an overreliance on probabilistic topic models [2], [3], [4], [5]',\n",
       " ' More importantly, the choice of topic models such as LDA is motivated by popular use and is not backed by experiments to evaluate performance metrics especially in the social sciences [6]',\n",
       " ' Given this background, the objective of this article is to evaluate the performance of conventional BOW-based models with embeddings-based models',\n",
       " ' Specifically, we run experiments using LDA, NMF, and BERTopic',\n",
       " ' We choose the most appropriate model based on the evaluation metrics and use it to map themes within the Economic Times corpus',\n",
       " '  The article is organized as follows',\n",
       " ' Section II introduces the literature on automated content analysis of climate news and discusses how topic models are used in the literature',\n",
       " ' Section III presents the data and methods',\n",
       " ' Section IV offers the details of the experiment and its results',\n",
       " ' Section V presents the results of the final implantation of the chosen model',\n",
       " '  II',\n",
       " ' LITERATURE REVIEW The adoption of NLP techniques in social sciences has seen significant growth in the recent decade',\n",
       " ' Language is a social construct and can be considered a proxy for behav- ior [7]',\n",
       " ' As expressed by people, language may contain rich latent information that can help identify several dimen- sions and traits of behavior',\n",
       " ' Therefore, the rapid growth of NLP methods within social sciences is not surprising',\n",
       " ' The social science applications of NLP range from understand- ing political affiliations and voter intentions [8], [9], [10], health care delivery [11], [12], media monitoring [13], and  improving public policy implementation [14]',\n",
       " ' The use of NLP in mining social discourse around climate change is still nascent and broadly categorized based on the analy- sis of social media platforms, online news, and scientific or technical documents',\n",
       " ' Several studies analyze societal stance on climate change through the study of Twitter and microblogs',\n",
       " ' For example, Elgasem et al',\n",
       " ' combine sentiment and networking analysis to identify climate skeptics and accept communities [15]',\n",
       " ' Stance detection in climate change tweets emerged as a separate subdomain with the release of the SemEval dataset, which defined detecting climate concerns as a task [8]',\n",
       " ' Several researchers have employed advanced NLP and machine learning techniques for improv- ing stance detection in climate tweets with the Semeval dataset [16]',\n",
       " '  While content analysis of climate news is an established norm, the use of NLP in analyzing climate news is relatively new',\n",
       " ' Compared with the studies on social media, news analy- sis has received less attention [17]',\n",
       " ' Keller et al',\n",
       " ' in [2] applied LDA to ∼18000 climate change articles sourced from two Indian dailies',\n",
       " ' They extracted 29 themes ranging from cli- mate change impact and politics to climate science',\n",
       " ' Similarly, Bohr [3] segments 52 US newspapers based on geography, partisan bias, circulation, etc',\n",
       " ' He then uses structural topic models to discover topics and further analyses the influence of these attributes on the topics [3]',\n",
       " ' Benites-Lazaro et al',\n",
       " ' [4] use a corpus of news articles between 2007 -2017 to analyze the climate, food, and energy nexus using topic models',\n",
       " ' From a methodological perspective, most authors have used topic models, especially the popular Latent Dirichlet Allocation (LDA) [2], [3], [4]',\n",
       " ' Despite the popularity of LDA, it suf- fers from certain shortcomings',\n",
       " ' Notably, the foundational assumption of LDA is that documents are an unordered set, i',\n",
       " 'e',\n",
       " ', the words that appear in the text are independent of each other [4]',\n",
       " ' This assumption is common to all models based on the Bag of Words text representation',\n",
       " ' This assumption conflicts with linguistic theory, which suggests that words in any human language are always connected and sequen- tial [18]',\n",
       " ' Another aspect of LDA is that it needs the number of topics extracted as input right at the beginning',\n",
       " ' Know- ing the optimal number of issues at the beginning is only feasible in some contexts and hence introduces an element of subjectivity [17]',\n",
       " ' Finally, labeling the extracted topics is also left to the users’ discretion and can vary considerably between coders',\n",
       " ' Therefore for credible topic extraction as well as validation, additional information from subject matter experts becomes crucial [19]',\n",
       " ' In this sense, LDA models do not lend to automation',\n",
       " ' While some of the shortcomings, such as having to specify a specific number of topics, can be countered using matrix factorization methods such as NMF, the underlying issues related to BoW representation remain the same',\n",
       " ' The dependency on classical topic models is partic- ularly conspicuous given several algorithmic developments in the domain',\n",
       " ' For example Topic models based on embeddings have gained traction in recent years',\n",
       " ' Embeddings are dense text representations such that words with semantic similarity  VOLUME 11, 2023  26555  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 1',\n",
       " ' Diagrammatic representation of LDA',\n",
       " '  are closer in a low dimensional space [20]',\n",
       " ' Word embeddings capture the relationship between words and convey context better than the word count-based BoW representation of text',\n",
       " ' In general, embeddings-based models have reported supe- rior performance',\n",
       " ' However, the application of embeddings based topic models in social sciences has been sparse [21]',\n",
       " ' Another significant gap in social science based applications is the lack of robust experiments for model selection',\n",
       " ' Stud- ies typically choose a particular model on a apriori basis based on theoretical assumptions or prior literature contribu- tions [14], [22], [23]  III',\n",
       " ' METHODS A',\n",
       " ' LATENT DIRICHLET ALLOCATION LDA is the most basic model in probabilistic topic models and is popular in several social science disciplines [5]',\n",
       " ' LDA typically works on a Bag of Words vector representation of documents of a specific length',\n",
       " ' The learning algorithm in LDA is unsupervised and is used to discover latent semantic patterns in unstructured documents',\n",
       " ' The technique assumes that each document is generated by a complex probabilistic generative mechanism',\n",
       " 'LDA assumes that each document is created one word at a time by selecting a topic from the document’s topic distribution and then picking a word from that topic',\n",
       " ' Therefore, each document is modeled as a mixture of latent topics and each topic is modeled as a multinomial distribution of words',\n",
       " ' Thus, the learning in LDA is to estimate the parameters of the underlying mechanism that most likely generated the corpus as precisely as possible',\n",
       " ' Fig 1 provides the outline of the mathematical model  Were N — Number of words in each document',\n",
       " ' k  — Number of topics a document belongs to (a fixed number)',\n",
       " '  — Single document',\n",
       " '  D — Corpus, a collection of M documents',\n",
       " ' d zd,n —topic for the nth word in the dth document',\n",
       " ' θ — Distribution of topics for each document',\n",
       " '  βk — Distribution of words within each topic up  α, η — parameters of prior distributions over θ  to k',\n",
       " '  and β',\n",
       " '  The distribution of topics θ derives from a Dirichlet distri- bution',\n",
       " ' The use of Dirichlet allows each document to embed topic sparsity, thus mimicking real-word documents',\n",
       " ' The generative process then can be defined as the joint probability of the documents and topics as represented in  p(θ, z, w | α, β)  (1)  where α, and β refer to the hyperparameter related to the Dirichlet distribution',\n",
       " ' The joint distributions are further com- puted as the conditional distribution of the hidden topics given the set of documents',\n",
       " ' The mathematical formulation is given in Eqn 2',\n",
       " '  p(θ, z | w, α, β) =  p(θ, z, w | α, β) p(w | α, β)  (2)  However, computing the above expression can be difficult since the denominator requires the summation of all possible combinations of topics',\n",
       " ' The use of approximation methods such as Markov Chain Monte Carlo and variational inference is standard in this context',\n",
       " '  B',\n",
       " ' NON-NEGATIVE MATRIX FACTORIZATION NMF is a linear algebra-based multivariate technique based on matrix decompositions of a high dimensional vector space',\n",
       " ' The decomposed non-negative matrices represent hidden structures considered coordinate axes in a transformed low- dimensional vector space [24]',\n",
       " ' NMF considers every individ- ual document d as a vector of its terms',\n",
       " ' We then represent the term-document matrix D as follows  D = [d 1, d 2, ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' , d n] ∈ RMN  (3)  D can be decomposed into low dimensional vector spaces as represented by matrices U & V shown in equation 6',\n",
       " ' Elements of U and V are non-negative real numbers indicated by the  26556  VOLUME 11, 2023  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 2',\n",
       " ' Diagrammatic representation of NMF',\n",
       " '  matrix R',\n",
       " 'U has dimensions M and K where M denotes topics and K denotes terms or words  distribution for every cluster, thus creating topic representa- tions from clustered embeddings  U = [u1, u2, ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' , uK ] ∈ RMN V = [v1, v2, ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' , vN ] ∈ RKN D ≈ UV  (4) (5) (6)  While V represents K topics and N documents',\n",
       " ' A more intuitive representation of NMF is given in Fig 2',\n",
       " ' The quality of the formulation in Equation 4 is measured as the squared difference between the document term matrix D and U',\n",
       " 'V',\n",
       " ' This can be formally represented below',\n",
       " '  Min ||D − UV∥2  (7)  NMF learns U and V iteratively using multiplicative updates such that the error is minimized [25]  C',\n",
       " ' BERTopic BERTopic [19] is a combination of several modules',\n",
       " ' At the top it involves representing text as Embeddings using a pre-trained language model-specifically the Bidirectional Encoder Representations from Transformers(BERT)',\n",
       " ' Such a representation generates a dense matrix where text either in sentences or paragraphs is presented as numeric vectors that capture the semantic similarity',\n",
       " ' The second layer involves using a dimensionality reduction algorithm such as UMAP',\n",
       " ' to convert the embeddings into a low-dimensional space',\n",
       " ' Finally, the reduced embeddings are clustered by HDB- SCAN',\n",
       " ' or K-means algorithms',\n",
       " ' At the final layer the topic representations are done such that each cluster is assigned a topic',\n",
       " ' The topics are then represented using the TF-IDF mea- sure, which combines term frequency and inverse document frequency for rank ordering the importance of words in the text',\n",
       " ' We define the classical TF-IDF process below  Dt,d = tft,d · log  (cid:19)  (cid:18) N dft  (8)  where  D - Term document matrix Tf - Term frequency Df - Document frequency BERTopic alters the application of the TF-IDF procedure using a cluster of documents rather than an individual doc- ument',\n",
       " ' This enables the BERTopic to output the topic word  D',\n",
       " ' EVALUATION METRICS When measuring topic quality, human evaluation of a topic is considered a gold standard',\n",
       " ' However, it is a resource intensive and expensive strategy to implement',\n",
       " ' As an alter- native, recent literature has focused on the automatic mea- surement of coherence measures that closely mimic human judgment',\n",
       " ' Topic coherence generally measures the degree to which the words in a topic are semantically related and make sense together [26] Alternatively, coherence can also be defined as a measure of the internal consistency of a topic and how well it represents the documents it is gen- erated from [27]',\n",
       " '',\n",
       " ' For measuring semantic similarity, the most common methods are the Normalized Pointwise Mutual Information (NPMI',\n",
       " ')',\n",
       " ' The NPMI',\n",
       " ' method is based on context vectors j',\n",
       " ' It is constructed by extracting co-occurrences and counts within a context window of ±n tokens around [26]',\n",
       " ' Therefore mathematically, the jth element of the i has an NPMI as represented in  i of word  j',\n",
       " '  context vector equation 9',\n",
       " '  vij = NPMI (cid:0)wi, wj  (cid:1) =  \\uf8eb  \\uf8ec \\uf8ed  log  P(wi,wj)+ϵ P(wi)·P(wj)  − log (cid:0)P (cid:0)wi, wj  (cid:1) + ϵ(cid:1)  \\uf8f6  \\uf8f7 \\uf8f8 (9)  The NPMI metric measures how different in meaning the discovered topics are',\n",
       " ' A higher score in diversity reduces the redundancy among the topics',\n",
       " ' A low score suggests that the topics are repetitive and the model’s ability to abstract the themes in the corpus is low [28]',\n",
       " '  It’s worth noting that the number of topics chosen for the model can impact topic diversity',\n",
       " ' Choosing too many topics can lead to similar topics with overlapping top words while choosing too few can result in broad topics that are difficult to interpret',\n",
       " ' The most common measure for topic diversity is the ratio of unique words among the top 25 words [29]',\n",
       " '  IV',\n",
       " ' EXPERIMENTAL SETUP For the experiment, we used the 20-group news and the BBC news datasets as benchmark datasets',\n",
       " ' Both these datasets  VOLUME 11, 2023  26557  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  TABLE 1',\n",
       " ' Aggregate performance of topic models by dataset',\n",
       " '  FIGURE 3',\n",
       " ' Experiment workflow',\n",
       " '  were available in the preprocessed form in OCTIS — an open-source framework for training, evaluating, and com- paring Topic models',\n",
       " ' The Economic Times corpus was our custom dataset',\n",
       " ' We used pretrained transformers for extracting sentence embeddings',\n",
       " ' Specifically, we used the ’all- mpnet-base-v2, which provides higher and consistent performance [20]',\n",
       " ' The generic workflow of the experiment is represented in Fig 3',\n",
       " ' For evaluating and configuring hyperparameters, we used OCTIS',\n",
       " ' OCTIS provides a unified platform with several topic models, datasets, and evaluation metrics, enabling easy comparison of topic model perfor- mance',\n",
       " ' It adopts a dataset-model-metric approach to pro- vide optimal hyperparameters using a Bayesian Optimization strategy [21]',\n",
       " ' The OCTIS framework is represented in Fig 4',\n",
       " ' For the experiment we have chosen topic coherence and topic diversity measures as defined in the earlier section',\n",
       " ' Topic  FIGURE 4',\n",
       " ' OCTIS workflow',\n",
       " '  coherence metric is based on NPMI and topic diversity is based on unique words among top 10 words',\n",
       " ' The metrics are common for all the three models and is available as part of the OCTIS library',\n",
       " '  A',\n",
       " ' DATA We have used three datasets in the articles - The primary dataset actively curated by researchers and two other stan- dard news datasets for benchmarking the performance of the various topic models',\n",
       " '  1) THE ECONOMIC TIMES NEWS CORPUS The experiment dataset for the article comes from the dig- ital archives of the Economic Times, which is an estab- lished English business daily in India',\n",
       " ' We curated the dataset from news articles between 2008 and 2021',\n",
       " ' The news articles were extracted based on the keyword search ‘climate change’',\n",
       " ' After removing shot news stubs and news items with videos, the final dataset had 9774 documents',\n",
       " ' The total number of terms in the corpus approximates 5 million',\n",
       " '  2) BENCHMARK DATASETS We used the 20 newsgroups and the B',\n",
       " 'B',\n",
       " 'C',\n",
       " ' news dataset as benchmark datasets for evaluating and comparing the  26558  VOLUME 11, 2023  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 5',\n",
       " ' Panel of three figures',\n",
       " ' Figures represent the NPMI score for across values of k',\n",
       " '  VOLUME 11, 2023  26559  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 5',\n",
       " ' (Continued',\n",
       " ') Panel of three figures',\n",
       " ' Figures represent the NPMI score for across values of k',\n",
       " '  performance of topic modes',\n",
       " ' The 20 newsgroups dataset is popularly used in NLP experiments as a benchmarking dataset consisting of 20000 news articles equally distributed across 20 categories',\n",
       " ' The categories include technology, pol- itics, religion, auto, sports, etc',\n",
       " ' Similarly, the B',\n",
       " 'B',\n",
       " 'C',\n",
       " ' news dataset has 2225 news articles reported between 2004-2005 across five topical areas',\n",
       " ' The categories include Business, Sports, Technology, Entertainment, and Politics',\n",
       " '  3) RESULTS Every topic model chosen for the experiment was run on each dataset three times',\n",
       " ' In each run was defined by the choice of a random seed',\n",
       " ' In every run we selected five OCTIS values for k in multiples of 10 until 50',\n",
       " ' In other words, each topic model was run 15 times on a dataset',\n",
       " ' The performance of the model can be analyzed in two ways',\n",
       " ' First, both NPMI',\n",
       " ' and topic diversity can be aggregated at a dataset level',\n",
       " ' We average the NPMI and topic diversity scores over the three iterations and summarize it at a dataset level as shown in Table 1',\n",
       " ' BERTopic performs better than LDA and NMF across all three datasets and on both the evaluation metrics',\n",
       " ' Between LDA and NMF, we also see that LDA performs better than NMF in terms of topic diversity, while NMF does better in terms of coherence',\n",
       " ' Second, we drill down into the performance of the three mod- els at a dataset level where we calculate NPMI',\n",
       " ' with respect to k',\n",
       " ' Figure 5 is a panel of three graphs that reports the NPMI',\n",
       " ' performance',\n",
       " ' From the display, we observe that BERtopic outperforms the two other models across all three datasets',\n",
       " ' We find that NPMI',\n",
       " ' reached its maximum for different values  of k for each of the dataset',\n",
       " ' In the case of the Economic Times dataset, we find that NPMI',\n",
       " ' value increases steadily till k = 30, declines and increases again until it reaches its high at k = 50',\n",
       " ' For the other two datasets, maximum NPMI',\n",
       " ' is achieved at k = 20 and k = 40, respectively',\n",
       " '  Besides coherence, the experiment also yielded topic diver- sity scores for each model',\n",
       " ' The results are displayed in Figure 6',\n",
       " ' Once again, we observe that BERTopic performs better than the other models for the experiment and bench- mark datasets',\n",
       " ' For the Economic Times corpus we see that topic diversity is maximized at k = 20 and remains stable till k = 50',\n",
       " ' For the benchmark datasets on the other hand, we see that topic diversity declines continuously for higher values of k',\n",
       " ' Based on the results for the Economic Times dataset, we set the k value at 50, where we achieved the highest NPMI',\n",
       " ' and highest topic diversity as well',\n",
       " '  V',\n",
       " ' MAPPING THEMES USING BERTOPIC Based on the model selection experiment in the last section, we chose BERTopic as the most suitable among all the three models',\n",
       " ' We then applied it to the E',\n",
       " 'T',\n",
       " ' corpus with k = 50',\n",
       " ' One of the advantages of BERTopic is that it generates topic labels along with the topics',\n",
       " ' The labels are generated as a combination of the topic number and dominant keywords',\n",
       " ' A partial list is provided below  • 1_water_said_pollution_air • 2_countries_climate_developing_agreement • 4_energy_climate_india_coal • 5_ice_climate_said_warming  26560  VOLUME 11, 2023  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 6',\n",
       " ' Panel of three figures',\n",
       " ' Figures represent the diversity score for across values of k',\n",
       " '  VOLUME 11, 2023  26561  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 6',\n",
       " ' (Continued',\n",
       " ') Panel of three figures',\n",
       " ' Figures represent the diversity score for across values of k',\n",
       " '  • 6_climate_emissions_countries_change’ • 8_energy_solar_renewable_power • 12_monsoon_rainfall_climate_said • 15_climate_trump_paris_said • 17_species_tiger_conservation_wildlife • 39_energy_india_renewable_clean • 47_adani_mine_queensland_coal • 49_Yoga_Resolution_day  While topics might be coherent semantically, not all of them may be directly relevant to climate change',\n",
       " ' For example, in topic 49 in the above-covered news about yoga day events, speakers also mentioned climate change',\n",
       " ' Alternatively, top- ics can be merged to represent a larger theme, such as in the case of topics 2 & 6',\n",
       " ' Both topics relate to climate change negotiations with obvious differences in their point of view',\n",
       " ' Topic 2 covers news articles with a ‘developing country’ perspective, while topic six focuses on news articles which discuss country-specific emissions within the summit context',\n",
       " '  To further identify opportunities for merging similar top- ics we performed the following steps',\n",
       " ' First we examined keywords within each topic to identify topics that can be merged or need to be discarded',\n",
       " ' Second, we manually sampled the full articles within each topic to infer the context',\n",
       " ' Finally, we investigated the relationship between topics through clustering based on the cosine distance between topic embeddings',\n",
       " ' The extracted themes, and their  relationship is presented in Fig 7',\n",
       " ' At the end of the process we were able to identify topics that were peripheral to climate change and discarded them',\n",
       " ' The other topics, we grouped them into eight overarching themes',\n",
       " ' The themes were ‘Cli- mate cooperation’ and ‘Climate agreements & ’Energy & Emissions’, ‘clean energy’, Resource management, Country emissions and Business engagement ‘country emissions',\n",
       " '’ ‘Climate cooperation’ combines topics 2,13,21,27,35,42, 43,46, represents all news articles on bilateral and mul- trade relationships, and regional summits tilateral visits, (ex: SAARC, Quad), etc',\n",
       " ', where climate change is on the agenda',\n",
       " ' ’Resource management and ecosystems’ consists of news articles about natural resources management, including air, land, water, wildlife, food, etc',\n",
       " ' News stories often discuss the impact of climate change on natural resources',\n",
       " ' The theme groups topics 4,9,17,33 and ranks second in terms of overall contribution to the corpus',\n",
       " ' ‘Climate agreements & negoti- ations’ contains news articles that discuss COP summits, primarily including Paris, Glasgow, Copenhagen, and Lima',\n",
       " ' Following this we have two themes on energy’ Energy & Emissions’ and ’clean energy',\n",
       " ' While they cover energy at large, the key distinctions come from the focus of the news stories',\n",
       " ' The ‘Energy & emissions’ topic consists of articles that report the role of energy in emissions and emission reduc- tion',\n",
       " ' On the other hand ‘clean energy’ exclusively focuses on domestic action in improving the share of renewables in electricity regeneration',\n",
       " ' On a similar note, we find the  26562  VOLUME 11, 2023  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 7',\n",
       " ' Hierarchial topic cluster of the corpus',\n",
       " '  ‘country emissions’ focuses on new stories that present national emission targets and goals, and performance of dif- ferent countries',\n",
       " ' Specifically, several stories discuss India and China and their role as prominent contributors in terms of absolute emissions',\n",
       " '  Finally, we have the ‘‘Business and climate’’ themes, which subsume three topic categories- 12,22 and 49',\n",
       " ' The news stories are a collection of company agendas for reducing climate footprints, improving sustainability practices, CEO',\n",
       " ' speeches on climate change, impact on the stock, product markets, and new emerging industries such as Electric Vehi- cles & Batteries',\n",
       " ' We further track the aggregated themes over 2008 -2020, as shown in Fig 8',\n",
       " ' It is interesting to note that news across all categories spiked sharply in 2015  and 2021',\n",
       " ' These spikes may be attributed to the greater atten- tion given to climate change around the Paris COP and the Glasgow COP',\n",
       " '  There are also interesting insights that emerge from how different themes trend',\n",
       " ' Before 2014 news on climate sum- mits and negotiations dominated other categories',\n",
       " ' Post 2014, we see that news on climate cooperation has the highest contribution to the overall corpus',\n",
       " ' In tandem with climate cooperation, we also observe the increase in reporting on the impact of climate change on resources and ecosystems',\n",
       " ' We also observe that both the categories of energy news (emissions & clean energy) move together',\n",
       " ' Finally we, see that news around business themes has gained traction since 2012 but has increased significantly from 2017 onwards',\n",
       " '  VOLUME 11, 2023  26563  \\x0cS',\n",
       " ' Umamaheswaran et al',\n",
       " ': Mapping Climate Themes From 2008-2021—An Analysis of Business News Using Topic Models  FIGURE 8',\n",
       " ' Trends in climate change news within themes',\n",
       " '  VI',\n",
       " ' DISCUSSION AND CONCLUSION The scientific consensus on climate change is unassail- able and there is an urgent need for stronger climate action globally',\n",
       " ' However, countries continue to grapple with socio-political challenges which have emerged as a deterrent to effective climate action',\n",
       " ' The social and political discourse around climate change involves the public, the state, and the scientific community, with media as the common platform',\n",
       " ' Understanding how media reports on climate change gives a glimpse into this diverse stakeholder system’s perspectives, debates, and responses',\n",
       " '  However, the tools used to understand the climate change discourse have been derived mainly from traditional quan- titative and qualitative research',\n",
       " ' Natural Language based techniques can augment the existing toolbox to unfold and extract insights from the climate change debate',\n",
       " ' This study is a step towards achieving integration of NLP in social science research and, more specifically, within the domain of climate change discourse [17], [30]',\n",
       " ' Another important contribution of the study is the demonstration and use of embeddings based topic models BERTopic as an alternative to LDA and NMF',\n",
       " ' Additionally, it also furthers the use of experimental frameworks such as OCTIS for evaluating the performance of topic models',\n",
       " ' Finally, through the longitudinal analysis of climate news the study provides a developing country perspective to the literature on climate change discourse  The analysis in our study includes two components',\n",
       " ' First an experimental study on topic models to determine relative performance',\n",
       " 'Second is the actual mapping of the climate news based on the model selected in the experiment Our experiments show that embedding models perform signif- icantly better than the other topic models',\n",
       " ' The results are consistent with other recent comparative studies [31], [32]',\n",
       " '  We further demonstrate the application of BERTopic by training it on the Economic Times corpus and discovering news frames and their evolution',\n",
       " ' Results suggest that news frames around climate negotiation have been dominant before 2014 while climate cooperation gains importance post 2014',\n",
       " ' We also found that climate frames related to domestic action also has gained traction in recent years',\n",
       " '  Stakeholder consensus is an essential aspect of climate action',\n",
       " ' Given the multiple stakeholders in climate change discourse, media analysis can play a crucial role in gauging the engagement of citizens, the state, and the commercial sector',\n",
       " ' Using NLP techniques in this context can scale up information processing and augment insights derived from other research methods, thus helping make informed policy decisions for countering climate change']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.DataFrame(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In terms of absolute emissions, India surpass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in 2018 to become the third-largest emitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Solving this wicked problem calls for climate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>While extant literature has focused significa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>Results suggest that news frames around clima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>We also found that climate frames related to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Stakeholder consensus is an essential aspect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Given the multiple stakeholders in climate ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Using NLP techniques in this context can scal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>342 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0     In terms of absolute emissions, India surpass...\n",
       "1                                                    S\n",
       "2          in 2018 to become the third-largest emitter\n",
       "3     Solving this wicked problem calls for climate...\n",
       "4     While extant literature has focused significa...\n",
       "..                                                 ...\n",
       "337   Results suggest that news frames around clima...\n",
       "338   We also found that climate frames related to ...\n",
       "339    Stakeholder consensus is an essential aspect...\n",
       "340   Given the multiple stakeholders in climate ch...\n",
       "341   Using NLP techniques in this context can scal...\n",
       "\n",
       "[342 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={0:'text'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=data.text.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=pd.Series(new_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= data2.apply(lambda x: re.sub(r'[^a-zA-Z/s]+',' ',str(x)).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "data_final=data2.apply(lambda x:\" \".join([word for word in word_tokenize(x) if word not in stop_words and len(word)>2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         terms absolute emissions india surpassed china\n",
       "1                                                       \n",
       "2                           become third largest emitter\n",
       "3      solving wicked problem calls climate action ac...\n",
       "4      extant literature focused significantly role g...\n",
       "                             ...                        \n",
       "337    results suggest news frames around climate neg...\n",
       "338    also found climate frames related domestic act...\n",
       "339    stakeholder consensus essential aspect climate...\n",
       "340    given multiple stakeholders climate change dis...\n",
       "341    using nlp techniques context scale information...\n",
       "Length: 342, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=[]\n",
    "for sentence in data_final:\n",
    "    word_list.extend(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['terms', 'absolute', 'emissions', 'india', 'surpassed', 'china'],\n",
       " [],\n",
       " ['become', 'third', 'largest', 'emitter'],\n",
       " ['solving',\n",
       "  'wicked',\n",
       "  'problem',\n",
       "  'calls',\n",
       "  'climate',\n",
       "  'action',\n",
       "  'across',\n",
       "  'stakeholder',\n",
       "  'spectrum',\n",
       "  'involving',\n",
       "  'governments',\n",
       "  'business',\n",
       "  'communities',\n",
       "  'citizens'],\n",
       " ['extant',\n",
       "  'literature',\n",
       "  'focused',\n",
       "  'significantly',\n",
       "  'role',\n",
       "  'governments',\n",
       "  'individual',\n",
       "  'perceptions',\n",
       "  'business',\n",
       "  'sector',\n",
       "  'needs',\n",
       "  'represented'],\n",
       " ['study',\n",
       "  'consider',\n",
       "  'business',\n",
       "  'news',\n",
       "  'media',\n",
       "  'platform',\n",
       "  'reflects',\n",
       "  'industry',\n",
       "  'engagement',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'source',\n",
       "  'information',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'business',\n",
       "  'decision',\n",
       "  'makers'],\n",
       " ['hence',\n",
       "  'understanding',\n",
       "  'topic',\n",
       "  'themes',\n",
       "  'nexus',\n",
       "  'climate',\n",
       "  'business',\n",
       "  'important',\n",
       "  'evaluate',\n",
       "  'business',\n",
       "  'sector',\n",
       "  'stance',\n",
       "  'towards',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'evolved'],\n",
       " ['work',\n",
       "  'explores',\n",
       "  'business',\n",
       "  'news',\n",
       "  'related',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'using',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'techniques'],\n",
       " ['first',\n",
       "  'experiment',\n",
       "  'three',\n",
       "  'topic',\n",
       "  'modeling',\n",
       "  'techniques',\n",
       "  'lda',\n",
       "  'nmf',\n",
       "  'bertopic',\n",
       "  'business',\n",
       "  'news',\n",
       "  'two',\n",
       "  'benchmark',\n",
       "  'news',\n",
       "  'datasets'],\n",
       " ['test',\n",
       "  'data',\n",
       "  'derived',\n",
       "  'digital',\n",
       "  'news',\n",
       "  'archives',\n",
       "  'economic',\n",
       "  'times',\n",
       "  'india',\n",
       "  'leading',\n",
       "  'business',\n",
       "  'news',\n",
       "  'daily'],\n",
       " ['evaluate',\n",
       "  'performance',\n",
       "  'based',\n",
       "  'quantitative',\n",
       "  'metrics',\n",
       "  'commonly',\n",
       "  'used',\n",
       "  'topic',\n",
       "  'models'],\n",
       " ['choose',\n",
       "  'algorithm',\n",
       "  'provides',\n",
       "  'highest',\n",
       "  'precision',\n",
       "  'climate',\n",
       "  'specific',\n",
       "  'information',\n",
       "  'represented',\n",
       "  'test',\n",
       "  'dataset'],\n",
       " ['apply',\n",
       "  'algorithm',\n",
       "  'best',\n",
       "  'performance',\n",
       "  'evaluated',\n",
       "  'experiment',\n",
       "  'large',\n",
       "  'corpus',\n",
       "  'indian',\n",
       "  'climate',\n",
       "  'news'],\n",
       " [],\n",
       " ['spanning'],\n",
       " ['present',\n",
       "  'different',\n",
       "  'themes',\n",
       "  'including',\n",
       "  'industry',\n",
       "  'engagement',\n",
       "  'evolved',\n",
       "  'last',\n",
       "  'two',\n",
       "  'decades'],\n",
       " ['results',\n",
       "  'suggest',\n",
       "  'climate',\n",
       "  'cooperation',\n",
       "  'highest',\n",
       "  'contribution',\n",
       "  'corpus',\n",
       "  'themes',\n",
       "  'resource',\n",
       "  'management',\n",
       "  'energy',\n",
       "  'business',\n",
       "  'gaining',\n",
       "  'traction',\n",
       "  'recent',\n",
       "  'years'],\n",
       " ['index',\n",
       "  'terms',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'media',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'nlp',\n",
       "  'computational',\n",
       "  'social',\n",
       "  'sciences',\n",
       "  'experiment'],\n",
       " [],\n",
       " ['introduction', 'climate', 'change', 'critical', 'issue', 'millennium'],\n",
       " ['india',\n",
       "  'high',\n",
       "  'stakes',\n",
       "  'global',\n",
       "  'debate',\n",
       "  'since',\n",
       "  'third',\n",
       "  'largest',\n",
       "  'emitter',\n",
       "  'also',\n",
       "  'high',\n",
       "  'climate',\n",
       "  'vulnerability'],\n",
       " ['recent',\n",
       "  'update',\n",
       "  'paris',\n",
       "  'pledge',\n",
       "  'country',\n",
       "  'committed',\n",
       "  'reducing',\n",
       "  'emission',\n",
       "  'intensity',\n",
       "  'levels'],\n",
       " ['emission',\n",
       "  'reduction',\n",
       "  'scale',\n",
       "  'requires',\n",
       "  'solid',\n",
       "  'public',\n",
       "  'support',\n",
       "  'across',\n",
       "  'stakeholder',\n",
       "  'spectrum',\n",
       "  'includ',\n",
       "  'ing',\n",
       "  'citizens',\n",
       "  'enterprises'],\n",
       " ['role',\n",
       "  'commercial',\n",
       "  'sector',\n",
       "  'particularly',\n",
       "  'relevant',\n",
       "  'given',\n",
       "  'contribution',\n",
       "  'country',\n",
       "  'level',\n",
       "  'emissions'],\n",
       " ['associate',\n",
       "  'editor',\n",
       "  'coordinating',\n",
       "  'review',\n",
       "  'manuscript',\n",
       "  'approving',\n",
       "  'publication',\n",
       "  'arianna',\n",
       "  'dulizia'],\n",
       " ['practical',\n",
       "  'approach',\n",
       "  'toward',\n",
       "  'understanding',\n",
       "  'different',\n",
       "  'stakeholders',\n",
       "  'engage',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'analysis',\n",
       "  'news',\n",
       "  'coverage'],\n",
       " ['news',\n",
       "  'media',\n",
       "  'reflect',\n",
       "  'contemporary',\n",
       "  'public',\n",
       "  'discourse',\n",
       "  'business',\n",
       "  'newspapers',\n",
       "  'tra',\n",
       "  'ditionally',\n",
       "  'report',\n",
       "  'news',\n",
       "  'relevant',\n",
       "  'business',\n",
       "  'decision',\n",
       "  'makers'],\n",
       " ['therefore',\n",
       "  'mapping',\n",
       "  'topics',\n",
       "  'pertinent',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'business',\n",
       "  'media',\n",
       "  'issues',\n",
       "  'evolved',\n",
       "  'help',\n",
       "  'understand',\n",
       "  'commercial',\n",
       "  'sector',\n",
       "  'engages',\n",
       "  'issue',\n",
       "  'climate',\n",
       "  'change'],\n",
       " ['automated',\n",
       "  'content',\n",
       "  'analysis',\n",
       "  'large',\n",
       "  'corpora',\n",
       "  'estab',\n",
       "  'lished',\n",
       "  'technique',\n",
       "  'social',\n",
       "  'sciences'],\n",
       " ['specifically',\n",
       "  'use',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'extracting',\n",
       "  'latent',\n",
       "  'topics',\n",
       "  'newspapers',\n",
       "  'discussion',\n",
       "  'forums',\n",
       "  'social',\n",
       "  'media',\n",
       "  'content',\n",
       "  'gaining',\n",
       "  'traction',\n",
       "  'extant',\n",
       "  'literature'],\n",
       " ['among',\n",
       "  'different',\n",
       "  'work',\n",
       "  'licensed',\n",
       "  'creative',\n",
       "  'commons',\n",
       "  'attribution',\n",
       "  'noncommercial',\n",
       "  'noderivatives'],\n",
       " ['license'],\n",
       " ['information', 'see', 'https', '//creativecommons'],\n",
       " ['org/licenses/by', 'nd/'],\n",
       " ['volume'],\n",
       " ['umamaheswaran'],\n",
       " ['mapping',\n",
       "  'climate',\n",
       "  'themes',\n",
       "  'analysis',\n",
       "  'business',\n",
       "  'news',\n",
       "  'using',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'nlp',\n",
       "  'techniques',\n",
       "  'topic',\n",
       "  'mod',\n",
       "  'eling',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'task',\n",
       "  'groups',\n",
       "  'documents',\n",
       "  'words',\n",
       "  'similar',\n",
       "  'meanings'],\n",
       " ['topic',\n",
       "  'models',\n",
       "  'yield',\n",
       "  'topics',\n",
       "  'ranked',\n",
       "  'based',\n",
       "  'specific',\n",
       "  'terms',\n",
       "  'relevance',\n",
       "  'topic'],\n",
       " ['topic',\n",
       "  'emerges',\n",
       "  'automatically',\n",
       "  'without',\n",
       "  'prior',\n",
       "  'annotation',\n",
       "  'labeling',\n",
       "  'raw',\n",
       "  'data'],\n",
       " ['sense',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'form',\n",
       "  'important',\n",
       "  'category',\n",
       "  'unsupervised',\n",
       "  'tech',\n",
       "  'niques',\n",
       "  'within',\n",
       "  'larger',\n",
       "  'field',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'process',\n",
       "  'ing'],\n",
       " ['many',\n",
       "  'mathematical',\n",
       "  'approaches',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'even',\n",
       "  'higher',\n",
       "  'spread',\n",
       "  'algorithms',\n",
       "  'learn',\n",
       "  'param',\n",
       "  'eters',\n",
       "  'model',\n",
       "  'approaches'],\n",
       " ['conventional',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'range',\n",
       "  'linear',\n",
       "  'algebraic',\n",
       "  'based',\n",
       "  'techniques',\n",
       "  'lsa',\n",
       "  'popular',\n",
       "  'probabilistic',\n",
       "  'models',\n",
       "  'like',\n",
       "  'lda',\n",
       "  'plsa'],\n",
       " ['metrics',\n",
       "  'decomposition',\n",
       "  'based',\n",
       "  'methods',\n",
       "  'nmf',\n",
       "  'also',\n",
       "  'mainstay',\n",
       "  'nlp',\n",
       "  'applications'],\n",
       " ['underlying',\n",
       "  'bag',\n",
       "  'words',\n",
       "  'bow',\n",
       "  'approach',\n",
       "  'distinguishing',\n",
       "  'factor',\n",
       "  'among',\n",
       "  'approaches'],\n",
       " ['however',\n",
       "  'bow',\n",
       "  'based',\n",
       "  'topic',\n",
       "  'model',\n",
       "  'based',\n",
       "  'unrealistic',\n",
       "  'assumption',\n",
       "  'words',\n",
       "  'independent',\n",
       "  'relies',\n",
       "  'frequencies',\n",
       "  'word',\n",
       "  'occurrences'],\n",
       " ['recent',\n",
       "  'development',\n",
       "  'nlp',\n",
       "  'representation',\n",
       "  'text',\n",
       "  'using',\n",
       "  'embeddings'],\n",
       " ['embeddings',\n",
       "  'context',\n",
       "  'nlp',\n",
       "  'dense',\n",
       "  'representations',\n",
       "  'units',\n",
       "  'text',\n",
       "  'vector',\n",
       "  'space',\n",
       "  'way',\n",
       "  'semantic',\n",
       "  'similarity',\n",
       "  'units',\n",
       "  'text',\n",
       "  'captured'],\n",
       " ['although',\n",
       "  'automated',\n",
       "  'content',\n",
       "  'analysis',\n",
       "  'gaining',\n",
       "  'importance',\n",
       "  'within',\n",
       "  'climate',\n",
       "  'discourse',\n",
       "  'literature',\n",
       "  'overreliance',\n",
       "  'probabilistic',\n",
       "  'topic',\n",
       "  'models'],\n",
       " ['importantly',\n",
       "  'choice',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'lda',\n",
       "  'motivated',\n",
       "  'popular',\n",
       "  'use',\n",
       "  'backed',\n",
       "  'experiments',\n",
       "  'evaluate',\n",
       "  'performance',\n",
       "  'metrics',\n",
       "  'especially',\n",
       "  'social',\n",
       "  'sciences'],\n",
       " ['given',\n",
       "  'background',\n",
       "  'objective',\n",
       "  'article',\n",
       "  'evaluate',\n",
       "  'performance',\n",
       "  'conventional',\n",
       "  'bow',\n",
       "  'based',\n",
       "  'models',\n",
       "  'embeddings',\n",
       "  'based',\n",
       "  'models'],\n",
       " ['specifically', 'run', 'experiments', 'using', 'lda', 'nmf', 'bertopic'],\n",
       " ['choose',\n",
       "  'appropriate',\n",
       "  'model',\n",
       "  'based',\n",
       "  'evaluation',\n",
       "  'metrics',\n",
       "  'use',\n",
       "  'map',\n",
       "  'themes',\n",
       "  'within',\n",
       "  'economic',\n",
       "  'times',\n",
       "  'corpus'],\n",
       " ['article', 'organized', 'follows'],\n",
       " ['section',\n",
       "  'introduces',\n",
       "  'literature',\n",
       "  'automated',\n",
       "  'content',\n",
       "  'analysis',\n",
       "  'climate',\n",
       "  'news',\n",
       "  'discusses',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'used',\n",
       "  'literature'],\n",
       " ['section', 'iii', 'presents', 'data', 'methods'],\n",
       " ['section', 'offers', 'details', 'experiment', 'results'],\n",
       " ['section',\n",
       "  'presents',\n",
       "  'results',\n",
       "  'final',\n",
       "  'implantation',\n",
       "  'chosen',\n",
       "  'model'],\n",
       " [],\n",
       " ['literature',\n",
       "  'review',\n",
       "  'adoption',\n",
       "  'nlp',\n",
       "  'techniques',\n",
       "  'social',\n",
       "  'sciences',\n",
       "  'seen',\n",
       "  'significant',\n",
       "  'growth',\n",
       "  'recent',\n",
       "  'decade'],\n",
       " ['language', 'social', 'construct', 'considered', 'proxy', 'behav', 'ior'],\n",
       " ['expressed',\n",
       "  'people',\n",
       "  'language',\n",
       "  'may',\n",
       "  'contain',\n",
       "  'rich',\n",
       "  'latent',\n",
       "  'information',\n",
       "  'help',\n",
       "  'identify',\n",
       "  'several',\n",
       "  'dimen',\n",
       "  'sions',\n",
       "  'traits',\n",
       "  'behavior'],\n",
       " ['therefore',\n",
       "  'rapid',\n",
       "  'growth',\n",
       "  'nlp',\n",
       "  'methods',\n",
       "  'within',\n",
       "  'social',\n",
       "  'sciences',\n",
       "  'surprising'],\n",
       " ['social',\n",
       "  'science',\n",
       "  'applications',\n",
       "  'nlp',\n",
       "  'range',\n",
       "  'understand',\n",
       "  'ing',\n",
       "  'political',\n",
       "  'affiliations',\n",
       "  'voter',\n",
       "  'intentions',\n",
       "  'health',\n",
       "  'care',\n",
       "  'delivery',\n",
       "  'media',\n",
       "  'monitoring',\n",
       "  'improving',\n",
       "  'public',\n",
       "  'policy',\n",
       "  'implementation'],\n",
       " ['use',\n",
       "  'nlp',\n",
       "  'mining',\n",
       "  'social',\n",
       "  'discourse',\n",
       "  'around',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'still',\n",
       "  'nascent',\n",
       "  'broadly',\n",
       "  'categorized',\n",
       "  'based',\n",
       "  'analy',\n",
       "  'sis',\n",
       "  'social',\n",
       "  'media',\n",
       "  'platforms',\n",
       "  'online',\n",
       "  'news',\n",
       "  'scientific',\n",
       "  'technical',\n",
       "  'documents'],\n",
       " ['several',\n",
       "  'studies',\n",
       "  'analyze',\n",
       "  'societal',\n",
       "  'stance',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'study',\n",
       "  'twitter',\n",
       "  'microblogs'],\n",
       " ['example', 'elgasem'],\n",
       " ['combine',\n",
       "  'sentiment',\n",
       "  'networking',\n",
       "  'analysis',\n",
       "  'identify',\n",
       "  'climate',\n",
       "  'skeptics',\n",
       "  'accept',\n",
       "  'communities'],\n",
       " ['stance',\n",
       "  'detection',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'tweets',\n",
       "  'emerged',\n",
       "  'separate',\n",
       "  'subdomain',\n",
       "  'release',\n",
       "  'semeval',\n",
       "  'dataset',\n",
       "  'defined',\n",
       "  'detecting',\n",
       "  'climate',\n",
       "  'concerns',\n",
       "  'task'],\n",
       " ['several',\n",
       "  'researchers',\n",
       "  'employed',\n",
       "  'advanced',\n",
       "  'nlp',\n",
       "  'machine',\n",
       "  'learning',\n",
       "  'techniques',\n",
       "  'improv',\n",
       "  'ing',\n",
       "  'stance',\n",
       "  'detection',\n",
       "  'climate',\n",
       "  'tweets',\n",
       "  'semeval',\n",
       "  'dataset'],\n",
       " ['content',\n",
       "  'analysis',\n",
       "  'climate',\n",
       "  'news',\n",
       "  'established',\n",
       "  'norm',\n",
       "  'use',\n",
       "  'nlp',\n",
       "  'analyzing',\n",
       "  'climate',\n",
       "  'news',\n",
       "  'relatively',\n",
       "  'new'],\n",
       " ['compared',\n",
       "  'studies',\n",
       "  'social',\n",
       "  'media',\n",
       "  'news',\n",
       "  'analy',\n",
       "  'sis',\n",
       "  'received',\n",
       "  'less',\n",
       "  'attention'],\n",
       " ['keller'],\n",
       " ['applied',\n",
       "  'lda',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'articles',\n",
       "  'sourced',\n",
       "  'two',\n",
       "  'indian',\n",
       "  'dailies'],\n",
       " ['extracted',\n",
       "  'themes',\n",
       "  'ranging',\n",
       "  'cli',\n",
       "  'mate',\n",
       "  'change',\n",
       "  'impact',\n",
       "  'politics',\n",
       "  'climate',\n",
       "  'science'],\n",
       " ['similarly',\n",
       "  'bohr',\n",
       "  'segments',\n",
       "  'newspapers',\n",
       "  'based',\n",
       "  'geography',\n",
       "  'partisan',\n",
       "  'bias',\n",
       "  'circulation',\n",
       "  'etc'],\n",
       " ['uses',\n",
       "  'structural',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'discover',\n",
       "  'topics',\n",
       "  'analyses',\n",
       "  'influence',\n",
       "  'attributes',\n",
       "  'topics'],\n",
       " ['benites', 'lazaro'],\n",
       " ['use',\n",
       "  'corpus',\n",
       "  'news',\n",
       "  'articles',\n",
       "  'analyze',\n",
       "  'climate',\n",
       "  'food',\n",
       "  'energy',\n",
       "  'nexus',\n",
       "  'using',\n",
       "  'topic',\n",
       "  'models'],\n",
       " ['methodological',\n",
       "  'perspective',\n",
       "  'authors',\n",
       "  'used',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'especially',\n",
       "  'popular',\n",
       "  'latent',\n",
       "  'dirichlet',\n",
       "  'allocation',\n",
       "  'lda'],\n",
       " ['despite', 'popularity', 'lda', 'suf', 'fers', 'certain', 'shortcomings'],\n",
       " ['notably',\n",
       "  'foundational',\n",
       "  'assumption',\n",
       "  'lda',\n",
       "  'documents',\n",
       "  'unordered',\n",
       "  'set'],\n",
       " [],\n",
       " ['words', 'appear', 'text', 'independent'],\n",
       " ['assumption',\n",
       "  'common',\n",
       "  'models',\n",
       "  'based',\n",
       "  'bag',\n",
       "  'words',\n",
       "  'text',\n",
       "  'representation'],\n",
       " ['assumption',\n",
       "  'conflicts',\n",
       "  'linguistic',\n",
       "  'theory',\n",
       "  'suggests',\n",
       "  'words',\n",
       "  'human',\n",
       "  'language',\n",
       "  'always',\n",
       "  'connected',\n",
       "  'sequen',\n",
       "  'tial'],\n",
       " ['another',\n",
       "  'aspect',\n",
       "  'lda',\n",
       "  'needs',\n",
       "  'number',\n",
       "  'topics',\n",
       "  'extracted',\n",
       "  'input',\n",
       "  'right',\n",
       "  'beginning'],\n",
       " ['know',\n",
       "  'ing',\n",
       "  'optimal',\n",
       "  'number',\n",
       "  'issues',\n",
       "  'beginning',\n",
       "  'feasible',\n",
       "  'contexts',\n",
       "  'hence',\n",
       "  'introduces',\n",
       "  'element',\n",
       "  'subjectivity'],\n",
       " ['finally',\n",
       "  'labeling',\n",
       "  'extracted',\n",
       "  'topics',\n",
       "  'also',\n",
       "  'left',\n",
       "  'users',\n",
       "  'discretion',\n",
       "  'vary',\n",
       "  'considerably',\n",
       "  'coders'],\n",
       " ['therefore',\n",
       "  'credible',\n",
       "  'topic',\n",
       "  'extraction',\n",
       "  'well',\n",
       "  'validation',\n",
       "  'additional',\n",
       "  'information',\n",
       "  'subject',\n",
       "  'matter',\n",
       "  'experts',\n",
       "  'becomes',\n",
       "  'crucial'],\n",
       " ['sense', 'lda', 'models', 'lend', 'automation'],\n",
       " ['shortcomings',\n",
       "  'specify',\n",
       "  'specific',\n",
       "  'number',\n",
       "  'topics',\n",
       "  'countered',\n",
       "  'using',\n",
       "  'matrix',\n",
       "  'factorization',\n",
       "  'methods',\n",
       "  'nmf',\n",
       "  'underlying',\n",
       "  'issues',\n",
       "  'related',\n",
       "  'bow',\n",
       "  'representation',\n",
       "  'remain'],\n",
       " ['dependency',\n",
       "  'classical',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'partic',\n",
       "  'ularly',\n",
       "  'conspicuous',\n",
       "  'given',\n",
       "  'several',\n",
       "  'algorithmic',\n",
       "  'developments',\n",
       "  'domain'],\n",
       " ['example',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'based',\n",
       "  'embeddings',\n",
       "  'gained',\n",
       "  'traction',\n",
       "  'recent',\n",
       "  'years'],\n",
       " ['embeddings',\n",
       "  'dense',\n",
       "  'text',\n",
       "  'representations',\n",
       "  'words',\n",
       "  'semantic',\n",
       "  'similarity',\n",
       "  'volume'],\n",
       " ['umamaheswaran'],\n",
       " ['mapping',\n",
       "  'climate',\n",
       "  'themes',\n",
       "  'analysis',\n",
       "  'business',\n",
       "  'news',\n",
       "  'using',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'figure'],\n",
       " ['diagrammatic', 'representation', 'lda'],\n",
       " ['closer', 'low', 'dimensional', 'space'],\n",
       " ['word',\n",
       "  'embeddings',\n",
       "  'capture',\n",
       "  'relationship',\n",
       "  'words',\n",
       "  'convey',\n",
       "  'context',\n",
       "  'better',\n",
       "  'word',\n",
       "  'count',\n",
       "  'based',\n",
       "  'bow',\n",
       "  'representation',\n",
       "  'text'],\n",
       " ['general',\n",
       "  'embeddings',\n",
       "  'based',\n",
       "  'models',\n",
       "  'reported',\n",
       "  'supe',\n",
       "  'rior',\n",
       "  'performance'],\n",
       " ['however',\n",
       "  'application',\n",
       "  'embeddings',\n",
       "  'based',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'social',\n",
       "  'sciences',\n",
       "  'sparse'],\n",
       " ['another',\n",
       "  'significant',\n",
       "  'gap',\n",
       "  'social',\n",
       "  'science',\n",
       "  'based',\n",
       "  'applications',\n",
       "  'lack',\n",
       "  'robust',\n",
       "  'experiments',\n",
       "  'model',\n",
       "  'selection'],\n",
       " ['stud',\n",
       "  'ies',\n",
       "  'typically',\n",
       "  'choose',\n",
       "  'particular',\n",
       "  'model',\n",
       "  'apriori',\n",
       "  'basis',\n",
       "  'based',\n",
       "  'theoretical',\n",
       "  'assumptions',\n",
       "  'prior',\n",
       "  'literature',\n",
       "  'contribu',\n",
       "  'tions',\n",
       "  'iii'],\n",
       " ['methods'],\n",
       " ['latent',\n",
       "  'dirichlet',\n",
       "  'allocation',\n",
       "  'lda',\n",
       "  'basic',\n",
       "  'model',\n",
       "  'probabilistic',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'popular',\n",
       "  'several',\n",
       "  'social',\n",
       "  'science',\n",
       "  'disciplines'],\n",
       " ['lda',\n",
       "  'typically',\n",
       "  'works',\n",
       "  'bag',\n",
       "  'words',\n",
       "  'vector',\n",
       "  'representation',\n",
       "  'documents',\n",
       "  'specific',\n",
       "  'length'],\n",
       " ['learning',\n",
       "  'algorithm',\n",
       "  'lda',\n",
       "  'unsupervised',\n",
       "  'used',\n",
       "  'discover',\n",
       "  'latent',\n",
       "  'semantic',\n",
       "  'patterns',\n",
       "  'unstructured',\n",
       "  'documents'],\n",
       " ['technique',\n",
       "  'assumes',\n",
       "  'document',\n",
       "  'generated',\n",
       "  'complex',\n",
       "  'probabilistic',\n",
       "  'generative',\n",
       "  'mechanism'],\n",
       " ['lda',\n",
       "  'assumes',\n",
       "  'document',\n",
       "  'created',\n",
       "  'one',\n",
       "  'word',\n",
       "  'time',\n",
       "  'selecting',\n",
       "  'topic',\n",
       "  'document',\n",
       "  'topic',\n",
       "  'distribution',\n",
       "  'picking',\n",
       "  'word',\n",
       "  'topic'],\n",
       " ['therefore',\n",
       "  'document',\n",
       "  'modeled',\n",
       "  'mixture',\n",
       "  'latent',\n",
       "  'topics',\n",
       "  'topic',\n",
       "  'modeled',\n",
       "  'multinomial',\n",
       "  'distribution',\n",
       "  'words'],\n",
       " ['thus',\n",
       "  'learning',\n",
       "  'lda',\n",
       "  'estimate',\n",
       "  'parameters',\n",
       "  'underlying',\n",
       "  'mechanism',\n",
       "  'likely',\n",
       "  'generated',\n",
       "  'corpus',\n",
       "  'precisely',\n",
       "  'possible'],\n",
       " ['fig',\n",
       "  'provides',\n",
       "  'outline',\n",
       "  'mathematical',\n",
       "  'model',\n",
       "  'number',\n",
       "  'words',\n",
       "  'document'],\n",
       " ['number', 'topics', 'document', 'belongs', 'fixed', 'number'],\n",
       " ['single', 'document'],\n",
       " ['corpus', 'collection', 'documents'],\n",
       " ['topic', 'nth', 'word', 'dth', 'document'],\n",
       " ['distribution', 'topics', 'document'],\n",
       " ['distribution',\n",
       "  'words',\n",
       "  'within',\n",
       "  'topic',\n",
       "  'parameters',\n",
       "  'prior',\n",
       "  'distributions'],\n",
       " [],\n",
       " ['distribution', 'topics', 'derives', 'dirichlet', 'distri', 'bution'],\n",
       " ['use',\n",
       "  'dirichlet',\n",
       "  'allows',\n",
       "  'document',\n",
       "  'embed',\n",
       "  'topic',\n",
       "  'sparsity',\n",
       "  'thus',\n",
       "  'mimicking',\n",
       "  'real',\n",
       "  'word',\n",
       "  'documents'],\n",
       " ['generative',\n",
       "  'process',\n",
       "  'defined',\n",
       "  'joint',\n",
       "  'probability',\n",
       "  'documents',\n",
       "  'topics',\n",
       "  'represented',\n",
       "  'refer',\n",
       "  'hyperparameter',\n",
       "  'related',\n",
       "  'dirichlet',\n",
       "  'distribution'],\n",
       " ['joint',\n",
       "  'distributions',\n",
       "  'com',\n",
       "  'puted',\n",
       "  'conditional',\n",
       "  'distribution',\n",
       "  'hidden',\n",
       "  'topics',\n",
       "  'given',\n",
       "  'set',\n",
       "  'documents'],\n",
       " ['mathematical', 'formulation', 'given', 'eqn'],\n",
       " ['however',\n",
       "  'computing',\n",
       "  'expression',\n",
       "  'difficult',\n",
       "  'since',\n",
       "  'denominator',\n",
       "  'requires',\n",
       "  'summation',\n",
       "  'possible',\n",
       "  'combinations',\n",
       "  'topics'],\n",
       " ['use',\n",
       "  'approximation',\n",
       "  'methods',\n",
       "  'markov',\n",
       "  'chain',\n",
       "  'monte',\n",
       "  'carlo',\n",
       "  'variational',\n",
       "  'inference',\n",
       "  'standard',\n",
       "  'context'],\n",
       " [],\n",
       " ['non',\n",
       "  'negative',\n",
       "  'matrix',\n",
       "  'factorization',\n",
       "  'nmf',\n",
       "  'linear',\n",
       "  'algebra',\n",
       "  'based',\n",
       "  'multivariate',\n",
       "  'technique',\n",
       "  'based',\n",
       "  'matrix',\n",
       "  'decompositions',\n",
       "  'high',\n",
       "  'dimensional',\n",
       "  'vector',\n",
       "  'space'],\n",
       " ['decomposed',\n",
       "  'non',\n",
       "  'negative',\n",
       "  'matrices',\n",
       "  'represent',\n",
       "  'hidden',\n",
       "  'structures',\n",
       "  'considered',\n",
       "  'coordinate',\n",
       "  'axes',\n",
       "  'transformed',\n",
       "  'low',\n",
       "  'dimensional',\n",
       "  'vector',\n",
       "  'space'],\n",
       " ['nmf',\n",
       "  'considers',\n",
       "  'every',\n",
       "  'individ',\n",
       "  'ual',\n",
       "  'document',\n",
       "  'vector',\n",
       "  'terms'],\n",
       " ['represent', 'term', 'document', 'matrix', 'follows'],\n",
       " [],\n",
       " [],\n",
       " ['rmn',\n",
       "  'decomposed',\n",
       "  'low',\n",
       "  'dimensional',\n",
       "  'vector',\n",
       "  'spaces',\n",
       "  'represented',\n",
       "  'matrices',\n",
       "  'shown',\n",
       "  'equation'],\n",
       " ['elements', 'non', 'negative', 'real', 'numbers', 'indicated', 'volume'],\n",
       " ['umamaheswaran'],\n",
       " ['mapping',\n",
       "  'climate',\n",
       "  'themes',\n",
       "  'analysis',\n",
       "  'business',\n",
       "  'news',\n",
       "  'using',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'figure'],\n",
       " ['diagrammatic', 'representation', 'nmf'],\n",
       " ['matrix'],\n",
       " ['dimensions',\n",
       "  'denotes',\n",
       "  'topics',\n",
       "  'denotes',\n",
       "  'terms',\n",
       "  'words',\n",
       "  'distribution',\n",
       "  'every',\n",
       "  'cluster',\n",
       "  'thus',\n",
       "  'creating',\n",
       "  'topic',\n",
       "  'representa',\n",
       "  'tions',\n",
       "  'clustered',\n",
       "  'embeddings'],\n",
       " [],\n",
       " [],\n",
       " ['rmn'],\n",
       " [],\n",
       " [],\n",
       " ['rkn', 'represents', 'topics', 'documents'],\n",
       " ['intuitive', 'representation', 'nmf', 'given', 'fig'],\n",
       " ['quality',\n",
       "  'formulation',\n",
       "  'equation',\n",
       "  'measured',\n",
       "  'squared',\n",
       "  'difference',\n",
       "  'document',\n",
       "  'term',\n",
       "  'matrix'],\n",
       " [],\n",
       " ['formally', 'represented'],\n",
       " ['min',\n",
       "  'nmf',\n",
       "  'learns',\n",
       "  'iteratively',\n",
       "  'using',\n",
       "  'multiplicative',\n",
       "  'updates',\n",
       "  'error',\n",
       "  'minimized'],\n",
       " ['bertopic', 'bertopic', 'combination', 'several', 'modules'],\n",
       " ['top',\n",
       "  'involves',\n",
       "  'representing',\n",
       "  'text',\n",
       "  'embeddings',\n",
       "  'using',\n",
       "  'pre',\n",
       "  'trained',\n",
       "  'language',\n",
       "  'model',\n",
       "  'specifically',\n",
       "  'bidirectional',\n",
       "  'encoder',\n",
       "  'representations',\n",
       "  'transformers',\n",
       "  'bert'],\n",
       " ['representation',\n",
       "  'generates',\n",
       "  'dense',\n",
       "  'matrix',\n",
       "  'text',\n",
       "  'either',\n",
       "  'sentences',\n",
       "  'paragraphs',\n",
       "  'presented',\n",
       "  'numeric',\n",
       "  'vectors',\n",
       "  'capture',\n",
       "  'semantic',\n",
       "  'similarity'],\n",
       " ['second',\n",
       "  'layer',\n",
       "  'involves',\n",
       "  'using',\n",
       "  'dimensionality',\n",
       "  'reduction',\n",
       "  'algorithm',\n",
       "  'umap'],\n",
       " ['convert', 'embeddings', 'low', 'dimensional', 'space'],\n",
       " ['finally', 'reduced', 'embeddings', 'clustered', 'hdb', 'scan'],\n",
       " ['means', 'algorithms'],\n",
       " ['final',\n",
       "  'layer',\n",
       "  'topic',\n",
       "  'representations',\n",
       "  'done',\n",
       "  'cluster',\n",
       "  'assigned',\n",
       "  'topic'],\n",
       " ['topics',\n",
       "  'represented',\n",
       "  'using',\n",
       "  'idf',\n",
       "  'mea',\n",
       "  'sure',\n",
       "  'combines',\n",
       "  'term',\n",
       "  'frequency',\n",
       "  'inverse',\n",
       "  'document',\n",
       "  'frequency',\n",
       "  'rank',\n",
       "  'ordering',\n",
       "  'importance',\n",
       "  'words',\n",
       "  'text'],\n",
       " ['define',\n",
       "  'classical',\n",
       "  'idf',\n",
       "  'process',\n",
       "  'tft',\n",
       "  'log',\n",
       "  'cid',\n",
       "  'cid',\n",
       "  'dft',\n",
       "  'term',\n",
       "  'document',\n",
       "  'matrix',\n",
       "  'term',\n",
       "  'frequency',\n",
       "  'document',\n",
       "  'frequency',\n",
       "  'bertopic',\n",
       "  'alters',\n",
       "  'application',\n",
       "  'idf',\n",
       "  'procedure',\n",
       "  'using',\n",
       "  'cluster',\n",
       "  'documents',\n",
       "  'rather',\n",
       "  'individual',\n",
       "  'doc',\n",
       "  'ument'],\n",
       " ['enables', 'bertopic', 'output', 'topic', 'word'],\n",
       " ['evaluation',\n",
       "  'metrics',\n",
       "  'measuring',\n",
       "  'topic',\n",
       "  'quality',\n",
       "  'human',\n",
       "  'evaluation',\n",
       "  'topic',\n",
       "  'considered',\n",
       "  'gold',\n",
       "  'standard'],\n",
       " ['however', 'resource', 'intensive', 'expensive', 'strategy', 'implement'],\n",
       " ['alter',\n",
       "  'native',\n",
       "  'recent',\n",
       "  'literature',\n",
       "  'focused',\n",
       "  'automatic',\n",
       "  'mea',\n",
       "  'surement',\n",
       "  'coherence',\n",
       "  'measures',\n",
       "  'closely',\n",
       "  'mimic',\n",
       "  'human',\n",
       "  'judgment'],\n",
       " ['topic',\n",
       "  'coherence',\n",
       "  'generally',\n",
       "  'measures',\n",
       "  'degree',\n",
       "  'words',\n",
       "  'topic',\n",
       "  'semantically',\n",
       "  'related',\n",
       "  'make',\n",
       "  'sense',\n",
       "  'together',\n",
       "  'alternatively',\n",
       "  'coherence',\n",
       "  'also',\n",
       "  'defined',\n",
       "  'measure',\n",
       "  'internal',\n",
       "  'consistency',\n",
       "  'topic',\n",
       "  'well',\n",
       "  'represents',\n",
       "  'documents',\n",
       "  'gen',\n",
       "  'erated'],\n",
       " [],\n",
       " ['measuring',\n",
       "  'semantic',\n",
       "  'similarity',\n",
       "  'common',\n",
       "  'methods',\n",
       "  'normalized',\n",
       "  'pointwise',\n",
       "  'mutual',\n",
       "  'information',\n",
       "  'npmi'],\n",
       " [],\n",
       " ['npmi'],\n",
       " ['method', 'based', 'context', 'vectors'],\n",
       " ['constructed',\n",
       "  'extracting',\n",
       "  'occurrences',\n",
       "  'counts',\n",
       "  'within',\n",
       "  'context',\n",
       "  'window',\n",
       "  'tokens',\n",
       "  'around'],\n",
       " ['therefore',\n",
       "  'mathematically',\n",
       "  'jth',\n",
       "  'element',\n",
       "  'npmi',\n",
       "  'represented',\n",
       "  'word'],\n",
       " ['context', 'vector', 'equation'],\n",
       " ['vij',\n",
       "  'npmi',\n",
       "  'cid',\n",
       "  'cid',\n",
       "  'log',\n",
       "  'log',\n",
       "  'cid',\n",
       "  'cid',\n",
       "  'cid',\n",
       "  'cid',\n",
       "  'npmi',\n",
       "  'metric',\n",
       "  'measures',\n",
       "  'different',\n",
       "  'meaning',\n",
       "  'discovered',\n",
       "  'topics'],\n",
       " ['higher', 'score', 'diversity', 'reduces', 'redundancy', 'among', 'topics'],\n",
       " ['low',\n",
       "  'score',\n",
       "  'suggests',\n",
       "  'topics',\n",
       "  'repetitive',\n",
       "  'model',\n",
       "  'ability',\n",
       "  'abstract',\n",
       "  'themes',\n",
       "  'corpus',\n",
       "  'low'],\n",
       " ['worth',\n",
       "  'noting',\n",
       "  'number',\n",
       "  'topics',\n",
       "  'chosen',\n",
       "  'model',\n",
       "  'impact',\n",
       "  'topic',\n",
       "  'diversity'],\n",
       " ['choosing',\n",
       "  'many',\n",
       "  'topics',\n",
       "  'lead',\n",
       "  'similar',\n",
       "  'topics',\n",
       "  'overlapping',\n",
       "  'top',\n",
       "  'words',\n",
       "  'choosing',\n",
       "  'result',\n",
       "  'broad',\n",
       "  'topics',\n",
       "  'difficult',\n",
       "  'interpret'],\n",
       " ['common',\n",
       "  'measure',\n",
       "  'topic',\n",
       "  'diversity',\n",
       "  'ratio',\n",
       "  'unique',\n",
       "  'words',\n",
       "  'among',\n",
       "  'top',\n",
       "  'words'],\n",
       " [],\n",
       " ['experimental',\n",
       "  'setup',\n",
       "  'experiment',\n",
       "  'used',\n",
       "  'group',\n",
       "  'news',\n",
       "  'bbc',\n",
       "  'news',\n",
       "  'datasets',\n",
       "  'benchmark',\n",
       "  'datasets'],\n",
       " ['datasets', 'volume'],\n",
       " ['umamaheswaran'],\n",
       " ['mapping',\n",
       "  'climate',\n",
       "  'themes',\n",
       "  'analysis',\n",
       "  'business',\n",
       "  'news',\n",
       "  'using',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'table'],\n",
       " ['aggregate', 'performance', 'topic', 'models', 'dataset'],\n",
       " ['figure'],\n",
       " ['experiment', 'workflow'],\n",
       " ['available',\n",
       "  'preprocessed',\n",
       "  'form',\n",
       "  'octis',\n",
       "  'open',\n",
       "  'source',\n",
       "  'framework',\n",
       "  'training',\n",
       "  'evaluating',\n",
       "  'com',\n",
       "  'paring',\n",
       "  'topic',\n",
       "  'models'],\n",
       " ['economic', 'times', 'corpus', 'custom', 'dataset'],\n",
       " ['used',\n",
       "  'pretrained',\n",
       "  'transformers',\n",
       "  'extracting',\n",
       "  'sentence',\n",
       "  'embeddings'],\n",
       " ['specifically',\n",
       "  'used',\n",
       "  'mpnet',\n",
       "  'base',\n",
       "  'provides',\n",
       "  'higher',\n",
       "  'consistent',\n",
       "  'performance'],\n",
       " ['generic', 'workflow', 'experiment', 'represented', 'fig'],\n",
       " ['evaluating', 'configuring', 'hyperparameters', 'used', 'octis'],\n",
       " ['octis',\n",
       "  'provides',\n",
       "  'unified',\n",
       "  'platform',\n",
       "  'several',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'datasets',\n",
       "  'evaluation',\n",
       "  'metrics',\n",
       "  'enabling',\n",
       "  'easy',\n",
       "  'comparison',\n",
       "  'topic',\n",
       "  'model',\n",
       "  'perfor',\n",
       "  'mance'],\n",
       " ['adopts',\n",
       "  'dataset',\n",
       "  'model',\n",
       "  'metric',\n",
       "  'approach',\n",
       "  'pro',\n",
       "  'vide',\n",
       "  'optimal',\n",
       "  'hyperparameters',\n",
       "  'using',\n",
       "  'bayesian',\n",
       "  'optimization',\n",
       "  'strategy'],\n",
       " ['octis', 'framework', 'represented', 'fig'],\n",
       " ['experiment',\n",
       "  'chosen',\n",
       "  'topic',\n",
       "  'coherence',\n",
       "  'topic',\n",
       "  'diversity',\n",
       "  'measures',\n",
       "  'defined',\n",
       "  'earlier',\n",
       "  'section'],\n",
       " ['topic', 'figure'],\n",
       " ['octis', 'workflow'],\n",
       " ['coherence',\n",
       "  'metric',\n",
       "  'based',\n",
       "  'npmi',\n",
       "  'topic',\n",
       "  'diversity',\n",
       "  'based',\n",
       "  'unique',\n",
       "  'words',\n",
       "  'among',\n",
       "  'top',\n",
       "  'words'],\n",
       " ['metrics',\n",
       "  'common',\n",
       "  'three',\n",
       "  'models',\n",
       "  'available',\n",
       "  'part',\n",
       "  'octis',\n",
       "  'library'],\n",
       " [],\n",
       " ['data',\n",
       "  'used',\n",
       "  'three',\n",
       "  'datasets',\n",
       "  'articles',\n",
       "  'primary',\n",
       "  'dataset',\n",
       "  'actively',\n",
       "  'curated',\n",
       "  'researchers',\n",
       "  'two',\n",
       "  'stan',\n",
       "  'dard',\n",
       "  'news',\n",
       "  'datasets',\n",
       "  'benchmarking',\n",
       "  'performance',\n",
       "  'various',\n",
       "  'topic',\n",
       "  'models'],\n",
       " ['economic',\n",
       "  'times',\n",
       "  'news',\n",
       "  'corpus',\n",
       "  'experiment',\n",
       "  'dataset',\n",
       "  'article',\n",
       "  'comes',\n",
       "  'dig',\n",
       "  'ital',\n",
       "  'archives',\n",
       "  'economic',\n",
       "  'times',\n",
       "  'estab',\n",
       "  'lished',\n",
       "  'english',\n",
       "  'business',\n",
       "  'daily',\n",
       "  'india'],\n",
       " ['curated', 'dataset', 'news', 'articles'],\n",
       " ['news',\n",
       "  'articles',\n",
       "  'extracted',\n",
       "  'based',\n",
       "  'keyword',\n",
       "  'search',\n",
       "  'climate',\n",
       "  'change'],\n",
       " ['removing',\n",
       "  'shot',\n",
       "  'news',\n",
       "  'stubs',\n",
       "  'news',\n",
       "  'items',\n",
       "  'videos',\n",
       "  'final',\n",
       "  'dataset',\n",
       "  'documents'],\n",
       " ['total', 'number', 'terms', 'corpus', 'approximates', 'million'],\n",
       " ['benchmark', 'datasets', 'used', 'newsgroups'],\n",
       " [],\n",
       " [],\n",
       " ['news',\n",
       "  'dataset',\n",
       "  'benchmark',\n",
       "  'datasets',\n",
       "  'evaluating',\n",
       "  'comparing',\n",
       "  'volume'],\n",
       " ['umamaheswaran'],\n",
       " ['mapping',\n",
       "  'climate',\n",
       "  'themes',\n",
       "  'analysis',\n",
       "  'business',\n",
       "  'news',\n",
       "  'using',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'figure'],\n",
       " ['panel', 'three', 'figures'],\n",
       " ['figures', 'represent', 'npmi', 'score', 'across', 'values'],\n",
       " ['volume'],\n",
       " ['umamaheswaran'],\n",
       " ['mapping',\n",
       "  'climate',\n",
       "  'themes',\n",
       "  'analysis',\n",
       "  'business',\n",
       "  'news',\n",
       "  'using',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'figure'],\n",
       " ['continued'],\n",
       " ['panel', 'three', 'figures'],\n",
       " ['figures', 'represent', 'npmi', 'score', 'across', 'values'],\n",
       " ['performance', 'topic', 'modes'],\n",
       " ['newsgroups',\n",
       "  'dataset',\n",
       "  'popularly',\n",
       "  'used',\n",
       "  'nlp',\n",
       "  'experiments',\n",
       "  'benchmarking',\n",
       "  'dataset',\n",
       "  'consisting',\n",
       "  'news',\n",
       "  'articles',\n",
       "  'equally',\n",
       "  'distributed',\n",
       "  'across',\n",
       "  'categories'],\n",
       " ['categories',\n",
       "  'include',\n",
       "  'technology',\n",
       "  'pol',\n",
       "  'itics',\n",
       "  'religion',\n",
       "  'auto',\n",
       "  'sports',\n",
       "  'etc'],\n",
       " ['similarly'],\n",
       " [],\n",
       " [],\n",
       " ['news',\n",
       "  'dataset',\n",
       "  'news',\n",
       "  'articles',\n",
       "  'reported',\n",
       "  'across',\n",
       "  'five',\n",
       "  'topical',\n",
       "  'areas'],\n",
       " ['categories',\n",
       "  'include',\n",
       "  'business',\n",
       "  'sports',\n",
       "  'technology',\n",
       "  'entertainment',\n",
       "  'politics'],\n",
       " ['results',\n",
       "  'every',\n",
       "  'topic',\n",
       "  'model',\n",
       "  'chosen',\n",
       "  'experiment',\n",
       "  'run',\n",
       "  'dataset',\n",
       "  'three',\n",
       "  'times'],\n",
       " ['run', 'defined', 'choice', 'random', 'seed'],\n",
       " ['every', 'run', 'selected', 'five', 'octis', 'values', 'multiples'],\n",
       " ['words', 'topic', 'model', 'run', 'times', 'dataset'],\n",
       " ['performance', 'model', 'analyzed', 'two', 'ways'],\n",
       " ['first', 'npmi'],\n",
       " ['topic', 'diversity', 'aggregated', 'dataset', 'level'],\n",
       " ['average',\n",
       "  'npmi',\n",
       "  'topic',\n",
       "  'diversity',\n",
       "  'scores',\n",
       "  'three',\n",
       "  'iterations',\n",
       "  'summarize',\n",
       "  'dataset',\n",
       "  'level',\n",
       "  'shown',\n",
       "  'table'],\n",
       " ['bertopic',\n",
       "  'performs',\n",
       "  'better',\n",
       "  'lda',\n",
       "  'nmf',\n",
       "  'across',\n",
       "  'three',\n",
       "  'datasets',\n",
       "  'evaluation',\n",
       "  'metrics'],\n",
       " ['lda',\n",
       "  'nmf',\n",
       "  'also',\n",
       "  'see',\n",
       "  'lda',\n",
       "  'performs',\n",
       "  'better',\n",
       "  'nmf',\n",
       "  'terms',\n",
       "  'topic',\n",
       "  'diversity',\n",
       "  'nmf',\n",
       "  'better',\n",
       "  'terms',\n",
       "  'coherence'],\n",
       " ['second',\n",
       "  'drill',\n",
       "  'performance',\n",
       "  'three',\n",
       "  'mod',\n",
       "  'els',\n",
       "  'dataset',\n",
       "  'level',\n",
       "  'calculate',\n",
       "  'npmi'],\n",
       " ['respect'],\n",
       " ['figure', 'panel', 'three', 'graphs', 'reports', 'npmi'],\n",
       " ['performance'],\n",
       " ['display',\n",
       "  'observe',\n",
       "  'bertopic',\n",
       "  'outperforms',\n",
       "  'two',\n",
       "  'models',\n",
       "  'across',\n",
       "  'three',\n",
       "  'datasets'],\n",
       " ['find', 'npmi'],\n",
       " ['reached', 'maximum', 'different', 'values', 'dataset'],\n",
       " ['case', 'economic', 'times', 'dataset', 'find', 'npmi'],\n",
       " ['value',\n",
       "  'increases',\n",
       "  'steadily',\n",
       "  'till',\n",
       "  'declines',\n",
       "  'increases',\n",
       "  'reaches',\n",
       "  'high'],\n",
       " ['two', 'datasets', 'maximum', 'npmi'],\n",
       " ['achieved', 'respectively'],\n",
       " ['besides',\n",
       "  'coherence',\n",
       "  'experiment',\n",
       "  'also',\n",
       "  'yielded',\n",
       "  'topic',\n",
       "  'diver',\n",
       "  'sity',\n",
       "  'scores',\n",
       "  'model'],\n",
       " ['results', 'displayed', 'figure'],\n",
       " ['observe',\n",
       "  'bertopic',\n",
       "  'performs',\n",
       "  'better',\n",
       "  'models',\n",
       "  'experiment',\n",
       "  'bench',\n",
       "  'mark',\n",
       "  'datasets'],\n",
       " ['economic',\n",
       "  'times',\n",
       "  'corpus',\n",
       "  'see',\n",
       "  'topic',\n",
       "  'diversity',\n",
       "  'maximized',\n",
       "  'remains',\n",
       "  'stable',\n",
       "  'till'],\n",
       " ['benchmark',\n",
       "  'datasets',\n",
       "  'hand',\n",
       "  'see',\n",
       "  'topic',\n",
       "  'diversity',\n",
       "  'declines',\n",
       "  'continuously',\n",
       "  'higher',\n",
       "  'values'],\n",
       " ['based',\n",
       "  'results',\n",
       "  'economic',\n",
       "  'times',\n",
       "  'dataset',\n",
       "  'set',\n",
       "  'value',\n",
       "  'achieved',\n",
       "  'highest',\n",
       "  'npmi'],\n",
       " ['highest', 'topic', 'diversity', 'well'],\n",
       " [],\n",
       " ['mapping',\n",
       "  'themes',\n",
       "  'using',\n",
       "  'bertopic',\n",
       "  'based',\n",
       "  'model',\n",
       "  'selection',\n",
       "  'experiment',\n",
       "  'last',\n",
       "  'section',\n",
       "  'chose',\n",
       "  'bertopic',\n",
       "  'suitable',\n",
       "  'among',\n",
       "  'three',\n",
       "  'models'],\n",
       " ['applied'],\n",
       " [],\n",
       " ['corpus'],\n",
       " ['one',\n",
       "  'advantages',\n",
       "  'bertopic',\n",
       "  'generates',\n",
       "  'topic',\n",
       "  'labels',\n",
       "  'along',\n",
       "  'topics'],\n",
       " ['labels',\n",
       "  'generated',\n",
       "  'combination',\n",
       "  'topic',\n",
       "  'number',\n",
       "  'dominant',\n",
       "  'keywords'],\n",
       " ['partial',\n",
       "  'list',\n",
       "  'provided',\n",
       "  'water',\n",
       "  'said',\n",
       "  'pollution',\n",
       "  'air',\n",
       "  'countries',\n",
       "  'climate',\n",
       "  'developing',\n",
       "  'agreement',\n",
       "  'energy',\n",
       "  'climate',\n",
       "  'india',\n",
       "  'coal',\n",
       "  'ice',\n",
       "  'climate',\n",
       "  'said',\n",
       "  'warming',\n",
       "  'volume'],\n",
       " ['umamaheswaran'],\n",
       " ['mapping',\n",
       "  'climate',\n",
       "  'themes',\n",
       "  'analysis',\n",
       "  'business',\n",
       "  'news',\n",
       "  'using',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'figure'],\n",
       " ['panel', 'three', 'figures'],\n",
       " ['figures', 'represent', 'diversity', 'score', 'across', 'values'],\n",
       " ['volume'],\n",
       " ['umamaheswaran'],\n",
       " ['mapping',\n",
       "  'climate',\n",
       "  'themes',\n",
       "  'analysis',\n",
       "  'business',\n",
       "  'news',\n",
       "  'using',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'figure'],\n",
       " ['continued'],\n",
       " ['panel', 'three', 'figures'],\n",
       " ['figures', 'represent', 'diversity', 'score', 'across', 'values'],\n",
       " ['climate',\n",
       "  'emissions',\n",
       "  'countries',\n",
       "  'change',\n",
       "  'energy',\n",
       "  'solar',\n",
       "  'renewable',\n",
       "  'power',\n",
       "  'monsoon',\n",
       "  'rainfall',\n",
       "  'climate',\n",
       "  'said',\n",
       "  'climate',\n",
       "  'trump',\n",
       "  'paris',\n",
       "  'said',\n",
       "  'species',\n",
       "  'tiger',\n",
       "  'conservation',\n",
       "  'wildlife',\n",
       "  'energy',\n",
       "  'india',\n",
       "  'renewable',\n",
       "  'clean',\n",
       "  'adani',\n",
       "  'mine',\n",
       "  'queensland',\n",
       "  'coal',\n",
       "  'yoga',\n",
       "  'resolution',\n",
       "  'day',\n",
       "  'topics',\n",
       "  'might',\n",
       "  'coherent',\n",
       "  'semantically',\n",
       "  'may',\n",
       "  'directly',\n",
       "  'relevant',\n",
       "  'climate',\n",
       "  'change'],\n",
       " ['example',\n",
       "  'topic',\n",
       "  'covered',\n",
       "  'news',\n",
       "  'yoga',\n",
       "  'day',\n",
       "  'events',\n",
       "  'speakers',\n",
       "  'also',\n",
       "  'mentioned',\n",
       "  'climate',\n",
       "  'change'],\n",
       " ['alternatively',\n",
       "  'top',\n",
       "  'ics',\n",
       "  'merged',\n",
       "  'represent',\n",
       "  'larger',\n",
       "  'theme',\n",
       "  'case',\n",
       "  'topics'],\n",
       " ['topics',\n",
       "  'relate',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'negotiations',\n",
       "  'obvious',\n",
       "  'differences',\n",
       "  'point',\n",
       "  'view'],\n",
       " ['topic',\n",
       "  'covers',\n",
       "  'news',\n",
       "  'articles',\n",
       "  'developing',\n",
       "  'country',\n",
       "  'perspective',\n",
       "  'topic',\n",
       "  'six',\n",
       "  'focuses',\n",
       "  'news',\n",
       "  'articles',\n",
       "  'discuss',\n",
       "  'country',\n",
       "  'specific',\n",
       "  'emissions',\n",
       "  'within',\n",
       "  'summit',\n",
       "  'context'],\n",
       " ['identify',\n",
       "  'opportunities',\n",
       "  'merging',\n",
       "  'similar',\n",
       "  'top',\n",
       "  'ics',\n",
       "  'performed',\n",
       "  'following',\n",
       "  'steps'],\n",
       " ['first',\n",
       "  'examined',\n",
       "  'keywords',\n",
       "  'within',\n",
       "  'topic',\n",
       "  'identify',\n",
       "  'topics',\n",
       "  'merged',\n",
       "  'need',\n",
       "  'discarded'],\n",
       " ['second',\n",
       "  'manually',\n",
       "  'sampled',\n",
       "  'full',\n",
       "  'articles',\n",
       "  'within',\n",
       "  'topic',\n",
       "  'infer',\n",
       "  'context'],\n",
       " ['finally',\n",
       "  'investigated',\n",
       "  'relationship',\n",
       "  'topics',\n",
       "  'clustering',\n",
       "  'based',\n",
       "  'cosine',\n",
       "  'distance',\n",
       "  'topic',\n",
       "  'embeddings'],\n",
       " ['extracted', 'themes', 'relationship', 'presented', 'fig'],\n",
       " ['end',\n",
       "  'process',\n",
       "  'able',\n",
       "  'identify',\n",
       "  'topics',\n",
       "  'peripheral',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'discarded'],\n",
       " ['topics', 'grouped', 'eight', 'overarching', 'themes'],\n",
       " ['themes',\n",
       "  'cli',\n",
       "  'mate',\n",
       "  'cooperation',\n",
       "  'climate',\n",
       "  'agreements',\n",
       "  'energy',\n",
       "  'emissions',\n",
       "  'clean',\n",
       "  'energy',\n",
       "  'resource',\n",
       "  'management',\n",
       "  'country',\n",
       "  'emissions',\n",
       "  'business',\n",
       "  'engagement',\n",
       "  'country',\n",
       "  'emissions'],\n",
       " ['climate',\n",
       "  'cooperation',\n",
       "  'combines',\n",
       "  'topics',\n",
       "  'represents',\n",
       "  'news',\n",
       "  'articles',\n",
       "  'bilateral',\n",
       "  'mul',\n",
       "  'trade',\n",
       "  'relationships',\n",
       "  'regional',\n",
       "  'summits',\n",
       "  'tilateral',\n",
       "  'visits',\n",
       "  'saarc',\n",
       "  'quad',\n",
       "  'etc'],\n",
       " ['climate', 'change', 'agenda'],\n",
       " ['resource',\n",
       "  'management',\n",
       "  'ecosystems',\n",
       "  'consists',\n",
       "  'news',\n",
       "  'articles',\n",
       "  'natural',\n",
       "  'resources',\n",
       "  'management',\n",
       "  'including',\n",
       "  'air',\n",
       "  'land',\n",
       "  'water',\n",
       "  'wildlife',\n",
       "  'food',\n",
       "  'etc'],\n",
       " ['news',\n",
       "  'stories',\n",
       "  'often',\n",
       "  'discuss',\n",
       "  'impact',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'natural',\n",
       "  'resources'],\n",
       " ['theme',\n",
       "  'groups',\n",
       "  'topics',\n",
       "  'ranks',\n",
       "  'second',\n",
       "  'terms',\n",
       "  'overall',\n",
       "  'contribution',\n",
       "  'corpus'],\n",
       " ['climate',\n",
       "  'agreements',\n",
       "  'negoti',\n",
       "  'ations',\n",
       "  'contains',\n",
       "  'news',\n",
       "  'articles',\n",
       "  'discuss',\n",
       "  'cop',\n",
       "  'summits',\n",
       "  'primarily',\n",
       "  'including',\n",
       "  'paris',\n",
       "  'glasgow',\n",
       "  'copenhagen',\n",
       "  'lima'],\n",
       " ['following',\n",
       "  'two',\n",
       "  'themes',\n",
       "  'energy',\n",
       "  'energy',\n",
       "  'emissions',\n",
       "  'clean',\n",
       "  'energy'],\n",
       " ['cover',\n",
       "  'energy',\n",
       "  'large',\n",
       "  'key',\n",
       "  'distinctions',\n",
       "  'come',\n",
       "  'focus',\n",
       "  'news',\n",
       "  'stories'],\n",
       " ['energy',\n",
       "  'emissions',\n",
       "  'topic',\n",
       "  'consists',\n",
       "  'articles',\n",
       "  'report',\n",
       "  'role',\n",
       "  'energy',\n",
       "  'emissions',\n",
       "  'emission',\n",
       "  'reduc',\n",
       "  'tion'],\n",
       " ['hand',\n",
       "  'clean',\n",
       "  'energy',\n",
       "  'exclusively',\n",
       "  'focuses',\n",
       "  'domestic',\n",
       "  'action',\n",
       "  'improving',\n",
       "  'share',\n",
       "  'renewables',\n",
       "  'electricity',\n",
       "  'regeneration'],\n",
       " ['similar', 'note', 'find', 'volume'],\n",
       " ['umamaheswaran'],\n",
       " ['mapping',\n",
       "  'climate',\n",
       "  'themes',\n",
       "  'analysis',\n",
       "  'business',\n",
       "  'news',\n",
       "  'using',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'figure'],\n",
       " ['hierarchial', 'topic', 'cluster', 'corpus'],\n",
       " ['country',\n",
       "  'emissions',\n",
       "  'focuses',\n",
       "  'new',\n",
       "  'stories',\n",
       "  'present',\n",
       "  'national',\n",
       "  'emission',\n",
       "  'targets',\n",
       "  'goals',\n",
       "  'performance',\n",
       "  'dif',\n",
       "  'ferent',\n",
       "  'countries'],\n",
       " ['specifically',\n",
       "  'several',\n",
       "  'stories',\n",
       "  'discuss',\n",
       "  'india',\n",
       "  'china',\n",
       "  'role',\n",
       "  'prominent',\n",
       "  'contributors',\n",
       "  'terms',\n",
       "  'absolute',\n",
       "  'emissions'],\n",
       " ['finally',\n",
       "  'business',\n",
       "  'climate',\n",
       "  'themes',\n",
       "  'subsume',\n",
       "  'three',\n",
       "  'topic',\n",
       "  'categories'],\n",
       " ['news',\n",
       "  'stories',\n",
       "  'collection',\n",
       "  'company',\n",
       "  'agendas',\n",
       "  'reducing',\n",
       "  'climate',\n",
       "  'footprints',\n",
       "  'improving',\n",
       "  'sustainability',\n",
       "  'practices',\n",
       "  'ceo'],\n",
       " ['speeches',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'impact',\n",
       "  'stock',\n",
       "  'product',\n",
       "  'markets',\n",
       "  'new',\n",
       "  'emerging',\n",
       "  'industries',\n",
       "  'electric',\n",
       "  'vehi',\n",
       "  'cles',\n",
       "  'batteries'],\n",
       " ['track', 'aggregated', 'themes', 'shown', 'fig'],\n",
       " ['interesting', 'note', 'news', 'across', 'categories', 'spiked', 'sharply'],\n",
       " ['spikes',\n",
       "  'may',\n",
       "  'attributed',\n",
       "  'greater',\n",
       "  'atten',\n",
       "  'tion',\n",
       "  'given',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'around',\n",
       "  'paris',\n",
       "  'cop',\n",
       "  'glasgow',\n",
       "  'cop'],\n",
       " ['also', 'interesting', 'insights', 'emerge', 'different', 'themes', 'trend'],\n",
       " ['news', 'climate', 'sum', 'mits', 'negotiations', 'dominated', 'categories'],\n",
       " ['post',\n",
       "  'see',\n",
       "  'news',\n",
       "  'climate',\n",
       "  'cooperation',\n",
       "  'highest',\n",
       "  'contribution',\n",
       "  'overall',\n",
       "  'corpus'],\n",
       " ['tandem',\n",
       "  'climate',\n",
       "  'cooperation',\n",
       "  'also',\n",
       "  'observe',\n",
       "  'increase',\n",
       "  'reporting',\n",
       "  'impact',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'resources',\n",
       "  'ecosystems'],\n",
       " ['also',\n",
       "  'observe',\n",
       "  'categories',\n",
       "  'energy',\n",
       "  'news',\n",
       "  'emissions',\n",
       "  'clean',\n",
       "  'energy',\n",
       "  'move',\n",
       "  'together'],\n",
       " ['finally',\n",
       "  'see',\n",
       "  'news',\n",
       "  'around',\n",
       "  'business',\n",
       "  'themes',\n",
       "  'gained',\n",
       "  'traction',\n",
       "  'since',\n",
       "  'increased',\n",
       "  'significantly',\n",
       "  'onwards'],\n",
       " ['volume'],\n",
       " ['umamaheswaran'],\n",
       " ['mapping',\n",
       "  'climate',\n",
       "  'themes',\n",
       "  'analysis',\n",
       "  'business',\n",
       "  'news',\n",
       "  'using',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'figure'],\n",
       " ['trends', 'climate', 'change', 'news', 'within', 'themes'],\n",
       " [],\n",
       " ['discussion',\n",
       "  'conclusion',\n",
       "  'scientific',\n",
       "  'consensus',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'unassail',\n",
       "  'able',\n",
       "  'urgent',\n",
       "  'need',\n",
       "  'stronger',\n",
       "  'climate',\n",
       "  'action',\n",
       "  'globally'],\n",
       " ['however',\n",
       "  'countries',\n",
       "  'continue',\n",
       "  'grapple',\n",
       "  'socio',\n",
       "  'political',\n",
       "  'challenges',\n",
       "  'emerged',\n",
       "  'deterrent',\n",
       "  'effective',\n",
       "  'climate',\n",
       "  'action'],\n",
       " ['social',\n",
       "  'political',\n",
       "  'discourse',\n",
       "  'around',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'involves',\n",
       "  'public',\n",
       "  'state',\n",
       "  'scientific',\n",
       "  'community',\n",
       "  'media',\n",
       "  'common',\n",
       "  'platform'],\n",
       " ['understanding',\n",
       "  'media',\n",
       "  'reports',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'gives',\n",
       "  'glimpse',\n",
       "  'diverse',\n",
       "  'stakeholder',\n",
       "  'system',\n",
       "  'perspectives',\n",
       "  'debates',\n",
       "  'responses'],\n",
       " ['however',\n",
       "  'tools',\n",
       "  'used',\n",
       "  'understand',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'discourse',\n",
       "  'derived',\n",
       "  'mainly',\n",
       "  'traditional',\n",
       "  'quan',\n",
       "  'titative',\n",
       "  'qualitative',\n",
       "  'research'],\n",
       " ['natural',\n",
       "  'language',\n",
       "  'based',\n",
       "  'techniques',\n",
       "  'augment',\n",
       "  'existing',\n",
       "  'toolbox',\n",
       "  'unfold',\n",
       "  'extract',\n",
       "  'insights',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'debate'],\n",
       " ['study',\n",
       "  'step',\n",
       "  'towards',\n",
       "  'achieving',\n",
       "  'integration',\n",
       "  'nlp',\n",
       "  'social',\n",
       "  'science',\n",
       "  'research',\n",
       "  'specifically',\n",
       "  'within',\n",
       "  'domain',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'discourse'],\n",
       " ['another',\n",
       "  'important',\n",
       "  'contribution',\n",
       "  'study',\n",
       "  'demonstration',\n",
       "  'use',\n",
       "  'embeddings',\n",
       "  'based',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'bertopic',\n",
       "  'alternative',\n",
       "  'lda',\n",
       "  'nmf'],\n",
       " ['additionally',\n",
       "  'also',\n",
       "  'furthers',\n",
       "  'use',\n",
       "  'experimental',\n",
       "  'frameworks',\n",
       "  'octis',\n",
       "  'evaluating',\n",
       "  'performance',\n",
       "  'topic',\n",
       "  'models'],\n",
       " ['finally',\n",
       "  'longitudinal',\n",
       "  'analysis',\n",
       "  'climate',\n",
       "  'news',\n",
       "  'study',\n",
       "  'provides',\n",
       "  'developing',\n",
       "  'country',\n",
       "  'perspective',\n",
       "  'literature',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'discourse',\n",
       "  'analysis',\n",
       "  'study',\n",
       "  'includes',\n",
       "  'two',\n",
       "  'components'],\n",
       " ['first',\n",
       "  'experimental',\n",
       "  'study',\n",
       "  'topic',\n",
       "  'models',\n",
       "  'determine',\n",
       "  'relative',\n",
       "  'performance'],\n",
       " ['second',\n",
       "  'actual',\n",
       "  'mapping',\n",
       "  'climate',\n",
       "  'news',\n",
       "  'based',\n",
       "  'model',\n",
       "  'selected',\n",
       "  'experiment',\n",
       "  'experiments',\n",
       "  'show',\n",
       "  'embedding',\n",
       "  'models',\n",
       "  'perform',\n",
       "  'signif',\n",
       "  'icantly',\n",
       "  'better',\n",
       "  'topic',\n",
       "  'models'],\n",
       " ['results', 'consistent', 'recent', 'comparative', 'studies'],\n",
       " ['demonstrate',\n",
       "  'application',\n",
       "  'bertopic',\n",
       "  'training',\n",
       "  'economic',\n",
       "  'times',\n",
       "  'corpus',\n",
       "  'discovering',\n",
       "  'news',\n",
       "  'frames',\n",
       "  'evolution'],\n",
       " ['results',\n",
       "  'suggest',\n",
       "  'news',\n",
       "  'frames',\n",
       "  'around',\n",
       "  'climate',\n",
       "  'negotiation',\n",
       "  'dominant',\n",
       "  'climate',\n",
       "  'cooperation',\n",
       "  'gains',\n",
       "  'importance',\n",
       "  'post'],\n",
       " ['also',\n",
       "  'found',\n",
       "  'climate',\n",
       "  'frames',\n",
       "  'related',\n",
       "  'domestic',\n",
       "  'action',\n",
       "  'also',\n",
       "  'gained',\n",
       "  'traction',\n",
       "  'recent',\n",
       "  'years'],\n",
       " ['stakeholder', 'consensus', 'essential', 'aspect', 'climate', 'action'],\n",
       " ['given',\n",
       "  'multiple',\n",
       "  'stakeholders',\n",
       "  'climate',\n",
       "  'change',\n",
       "  'discourse',\n",
       "  'media',\n",
       "  'analysis',\n",
       "  'play',\n",
       "  'crucial',\n",
       "  'role',\n",
       "  'gauging',\n",
       "  'engagement',\n",
       "  'citizens',\n",
       "  'state',\n",
       "  'commercial',\n",
       "  'sector'],\n",
       " ['using',\n",
       "  'nlp',\n",
       "  'techniques',\n",
       "  'context',\n",
       "  'scale',\n",
       "  'information',\n",
       "  'processing',\n",
       "  'augment',\n",
       "  'insights',\n",
       "  'derived',\n",
       "  'research',\n",
       "  'methods',\n",
       "  'thus',\n",
       "  'helping',\n",
       "  'make',\n",
       "  'informed',\n",
       "  'policy',\n",
       "  'decisions',\n",
       "  'countering',\n",
       "  'climate',\n",
       "  'change']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent=[x.split() for x in data_final]\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram=Phrases(sent)\n",
    "trigram=Phrases(bigram[sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser=Phraser(bigram)\n",
    "trigram_phraser=Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow=[trigram_phraser[bigram_phraser[words]] for words in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram[bigram[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_nostops = remove_stopwords(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_bigrams = make_bigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['term']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['term'],\n",
       " ['absolute'],\n",
       " ['emission'],\n",
       " [],\n",
       " ['surpass'],\n",
       " [],\n",
       " ['become'],\n",
       " ['third'],\n",
       " ['large'],\n",
       " ['emitter'],\n",
       " ['solve'],\n",
       " ['wicked'],\n",
       " ['problem'],\n",
       " ['call'],\n",
       " ['climate'],\n",
       " ['action'],\n",
       " [],\n",
       " ['stakeholder'],\n",
       " ['spectrum'],\n",
       " ['involve'],\n",
       " ['government'],\n",
       " ['business'],\n",
       " ['community'],\n",
       " ['citizen'],\n",
       " ['extant'],\n",
       " ['literature'],\n",
       " ['focus'],\n",
       " ['significantly'],\n",
       " ['role'],\n",
       " ['government'],\n",
       " ['individual'],\n",
       " ['perception'],\n",
       " ['business'],\n",
       " ['sector'],\n",
       " ['need'],\n",
       " ['represent'],\n",
       " ['study'],\n",
       " ['consider'],\n",
       " ['business'],\n",
       " ['news'],\n",
       " ['medium'],\n",
       " ['platform'],\n",
       " ['reflect'],\n",
       " ['industry'],\n",
       " ['engagement'],\n",
       " ['climate'],\n",
       " ['change'],\n",
       " ['source'],\n",
       " ['information'],\n",
       " ['climate'],\n",
       " ['change'],\n",
       " ['business'],\n",
       " ['decision'],\n",
       " ['maker'],\n",
       " ['hence'],\n",
       " ['understand'],\n",
       " ['topic'],\n",
       " ['theme'],\n",
       " ['nexus'],\n",
       " ['climate'],\n",
       " ['business'],\n",
       " ['important'],\n",
       " ['evaluate'],\n",
       " ['business'],\n",
       " ['sector'],\n",
       " ['stance'],\n",
       " [],\n",
       " ['climate'],\n",
       " ['change'],\n",
       " ['evolve'],\n",
       " ['work'],\n",
       " ['explore'],\n",
       " ['business'],\n",
       " ['news'],\n",
       " ['relate'],\n",
       " ['climate'],\n",
       " ['change'],\n",
       " ['use'],\n",
       " ['natural'],\n",
       " ['language'],\n",
       " ['technique'],\n",
       " ['first'],\n",
       " ['experiment'],\n",
       " [],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['technique'],\n",
       " [],\n",
       " ['nmf'],\n",
       " ['bertopic'],\n",
       " ['business'],\n",
       " ['news'],\n",
       " [],\n",
       " [],\n",
       " ['news'],\n",
       " ['dataset'],\n",
       " ['test'],\n",
       " ['datum'],\n",
       " ['derive'],\n",
       " ['digital'],\n",
       " ['news'],\n",
       " ['archive'],\n",
       " ['economic'],\n",
       " ['time'],\n",
       " [],\n",
       " ['lead'],\n",
       " ['business'],\n",
       " ['news'],\n",
       " ['daily'],\n",
       " ['evaluate'],\n",
       " ['performance'],\n",
       " ['base'],\n",
       " ['quantitative'],\n",
       " ['metric'],\n",
       " ['commonly'],\n",
       " ['use'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['choose'],\n",
       " [],\n",
       " ['provide'],\n",
       " ['high'],\n",
       " ['precision'],\n",
       " ['climate'],\n",
       " ['specific'],\n",
       " ['information'],\n",
       " ['represent'],\n",
       " ['test'],\n",
       " ['dataset'],\n",
       " ['apply'],\n",
       " [],\n",
       " ['good'],\n",
       " ['performance'],\n",
       " ['evaluate'],\n",
       " ['experiment'],\n",
       " ['large'],\n",
       " [],\n",
       " [],\n",
       " ['climate'],\n",
       " ['news'],\n",
       " ['span'],\n",
       " ['present'],\n",
       " ['different'],\n",
       " ['theme'],\n",
       " ['include'],\n",
       " ['industry'],\n",
       " ['engagement'],\n",
       " ['evolve'],\n",
       " ['last'],\n",
       " [],\n",
       " ['decade'],\n",
       " ['result'],\n",
       " ['suggest'],\n",
       " ['climate'],\n",
       " ['cooperation'],\n",
       " ['high'],\n",
       " ['contribution'],\n",
       " [],\n",
       " ['theme'],\n",
       " ['resource'],\n",
       " ['management'],\n",
       " ['energy'],\n",
       " ['business'],\n",
       " ['gain'],\n",
       " ['traction'],\n",
       " ['recent'],\n",
       " ['year'],\n",
       " ['index'],\n",
       " ['term'],\n",
       " ['climate'],\n",
       " ['change'],\n",
       " ['medium'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " [],\n",
       " ['computational'],\n",
       " ['social'],\n",
       " ['science'],\n",
       " ['experiment'],\n",
       " ['introduction'],\n",
       " ['climate'],\n",
       " ['change'],\n",
       " ['critical'],\n",
       " ['issue'],\n",
       " ['millennium'],\n",
       " [],\n",
       " ['high'],\n",
       " ['stake'],\n",
       " ['global'],\n",
       " ['debate'],\n",
       " [],\n",
       " ['third'],\n",
       " ['large'],\n",
       " ['emitter'],\n",
       " ['also'],\n",
       " ['high'],\n",
       " ['climate'],\n",
       " ['vulnerability'],\n",
       " ['recent'],\n",
       " ['update'],\n",
       " [],\n",
       " ['pledge'],\n",
       " ['country'],\n",
       " ['commit'],\n",
       " ['reduce'],\n",
       " ['emission'],\n",
       " ['intensity'],\n",
       " ['level'],\n",
       " ['emission'],\n",
       " ['reduction'],\n",
       " ['scale'],\n",
       " ['require'],\n",
       " ['solid'],\n",
       " ['public'],\n",
       " ['support'],\n",
       " [],\n",
       " ['stakeholder'],\n",
       " ['spectrum'],\n",
       " [],\n",
       " [],\n",
       " ['citizen'],\n",
       " ['enterprise'],\n",
       " ['role'],\n",
       " ['commercial'],\n",
       " ['sector'],\n",
       " ['particularly'],\n",
       " ['relevant'],\n",
       " ['give'],\n",
       " ['contribution'],\n",
       " ['country'],\n",
       " ['level'],\n",
       " ['emission'],\n",
       " ['associate'],\n",
       " ['editor'],\n",
       " ['coordinate'],\n",
       " ['review'],\n",
       " [],\n",
       " ['approve'],\n",
       " ['publication'],\n",
       " [],\n",
       " ['dulizia'],\n",
       " ['practical'],\n",
       " ['approach'],\n",
       " [],\n",
       " ['understand'],\n",
       " ['different'],\n",
       " ['stakeholder'],\n",
       " ['engage'],\n",
       " ['climate'],\n",
       " ['change'],\n",
       " ['analysis'],\n",
       " ['news'],\n",
       " ['coverage'],\n",
       " ['news'],\n",
       " ['medium'],\n",
       " ['reflect'],\n",
       " ['contemporary'],\n",
       " ['public'],\n",
       " ['discourse'],\n",
       " ['business'],\n",
       " ['newspaper'],\n",
       " [],\n",
       " ['ditionally'],\n",
       " ['report'],\n",
       " ['news'],\n",
       " ['relevant'],\n",
       " ['business'],\n",
       " ['decision'],\n",
       " ['maker'],\n",
       " ['therefore'],\n",
       " ['mapping'],\n",
       " ['topic'],\n",
       " ['pertinent'],\n",
       " ['climate'],\n",
       " ['change'],\n",
       " ['business'],\n",
       " ['medium'],\n",
       " ['issue'],\n",
       " ['evolve'],\n",
       " ['help'],\n",
       " ['understand'],\n",
       " ['commercial'],\n",
       " ['sector'],\n",
       " ['engage'],\n",
       " ['issue'],\n",
       " ['climate'],\n",
       " ['change'],\n",
       " ['automate'],\n",
       " ['content'],\n",
       " ['analysis'],\n",
       " ['large'],\n",
       " [],\n",
       " ['estab'],\n",
       " ['lishe'],\n",
       " ['technique'],\n",
       " ['social'],\n",
       " ['science'],\n",
       " ['specifically'],\n",
       " ['use'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['extract'],\n",
       " ['latent'],\n",
       " ['topic'],\n",
       " ['newspaper'],\n",
       " ['discussion'],\n",
       " ['forum'],\n",
       " ['social'],\n",
       " ['medium'],\n",
       " ['content'],\n",
       " ['gain'],\n",
       " ['traction'],\n",
       " ['extant'],\n",
       " ['literature'],\n",
       " [],\n",
       " ['different'],\n",
       " ['work'],\n",
       " ['license'],\n",
       " ['creative'],\n",
       " ['common'],\n",
       " ['attribution'],\n",
       " ['noncommercial'],\n",
       " ['noderivative'],\n",
       " ['license'],\n",
       " ['information'],\n",
       " ['see'],\n",
       " [],\n",
       " ['creativecommon'],\n",
       " ['org', 'license'],\n",
       " [],\n",
       " ['volume'],\n",
       " ['umamaheswaran'],\n",
       " ['mapping'],\n",
       " ['climate'],\n",
       " ['theme'],\n",
       " ['analysis'],\n",
       " ['business'],\n",
       " ['news'],\n",
       " ['use'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['natural'],\n",
       " ['language'],\n",
       " ['process'],\n",
       " [],\n",
       " ['technique'],\n",
       " ['topic'],\n",
       " [],\n",
       " ['ele'],\n",
       " ['machine'],\n",
       " ['learn'],\n",
       " ['task'],\n",
       " ['group'],\n",
       " ['document'],\n",
       " ['word'],\n",
       " ['similar'],\n",
       " ['meaning'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['yield'],\n",
       " ['topic'],\n",
       " ['rank'],\n",
       " ['base'],\n",
       " ['specific'],\n",
       " ['term'],\n",
       " ['relevance'],\n",
       " ['topic'],\n",
       " ['topic'],\n",
       " ['emerge'],\n",
       " ['automatically'],\n",
       " [],\n",
       " ['prior'],\n",
       " ['annotation'],\n",
       " ['label'],\n",
       " ['raw'],\n",
       " ['datum'],\n",
       " ['sense'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['form'],\n",
       " ['important'],\n",
       " ['category'],\n",
       " ['unsupervise'],\n",
       " ['tech'],\n",
       " ['nique'],\n",
       " [],\n",
       " ['large'],\n",
       " ['field'],\n",
       " ['natural'],\n",
       " ['language'],\n",
       " ['process'],\n",
       " [],\n",
       " ['many'],\n",
       " [],\n",
       " ['approach'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['even'],\n",
       " ['high'],\n",
       " ['spread'],\n",
       " ['algorithm'],\n",
       " ['learn'],\n",
       " ['param'],\n",
       " ['eter'],\n",
       " ['model'],\n",
       " ['approach'],\n",
       " ['conventional'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['range'],\n",
       " ['linear'],\n",
       " ['algebraic'],\n",
       " ['base'],\n",
       " ['technique'],\n",
       " ['lsa'],\n",
       " ['popular'],\n",
       " ['probabilistic'],\n",
       " ['model'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['metric'],\n",
       " ['decomposition'],\n",
       " ['base'],\n",
       " ['method'],\n",
       " ['nmf'],\n",
       " ['also'],\n",
       " ['mainstay'],\n",
       " [],\n",
       " ['application'],\n",
       " ['underlie'],\n",
       " ['bag'],\n",
       " ['word'],\n",
       " ['bow'],\n",
       " ['approach'],\n",
       " ['distinguish'],\n",
       " ['factor'],\n",
       " [],\n",
       " ['approach'],\n",
       " ['however'],\n",
       " ['bow'],\n",
       " ['base'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['base'],\n",
       " ['unrealistic'],\n",
       " ['assumption'],\n",
       " ['word'],\n",
       " ['independent'],\n",
       " ['rely'],\n",
       " ['frequency'],\n",
       " ['word'],\n",
       " ['occurrence'],\n",
       " ['recent'],\n",
       " ['development'],\n",
       " [],\n",
       " ['representation'],\n",
       " ['text'],\n",
       " ['use'],\n",
       " ['embedding'],\n",
       " ['embedding'],\n",
       " ['context'],\n",
       " [],\n",
       " ['dense'],\n",
       " ['representation'],\n",
       " ['unit'],\n",
       " ['text'],\n",
       " ['vector'],\n",
       " ['space'],\n",
       " ['way'],\n",
       " ['semantic'],\n",
       " ['similarity'],\n",
       " ['unit'],\n",
       " ['text'],\n",
       " ['capture'],\n",
       " [],\n",
       " ['automate'],\n",
       " ['content'],\n",
       " ['analysis'],\n",
       " ['gain'],\n",
       " ['importance'],\n",
       " [],\n",
       " ['climate'],\n",
       " ['discourse'],\n",
       " ['literature'],\n",
       " ['overreliance'],\n",
       " ['probabilistic'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['importantly'],\n",
       " ['choice'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " [],\n",
       " ['motivate'],\n",
       " ['popular'],\n",
       " ['use'],\n",
       " ['back'],\n",
       " ['experiment'],\n",
       " ['evaluate'],\n",
       " ['performance'],\n",
       " ['metric'],\n",
       " ['especially'],\n",
       " ['social'],\n",
       " ['science'],\n",
       " ['give'],\n",
       " ['background'],\n",
       " ['objective'],\n",
       " ['article'],\n",
       " ['evaluate'],\n",
       " ['performance'],\n",
       " ['conventional'],\n",
       " ['bow'],\n",
       " ['base'],\n",
       " ['model'],\n",
       " ['embedding'],\n",
       " ['base'],\n",
       " ['model'],\n",
       " ['specifically'],\n",
       " ['run'],\n",
       " ['experiment'],\n",
       " ['use'],\n",
       " [],\n",
       " ['nmf'],\n",
       " ['bertopic'],\n",
       " ['choose'],\n",
       " ['appropriate'],\n",
       " ['model'],\n",
       " ['base'],\n",
       " ['evaluation'],\n",
       " ['metric'],\n",
       " ['use'],\n",
       " ['map'],\n",
       " ['theme'],\n",
       " [],\n",
       " ['economic'],\n",
       " ['time'],\n",
       " [],\n",
       " ['article'],\n",
       " ['organize'],\n",
       " ['follow'],\n",
       " ['section'],\n",
       " ['introduce'],\n",
       " ['literature'],\n",
       " ['automate'],\n",
       " ['content'],\n",
       " ['analysis'],\n",
       " ['climate'],\n",
       " ['news'],\n",
       " ['discuss'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['use'],\n",
       " ['literature'],\n",
       " ['section'],\n",
       " [],\n",
       " ['present'],\n",
       " ['datum'],\n",
       " ['method'],\n",
       " ['section'],\n",
       " ['offer'],\n",
       " ['detail'],\n",
       " ['experiment'],\n",
       " ['result'],\n",
       " ['section'],\n",
       " ['present'],\n",
       " ['result'],\n",
       " ['final'],\n",
       " ['implantation'],\n",
       " ['choose'],\n",
       " ['model'],\n",
       " ['literature'],\n",
       " ['review'],\n",
       " ['adoption'],\n",
       " [],\n",
       " ['technique'],\n",
       " ['social'],\n",
       " ['science'],\n",
       " ['see'],\n",
       " ['significant'],\n",
       " ['growth'],\n",
       " ['recent'],\n",
       " ['decade'],\n",
       " ['language'],\n",
       " ['social'],\n",
       " ['construct'],\n",
       " ['consider'],\n",
       " ['proxy'],\n",
       " ['behav'],\n",
       " ['ior'],\n",
       " ['express'],\n",
       " ['people'],\n",
       " ['language'],\n",
       " [],\n",
       " ['contain'],\n",
       " ['rich'],\n",
       " ['latent'],\n",
       " ['information'],\n",
       " ['help'],\n",
       " ['identify'],\n",
       " ['several'],\n",
       " ['diman'],\n",
       " ['sion'],\n",
       " ['trait'],\n",
       " ['behavior'],\n",
       " ['therefore'],\n",
       " ['rapid'],\n",
       " ['growth'],\n",
       " [],\n",
       " ['method'],\n",
       " [],\n",
       " ['social'],\n",
       " ['science'],\n",
       " ['surprising'],\n",
       " ['social'],\n",
       " ['science'],\n",
       " ['application'],\n",
       " [],\n",
       " ['range'],\n",
       " ['understand'],\n",
       " [],\n",
       " ['political'],\n",
       " ['affiliation'],\n",
       " ['voter'],\n",
       " ['intention'],\n",
       " ['health'],\n",
       " ['care'],\n",
       " ['delivery'],\n",
       " ['medium'],\n",
       " ['monitor'],\n",
       " ['improve'],\n",
       " ['public'],\n",
       " ['policy'],\n",
       " ['implementation'],\n",
       " ['use'],\n",
       " [],\n",
       " ['mining'],\n",
       " ['social'],\n",
       " ['discourse'],\n",
       " ['around'],\n",
       " ['climate'],\n",
       " ['change'],\n",
       " ['still'],\n",
       " ['nascent'],\n",
       " ['broadly'],\n",
       " ['categorize'],\n",
       " ['base'],\n",
       " [],\n",
       " ['sis'],\n",
       " ['social'],\n",
       " ['medium'],\n",
       " ['platform'],\n",
       " ['online'],\n",
       " ['news'],\n",
       " ['scientific'],\n",
       " ['technical'],\n",
       " ['document'],\n",
       " ['several'],\n",
       " ['study'],\n",
       " ['analyze'],\n",
       " ['societal'],\n",
       " ['stance'],\n",
       " ['climate'],\n",
       " ['change'],\n",
       " ['study'],\n",
       " ['twitter'],\n",
       " ['microblog'],\n",
       " ['example'],\n",
       " ['elgasem'],\n",
       " ['combine'],\n",
       " ['sentiment'],\n",
       " ['network'],\n",
       " ['analysis'],\n",
       " ['identify'],\n",
       " ['climate'],\n",
       " ['skeptic'],\n",
       " ['accept'],\n",
       " ['community'],\n",
       " ['stance'],\n",
       " ['detection'],\n",
       " ['climate'],\n",
       " ['change'],\n",
       " ['tweet'],\n",
       " ['emerge'],\n",
       " ['separate'],\n",
       " ['subdomain'],\n",
       " ['release'],\n",
       " ['semeval'],\n",
       " ['dataset'],\n",
       " ['define'],\n",
       " ['detect'],\n",
       " ['climate'],\n",
       " ['concern'],\n",
       " ['task'],\n",
       " ['several'],\n",
       " ['researcher'],\n",
       " ['employ'],\n",
       " ['advance'],\n",
       " [],\n",
       " ['machine'],\n",
       " ['learn'],\n",
       " ['technique'],\n",
       " ['improv'],\n",
       " [],\n",
       " ['stance'],\n",
       " ['detection'],\n",
       " ['climate'],\n",
       " ['tweet'],\n",
       " ['semeval'],\n",
       " ['dataset'],\n",
       " ['content'],\n",
       " ['analysis'],\n",
       " ['climate'],\n",
       " ['news'],\n",
       " ['establish'],\n",
       " ['norm'],\n",
       " ['use'],\n",
       " [],\n",
       " ['analyze'],\n",
       " ['climate'],\n",
       " ['news'],\n",
       " ['relatively'],\n",
       " ['new'],\n",
       " ['compare'],\n",
       " ['study'],\n",
       " ['social'],\n",
       " ['medium'],\n",
       " ['news'],\n",
       " [],\n",
       " ['sis'],\n",
       " ['receive'],\n",
       " ['less'],\n",
       " ['attention'],\n",
       " [],\n",
       " ['apply'],\n",
       " [],\n",
       " ['climate'],\n",
       " ['change'],\n",
       " ['article'],\n",
       " ['source'],\n",
       " [],\n",
       " [],\n",
       " ['daily'],\n",
       " ['extract'],\n",
       " ['theme'],\n",
       " ['range'],\n",
       " ['cli'],\n",
       " ['mate'],\n",
       " ['change'],\n",
       " ['impact'],\n",
       " ['politic'],\n",
       " ['climate'],\n",
       " ['science'],\n",
       " ['similarly'],\n",
       " [],\n",
       " ['segment'],\n",
       " ['newspaper'],\n",
       " ['base'],\n",
       " ['geography'],\n",
       " ['partisan'],\n",
       " ['bias'],\n",
       " ['circulation'],\n",
       " [],\n",
       " ['use'],\n",
       " ['structural'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['discover'],\n",
       " ['topic'],\n",
       " ['analysis'],\n",
       " ['influence'],\n",
       " ['attribute'],\n",
       " ['topic'],\n",
       " ['benite'],\n",
       " ['lazaro'],\n",
       " ['use'],\n",
       " [],\n",
       " ['news'],\n",
       " ['article'],\n",
       " ['analyze'],\n",
       " ['climate'],\n",
       " ['food'],\n",
       " ['energy'],\n",
       " ['nexus'],\n",
       " ['use'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['methodological'],\n",
       " ['perspective'],\n",
       " ['author'],\n",
       " ['use'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['especially'],\n",
       " ['popular'],\n",
       " ['latent'],\n",
       " ['dirichlet'],\n",
       " ['allocation'],\n",
       " [],\n",
       " [],\n",
       " ['popularity'],\n",
       " [],\n",
       " ['suf'],\n",
       " ['fer'],\n",
       " ['certain'],\n",
       " ['shortcoming'],\n",
       " ['notably'],\n",
       " ['foundational'],\n",
       " ['assumption'],\n",
       " [],\n",
       " ['document'],\n",
       " ['unordere'],\n",
       " ['set'],\n",
       " ['word'],\n",
       " ['appear'],\n",
       " ['text'],\n",
       " ['independent'],\n",
       " ['assumption'],\n",
       " ['common'],\n",
       " ['model'],\n",
       " ['base'],\n",
       " ['bag'],\n",
       " ['word'],\n",
       " ['text'],\n",
       " ['representation'],\n",
       " ['assumption'],\n",
       " ['conflict'],\n",
       " ['linguistic'],\n",
       " ['theory'],\n",
       " ['suggest'],\n",
       " ['word'],\n",
       " [],\n",
       " ['language'],\n",
       " ['always'],\n",
       " ['connect'],\n",
       " ['sequen'],\n",
       " ['tial'],\n",
       " [],\n",
       " ['aspect'],\n",
       " [],\n",
       " ['need'],\n",
       " ['number'],\n",
       " ['topic'],\n",
       " ['extract'],\n",
       " ['input'],\n",
       " [],\n",
       " ['begin'],\n",
       " ['know'],\n",
       " [],\n",
       " ['optimal'],\n",
       " ['number'],\n",
       " ['issue'],\n",
       " ['begin'],\n",
       " ['feasible'],\n",
       " [],\n",
       " ['hence'],\n",
       " ['introduce'],\n",
       " ['element'],\n",
       " ['subjectivity'],\n",
       " ['finally'],\n",
       " ['label'],\n",
       " ['extract'],\n",
       " ['topic'],\n",
       " ['also'],\n",
       " ['leave'],\n",
       " ['user'],\n",
       " ['discretion'],\n",
       " ['vary'],\n",
       " ['considerably'],\n",
       " ['coder'],\n",
       " ['therefore'],\n",
       " ['credible'],\n",
       " ['topic'],\n",
       " ['extraction'],\n",
       " [],\n",
       " ['validation'],\n",
       " ['additional'],\n",
       " ['information'],\n",
       " ['subject'],\n",
       " ['matter'],\n",
       " ['expert'],\n",
       " ['become'],\n",
       " ['crucial'],\n",
       " ['sense'],\n",
       " [],\n",
       " ['model'],\n",
       " ['lend'],\n",
       " ['automation'],\n",
       " ['shortcoming'],\n",
       " ['specify'],\n",
       " ['specific'],\n",
       " ['number'],\n",
       " ['topic'],\n",
       " ['counter'],\n",
       " ['use'],\n",
       " ['matrix'],\n",
       " ['factorization'],\n",
       " ['method'],\n",
       " ['nmf'],\n",
       " ['underlie'],\n",
       " ['issue'],\n",
       " ['relate'],\n",
       " ['bow'],\n",
       " ['representation'],\n",
       " ['remain'],\n",
       " ['dependency'],\n",
       " ['classical'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['partic'],\n",
       " ['ularly'],\n",
       " ['conspicuous'],\n",
       " ['give'],\n",
       " ['several'],\n",
       " ['algorithmic'],\n",
       " ['development'],\n",
       " ['domain'],\n",
       " ['example'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['base'],\n",
       " ['embedding'],\n",
       " ['gain'],\n",
       " ['traction'],\n",
       " ['recent'],\n",
       " ['year'],\n",
       " ['embedding'],\n",
       " ['dense'],\n",
       " ['text'],\n",
       " ['representation'],\n",
       " ['word'],\n",
       " ['semantic'],\n",
       " ['similarity'],\n",
       " ['volume'],\n",
       " ['umamaheswaran'],\n",
       " ['mapping'],\n",
       " ['climate'],\n",
       " ['theme'],\n",
       " ['analysis'],\n",
       " ['business'],\n",
       " ['news'],\n",
       " ['use'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['figure'],\n",
       " ['diagrammatic'],\n",
       " ['representation'],\n",
       " [],\n",
       " ['close'],\n",
       " ['low'],\n",
       " ['dimensional'],\n",
       " ['space'],\n",
       " ['word'],\n",
       " ['embedding'],\n",
       " ['capture'],\n",
       " ['relationship'],\n",
       " ['word'],\n",
       " ['convey'],\n",
       " ['context'],\n",
       " ['well'],\n",
       " ['word'],\n",
       " ['count'],\n",
       " ['base'],\n",
       " ['bow'],\n",
       " ['representation'],\n",
       " ['text'],\n",
       " ['general'],\n",
       " ['embedding'],\n",
       " ['base'],\n",
       " ['model'],\n",
       " ['report'],\n",
       " ['supe'],\n",
       " [],\n",
       " ['performance'],\n",
       " ['however'],\n",
       " ['application'],\n",
       " ['embedding'],\n",
       " ['base'],\n",
       " ['topic'],\n",
       " ['model'],\n",
       " ['social'],\n",
       " ['science'],\n",
       " ['sparse'],\n",
       " [],\n",
       " ['significant'],\n",
       " ['gap'],\n",
       " ['social'],\n",
       " ['science'],\n",
       " ['base'],\n",
       " ['application'],\n",
       " ['lack'],\n",
       " ['robust'],\n",
       " ['experiment'],\n",
       " ['model'],\n",
       " ['selection'],\n",
       " [],\n",
       " [],\n",
       " ['typically'],\n",
       " ['choose'],\n",
       " ['particular'],\n",
       " ...]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word=corpora.Dictionary(data_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'term': 0,\n",
       " 'absolute': 1,\n",
       " 'emission': 2,\n",
       " 'surpass': 3,\n",
       " 'become': 4,\n",
       " 'third': 5,\n",
       " 'large': 6,\n",
       " 'emitter': 7,\n",
       " 'solve': 8,\n",
       " 'wicked': 9,\n",
       " 'problem': 10,\n",
       " 'call': 11,\n",
       " 'climate': 12,\n",
       " 'action': 13,\n",
       " 'stakeholder': 14,\n",
       " 'spectrum': 15,\n",
       " 'involve': 16,\n",
       " 'government': 17,\n",
       " 'business': 18,\n",
       " 'community': 19,\n",
       " 'citizen': 20,\n",
       " 'extant': 21,\n",
       " 'literature': 22,\n",
       " 'focus': 23,\n",
       " 'significantly': 24,\n",
       " 'role': 25,\n",
       " 'individual': 26,\n",
       " 'perception': 27,\n",
       " 'sector': 28,\n",
       " 'need': 29,\n",
       " 'represent': 30,\n",
       " 'study': 31,\n",
       " 'consider': 32,\n",
       " 'news': 33,\n",
       " 'medium': 34,\n",
       " 'platform': 35,\n",
       " 'reflect': 36,\n",
       " 'industry': 37,\n",
       " 'engagement': 38,\n",
       " 'change': 39,\n",
       " 'source': 40,\n",
       " 'information': 41,\n",
       " 'decision': 42,\n",
       " 'maker': 43,\n",
       " 'hence': 44,\n",
       " 'understand': 45,\n",
       " 'topic': 46,\n",
       " 'theme': 47,\n",
       " 'nexus': 48,\n",
       " 'important': 49,\n",
       " 'evaluate': 50,\n",
       " 'stance': 51,\n",
       " 'evolve': 52,\n",
       " 'work': 53,\n",
       " 'explore': 54,\n",
       " 'relate': 55,\n",
       " 'use': 56,\n",
       " 'natural': 57,\n",
       " 'language': 58,\n",
       " 'technique': 59,\n",
       " 'first': 60,\n",
       " 'experiment': 61,\n",
       " 'model': 62,\n",
       " 'nmf': 63,\n",
       " 'bertopic': 64,\n",
       " 'dataset': 65,\n",
       " 'test': 66,\n",
       " 'datum': 67,\n",
       " 'derive': 68,\n",
       " 'digital': 69,\n",
       " 'archive': 70,\n",
       " 'economic': 71,\n",
       " 'time': 72,\n",
       " 'lead': 73,\n",
       " 'daily': 74,\n",
       " 'performance': 75,\n",
       " 'base': 76,\n",
       " 'quantitative': 77,\n",
       " 'metric': 78,\n",
       " 'commonly': 79,\n",
       " 'choose': 80,\n",
       " 'provide': 81,\n",
       " 'high': 82,\n",
       " 'precision': 83,\n",
       " 'specific': 84,\n",
       " 'apply': 85,\n",
       " 'good': 86,\n",
       " 'span': 87,\n",
       " 'present': 88,\n",
       " 'different': 89,\n",
       " 'include': 90,\n",
       " 'last': 91,\n",
       " 'decade': 92,\n",
       " 'result': 93,\n",
       " 'suggest': 94,\n",
       " 'cooperation': 95,\n",
       " 'contribution': 96,\n",
       " 'resource': 97,\n",
       " 'management': 98,\n",
       " 'energy': 99,\n",
       " 'gain': 100,\n",
       " 'traction': 101,\n",
       " 'recent': 102,\n",
       " 'year': 103,\n",
       " 'index': 104,\n",
       " 'computational': 105,\n",
       " 'social': 106,\n",
       " 'science': 107,\n",
       " 'introduction': 108,\n",
       " 'critical': 109,\n",
       " 'issue': 110,\n",
       " 'millennium': 111,\n",
       " 'stake': 112,\n",
       " 'global': 113,\n",
       " 'debate': 114,\n",
       " 'also': 115,\n",
       " 'vulnerability': 116,\n",
       " 'update': 117,\n",
       " 'pledge': 118,\n",
       " 'country': 119,\n",
       " 'commit': 120,\n",
       " 'reduce': 121,\n",
       " 'intensity': 122,\n",
       " 'level': 123,\n",
       " 'reduction': 124,\n",
       " 'scale': 125,\n",
       " 'require': 126,\n",
       " 'solid': 127,\n",
       " 'public': 128,\n",
       " 'support': 129,\n",
       " 'enterprise': 130,\n",
       " 'commercial': 131,\n",
       " 'particularly': 132,\n",
       " 'relevant': 133,\n",
       " 'give': 134,\n",
       " 'associate': 135,\n",
       " 'editor': 136,\n",
       " 'coordinate': 137,\n",
       " 'review': 138,\n",
       " 'approve': 139,\n",
       " 'publication': 140,\n",
       " 'dulizia': 141,\n",
       " 'practical': 142,\n",
       " 'approach': 143,\n",
       " 'engage': 144,\n",
       " 'analysis': 145,\n",
       " 'coverage': 146,\n",
       " 'contemporary': 147,\n",
       " 'discourse': 148,\n",
       " 'newspaper': 149,\n",
       " 'ditionally': 150,\n",
       " 'report': 151,\n",
       " 'therefore': 152,\n",
       " 'mapping': 153,\n",
       " 'pertinent': 154,\n",
       " 'help': 155,\n",
       " 'automate': 156,\n",
       " 'content': 157,\n",
       " 'estab': 158,\n",
       " 'lishe': 159,\n",
       " 'specifically': 160,\n",
       " 'extract': 161,\n",
       " 'latent': 162,\n",
       " 'discussion': 163,\n",
       " 'forum': 164,\n",
       " 'license': 165,\n",
       " 'creative': 166,\n",
       " 'common': 167,\n",
       " 'attribution': 168,\n",
       " 'noncommercial': 169,\n",
       " 'noderivative': 170,\n",
       " 'see': 171,\n",
       " 'creativecommon': 172,\n",
       " 'org': 173,\n",
       " 'volume': 174,\n",
       " 'umamaheswaran': 175,\n",
       " 'process': 176,\n",
       " 'ele': 177,\n",
       " 'machine': 178,\n",
       " 'learn': 179,\n",
       " 'task': 180,\n",
       " 'group': 181,\n",
       " 'document': 182,\n",
       " 'word': 183,\n",
       " 'similar': 184,\n",
       " 'meaning': 185,\n",
       " 'yield': 186,\n",
       " 'rank': 187,\n",
       " 'relevance': 188,\n",
       " 'emerge': 189,\n",
       " 'automatically': 190,\n",
       " 'prior': 191,\n",
       " 'annotation': 192,\n",
       " 'label': 193,\n",
       " 'raw': 194,\n",
       " 'sense': 195,\n",
       " 'form': 196,\n",
       " 'category': 197,\n",
       " 'unsupervise': 198,\n",
       " 'tech': 199,\n",
       " 'nique': 200,\n",
       " 'field': 201,\n",
       " 'many': 202,\n",
       " 'even': 203,\n",
       " 'spread': 204,\n",
       " 'algorithm': 205,\n",
       " 'param': 206,\n",
       " 'eter': 207,\n",
       " 'conventional': 208,\n",
       " 'range': 209,\n",
       " 'linear': 210,\n",
       " 'algebraic': 211,\n",
       " 'lsa': 212,\n",
       " 'popular': 213,\n",
       " 'probabilistic': 214,\n",
       " 'decomposition': 215,\n",
       " 'method': 216,\n",
       " 'mainstay': 217,\n",
       " 'application': 218,\n",
       " 'underlie': 219,\n",
       " 'bag': 220,\n",
       " 'bow': 221,\n",
       " 'distinguish': 222,\n",
       " 'factor': 223,\n",
       " 'however': 224,\n",
       " 'unrealistic': 225,\n",
       " 'assumption': 226,\n",
       " 'independent': 227,\n",
       " 'rely': 228,\n",
       " 'frequency': 229,\n",
       " 'occurrence': 230,\n",
       " 'development': 231,\n",
       " 'representation': 232,\n",
       " 'text': 233,\n",
       " 'embedding': 234,\n",
       " 'context': 235,\n",
       " 'dense': 236,\n",
       " 'unit': 237,\n",
       " 'vector': 238,\n",
       " 'space': 239,\n",
       " 'way': 240,\n",
       " 'semantic': 241,\n",
       " 'similarity': 242,\n",
       " 'capture': 243,\n",
       " 'importance': 244,\n",
       " 'overreliance': 245,\n",
       " 'importantly': 246,\n",
       " 'choice': 247,\n",
       " 'motivate': 248,\n",
       " 'back': 249,\n",
       " 'especially': 250,\n",
       " 'background': 251,\n",
       " 'objective': 252,\n",
       " 'article': 253,\n",
       " 'run': 254,\n",
       " 'appropriate': 255,\n",
       " 'evaluation': 256,\n",
       " 'map': 257,\n",
       " 'organize': 258,\n",
       " 'follow': 259,\n",
       " 'section': 260,\n",
       " 'introduce': 261,\n",
       " 'discuss': 262,\n",
       " 'offer': 263,\n",
       " 'detail': 264,\n",
       " 'final': 265,\n",
       " 'implantation': 266,\n",
       " 'adoption': 267,\n",
       " 'significant': 268,\n",
       " 'growth': 269,\n",
       " 'construct': 270,\n",
       " 'proxy': 271,\n",
       " 'behav': 272,\n",
       " 'ior': 273,\n",
       " 'express': 274,\n",
       " 'people': 275,\n",
       " 'contain': 276,\n",
       " 'rich': 277,\n",
       " 'identify': 278,\n",
       " 'several': 279,\n",
       " 'diman': 280,\n",
       " 'sion': 281,\n",
       " 'trait': 282,\n",
       " 'behavior': 283,\n",
       " 'rapid': 284,\n",
       " 'surprising': 285,\n",
       " 'political': 286,\n",
       " 'affiliation': 287,\n",
       " 'voter': 288,\n",
       " 'intention': 289,\n",
       " 'health': 290,\n",
       " 'care': 291,\n",
       " 'delivery': 292,\n",
       " 'monitor': 293,\n",
       " 'improve': 294,\n",
       " 'policy': 295,\n",
       " 'implementation': 296,\n",
       " 'mining': 297,\n",
       " 'around': 298,\n",
       " 'still': 299,\n",
       " 'nascent': 300,\n",
       " 'broadly': 301,\n",
       " 'categorize': 302,\n",
       " 'sis': 303,\n",
       " 'online': 304,\n",
       " 'scientific': 305,\n",
       " 'technical': 306,\n",
       " 'analyze': 307,\n",
       " 'societal': 308,\n",
       " 'twitter': 309,\n",
       " 'microblog': 310,\n",
       " 'example': 311,\n",
       " 'elgasem': 312,\n",
       " 'combine': 313,\n",
       " 'sentiment': 314,\n",
       " 'network': 315,\n",
       " 'skeptic': 316,\n",
       " 'accept': 317,\n",
       " 'detection': 318,\n",
       " 'tweet': 319,\n",
       " 'separate': 320,\n",
       " 'subdomain': 321,\n",
       " 'release': 322,\n",
       " 'semeval': 323,\n",
       " 'define': 324,\n",
       " 'detect': 325,\n",
       " 'concern': 326,\n",
       " 'researcher': 327,\n",
       " 'employ': 328,\n",
       " 'advance': 329,\n",
       " 'improv': 330,\n",
       " 'establish': 331,\n",
       " 'norm': 332,\n",
       " 'relatively': 333,\n",
       " 'new': 334,\n",
       " 'compare': 335,\n",
       " 'receive': 336,\n",
       " 'less': 337,\n",
       " 'attention': 338,\n",
       " 'cli': 339,\n",
       " 'mate': 340,\n",
       " 'impact': 341,\n",
       " 'politic': 342,\n",
       " 'similarly': 343,\n",
       " 'segment': 344,\n",
       " 'geography': 345,\n",
       " 'partisan': 346,\n",
       " 'bias': 347,\n",
       " 'circulation': 348,\n",
       " 'structural': 349,\n",
       " 'discover': 350,\n",
       " 'influence': 351,\n",
       " 'attribute': 352,\n",
       " 'benite': 353,\n",
       " 'lazaro': 354,\n",
       " 'food': 355,\n",
       " 'methodological': 356,\n",
       " 'perspective': 357,\n",
       " 'author': 358,\n",
       " 'dirichlet': 359,\n",
       " 'allocation': 360,\n",
       " 'popularity': 361,\n",
       " 'suf': 362,\n",
       " 'fer': 363,\n",
       " 'certain': 364,\n",
       " 'shortcoming': 365,\n",
       " 'notably': 366,\n",
       " 'foundational': 367,\n",
       " 'unordere': 368,\n",
       " 'set': 369,\n",
       " 'appear': 370,\n",
       " 'conflict': 371,\n",
       " 'linguistic': 372,\n",
       " 'theory': 373,\n",
       " 'always': 374,\n",
       " 'connect': 375,\n",
       " 'sequen': 376,\n",
       " 'tial': 377,\n",
       " 'aspect': 378,\n",
       " 'number': 379,\n",
       " 'input': 380,\n",
       " 'begin': 381,\n",
       " 'know': 382,\n",
       " 'optimal': 383,\n",
       " 'feasible': 384,\n",
       " 'element': 385,\n",
       " 'subjectivity': 386,\n",
       " 'finally': 387,\n",
       " 'leave': 388,\n",
       " 'user': 389,\n",
       " 'discretion': 390,\n",
       " 'vary': 391,\n",
       " 'considerably': 392,\n",
       " 'coder': 393,\n",
       " 'credible': 394,\n",
       " 'extraction': 395,\n",
       " 'validation': 396,\n",
       " 'additional': 397,\n",
       " 'subject': 398,\n",
       " 'matter': 399,\n",
       " 'expert': 400,\n",
       " 'crucial': 401,\n",
       " 'lend': 402,\n",
       " 'automation': 403,\n",
       " 'specify': 404,\n",
       " 'counter': 405,\n",
       " 'matrix': 406,\n",
       " 'factorization': 407,\n",
       " 'remain': 408,\n",
       " 'dependency': 409,\n",
       " 'classical': 410,\n",
       " 'partic': 411,\n",
       " 'ularly': 412,\n",
       " 'conspicuous': 413,\n",
       " 'algorithmic': 414,\n",
       " 'domain': 415,\n",
       " 'figure': 416,\n",
       " 'diagrammatic': 417,\n",
       " 'close': 418,\n",
       " 'low': 419,\n",
       " 'dimensional': 420,\n",
       " 'relationship': 421,\n",
       " 'convey': 422,\n",
       " 'well': 423,\n",
       " 'count': 424,\n",
       " 'general': 425,\n",
       " 'supe': 426,\n",
       " 'sparse': 427,\n",
       " 'gap': 428,\n",
       " 'lack': 429,\n",
       " 'robust': 430,\n",
       " 'selection': 431,\n",
       " 'typically': 432,\n",
       " 'particular': 433,\n",
       " 'basis': 434,\n",
       " 'theoretical': 435,\n",
       " 'tion': 436,\n",
       " 'basic': 437,\n",
       " 'discipline': 438,\n",
       " 'length': 439,\n",
       " 'pattern': 440,\n",
       " 'unstructure': 441,\n",
       " 'assume': 442,\n",
       " 'generate': 443,\n",
       " 'complex': 444,\n",
       " 'generative': 445,\n",
       " 'mechanism': 446,\n",
       " 'create': 447,\n",
       " 'select': 448,\n",
       " 'distribution': 449,\n",
       " 'pick': 450,\n",
       " 'mixture': 451,\n",
       " 'multinomial': 452,\n",
       " 'thus': 453,\n",
       " 'estimate': 454,\n",
       " 'parameter': 455,\n",
       " 'likely': 456,\n",
       " 'precisely': 457,\n",
       " 'possible': 458,\n",
       " 'fig': 459,\n",
       " 'outline': 460,\n",
       " 'belong': 461,\n",
       " 'fix': 462,\n",
       " 'single': 463,\n",
       " 'collection': 464,\n",
       " 'nth': 465,\n",
       " 'distri': 466,\n",
       " 'bution': 467,\n",
       " 'allow': 468,\n",
       " 'embed': 469,\n",
       " 'sparsity': 470,\n",
       " 'mimic': 471,\n",
       " 'real': 472,\n",
       " 'joint': 473,\n",
       " 'probability': 474,\n",
       " 'refer': 475,\n",
       " 'hyperparameter': 476,\n",
       " 'com': 477,\n",
       " 'put': 478,\n",
       " 'conditional': 479,\n",
       " 'hide': 480,\n",
       " 'formulation': 481,\n",
       " 'compute': 482,\n",
       " 'expression': 483,\n",
       " 'difficult': 484,\n",
       " 'denominator': 485,\n",
       " 'summation': 486,\n",
       " 'combination': 487,\n",
       " 'approximation': 488,\n",
       " 'markov': 489,\n",
       " 'chain': 490,\n",
       " 'variational': 491,\n",
       " 'inference': 492,\n",
       " 'standard': 493,\n",
       " 'non': 494,\n",
       " 'negative': 495,\n",
       " 'algebra': 496,\n",
       " 'multivariate': 497,\n",
       " 'decompose': 498,\n",
       " 'structure': 499,\n",
       " 'axis': 500,\n",
       " 'transform': 501,\n",
       " 'individ': 502,\n",
       " 'ual': 503,\n",
       " 'rmn': 504,\n",
       " 'show': 505,\n",
       " 'equation': 506,\n",
       " 'indicate': 507,\n",
       " 'dimension': 508,\n",
       " 'denote': 509,\n",
       " 'cluster': 510,\n",
       " 'representa': 511,\n",
       " 'rkn': 512,\n",
       " 'intuitive': 513,\n",
       " 'quality': 514,\n",
       " 'measure': 515,\n",
       " 'square': 516,\n",
       " 'difference': 517,\n",
       " 'formally': 518,\n",
       " 'min': 519,\n",
       " 'iteratively': 520,\n",
       " 'multiplicative': 521,\n",
       " 'error': 522,\n",
       " 'minimize': 523,\n",
       " 'module': 524,\n",
       " 'top': 525,\n",
       " 'pre': 526,\n",
       " 'train': 527,\n",
       " 'bidirectional': 528,\n",
       " 'encoder': 529,\n",
       " 'transformer': 530,\n",
       " 'either': 531,\n",
       " 'sentence': 532,\n",
       " 'paragraph': 533,\n",
       " 'numeric': 534,\n",
       " 'second': 535,\n",
       " 'layer': 536,\n",
       " 'dimensionality': 537,\n",
       " 'umap': 538,\n",
       " 'convert': 539,\n",
       " 'hdb': 540,\n",
       " 'mean': 541,\n",
       " 'do': 542,\n",
       " 'assign': 543,\n",
       " 'sure': 544,\n",
       " 'inverse': 545,\n",
       " 'order': 546,\n",
       " 'tft': 547,\n",
       " 'log': 548,\n",
       " 'cid': 549,\n",
       " 'alter': 550,\n",
       " 'procedure': 551,\n",
       " 'rather': 552,\n",
       " 'ument': 553,\n",
       " 'enable': 554,\n",
       " 'output': 555,\n",
       " 'gold': 556,\n",
       " 'intensive': 557,\n",
       " 'expensive': 558,\n",
       " 'strategy': 559,\n",
       " 'implement': 560,\n",
       " 'native': 561,\n",
       " 'automatic': 562,\n",
       " 'surement': 563,\n",
       " 'coherence': 564,\n",
       " 'closely': 565,\n",
       " 'judgment': 566,\n",
       " 'generally': 567,\n",
       " 'degree': 568,\n",
       " 'semantically': 569,\n",
       " 'make': 570,\n",
       " 'together': 571,\n",
       " 'alternatively': 572,\n",
       " 'internal': 573,\n",
       " 'consistency': 574,\n",
       " 'erate': 575,\n",
       " 'normalize': 576,\n",
       " 'pointwise': 577,\n",
       " 'mutual': 578,\n",
       " 'window': 579,\n",
       " 'mathematically': 580,\n",
       " 'score': 581,\n",
       " 'diversity': 582,\n",
       " 'redundancy': 583,\n",
       " 'repetitive': 584,\n",
       " 'ability': 585,\n",
       " 'abstract': 586,\n",
       " 'worth': 587,\n",
       " 'note': 588,\n",
       " 'overlap': 589,\n",
       " 'broad': 590,\n",
       " 'interpret': 591,\n",
       " 'ratio': 592,\n",
       " 'unique': 593,\n",
       " 'experimental': 594,\n",
       " 'setup': 595,\n",
       " 'bbc': 596,\n",
       " 'table': 597,\n",
       " 'aggregate': 598,\n",
       " 'workflow': 599,\n",
       " 'available': 600,\n",
       " 'preprocesse': 601,\n",
       " 'octis': 602,\n",
       " 'open': 603,\n",
       " 'framework': 604,\n",
       " 'pare': 605,\n",
       " 'custom': 606,\n",
       " 'pretraine': 607,\n",
       " 'consistent': 608,\n",
       " 'generic': 609,\n",
       " 'configure': 610,\n",
       " 'unify': 611,\n",
       " 'easy': 612,\n",
       " 'comparison': 613,\n",
       " 'mance': 614,\n",
       " 'adopt': 615,\n",
       " 'pro': 616,\n",
       " 'vide': 617,\n",
       " 'optimization': 618,\n",
       " 'early': 619,\n",
       " 'part': 620,\n",
       " 'library': 621,\n",
       " 'primary': 622,\n",
       " 'actively': 623,\n",
       " 'curate': 624,\n",
       " 'dard': 625,\n",
       " 'benchmarke': 626,\n",
       " 'various': 627,\n",
       " 'come': 628,\n",
       " 'dig': 629,\n",
       " 'search': 630,\n",
       " 'remove': 631,\n",
       " 'shot': 632,\n",
       " 'stub': 633,\n",
       " 'item': 634,\n",
       " 'video': 635,\n",
       " 'total': 636,\n",
       " 'approximate': 637,\n",
       " 'newsgroup': 638,\n",
       " 'panel': 639,\n",
       " 'value': 640,\n",
       " 'continue': 641,\n",
       " 'mode': 642,\n",
       " 'popularly': 643,\n",
       " 'consist': 644,\n",
       " 'equally': 645,\n",
       " 'distribute': 646,\n",
       " 'technology': 647,\n",
       " 'pol': 648,\n",
       " 'itic': 649,\n",
       " 'religion': 650,\n",
       " 'auto': 651,\n",
       " 'sport': 652,\n",
       " 'topical': 653,\n",
       " 'area': 654,\n",
       " 'entertainment': 655,\n",
       " 'random': 656,\n",
       " 'seed': 657,\n",
       " 'multiple': 658,\n",
       " 'average': 659,\n",
       " 'iteration': 660,\n",
       " 'summarize': 661,\n",
       " 'perform': 662,\n",
       " 'drill': 663,\n",
       " 'el': 664,\n",
       " 'calculate': 665,\n",
       " 'respect': 666,\n",
       " 'graph': 667,\n",
       " 'display': 668,\n",
       " 'observe': 669,\n",
       " 'outperform': 670,\n",
       " 'find': 671,\n",
       " 'reach': 672,\n",
       " 'maximum': 673,\n",
       " 'case': 674,\n",
       " 'increase': 675,\n",
       " 'steadily': 676,\n",
       " 'decline': 677,\n",
       " 'achieve': 678,\n",
       " 'respectively': 679,\n",
       " 'diver': 680,\n",
       " 'sity': 681,\n",
       " 'bench': 682,\n",
       " 'mark': 683,\n",
       " 'maximize': 684,\n",
       " 'stable': 685,\n",
       " 'hand': 686,\n",
       " 'continuously': 687,\n",
       " 'suitable': 688,\n",
       " 'advantage': 689,\n",
       " 'dominant': 690,\n",
       " 'keyword': 691,\n",
       " 'partial': 692,\n",
       " 'list': 693,\n",
       " 'water': 694,\n",
       " 'say': 695,\n",
       " 'pollution': 696,\n",
       " 'air': 697,\n",
       " 'develop': 698,\n",
       " 'agreement': 699,\n",
       " 'coal': 700,\n",
       " 'ice': 701,\n",
       " 'warm': 702,\n",
       " 'solar': 703,\n",
       " 'renewable': 704,\n",
       " 'power': 705,\n",
       " 'monsoon': 706,\n",
       " 'rainfall': 707,\n",
       " 'trump': 708,\n",
       " 'specie': 709,\n",
       " 'tiger': 710,\n",
       " 'conservation': 711,\n",
       " 'wildlife': 712,\n",
       " 'clean': 713,\n",
       " 'queensland': 714,\n",
       " 'yoga': 715,\n",
       " 'resolution': 716,\n",
       " 'day': 717,\n",
       " 'coherent': 718,\n",
       " 'directly': 719,\n",
       " 'cover': 720,\n",
       " 'event': 721,\n",
       " 'speaker': 722,\n",
       " 'mention': 723,\n",
       " 'ic': 724,\n",
       " 'merge': 725,\n",
       " 'negotiation': 726,\n",
       " 'obvious': 727,\n",
       " 'point': 728,\n",
       " 'view': 729,\n",
       " 'summit': 730,\n",
       " 'opportunity': 731,\n",
       " 'step': 732,\n",
       " 'examine': 733,\n",
       " 'discard': 734,\n",
       " 'manually': 735,\n",
       " 'sample': 736,\n",
       " 'full': 737,\n",
       " 'infer': 738,\n",
       " 'investigate': 739,\n",
       " 'cosine': 740,\n",
       " 'distance': 741,\n",
       " 'end': 742,\n",
       " 'able': 743,\n",
       " 'peripheral': 744,\n",
       " 'overarch': 745,\n",
       " 'bilateral': 746,\n",
       " 'mul': 747,\n",
       " 'trade': 748,\n",
       " 'regional': 749,\n",
       " 'tilateral': 750,\n",
       " 'visit': 751,\n",
       " 'saarc': 752,\n",
       " 'quad': 753,\n",
       " 'agenda': 754,\n",
       " 'ecosystem': 755,\n",
       " 'land': 756,\n",
       " 'story': 757,\n",
       " 'often': 758,\n",
       " 'overall': 759,\n",
       " 'ation': 760,\n",
       " 'cop': 761,\n",
       " 'primarily': 762,\n",
       " 'glasgow': 763,\n",
       " 'copenhagen': 764,\n",
       " 'key': 765,\n",
       " 'distinction': 766,\n",
       " 'exclusively': 767,\n",
       " 'domestic': 768,\n",
       " 'share': 769,\n",
       " 'electricity': 770,\n",
       " 'regeneration': 771,\n",
       " 'hierarchial': 772,\n",
       " 'national': 773,\n",
       " 'target': 774,\n",
       " 'goal': 775,\n",
       " 'ferent': 776,\n",
       " 'prominent': 777,\n",
       " 'contributor': 778,\n",
       " 'subsume': 779,\n",
       " 'company': 780,\n",
       " 'footprint': 781,\n",
       " 'sustainability': 782,\n",
       " 'practice': 783,\n",
       " 'ceo': 784,\n",
       " 'speech': 785,\n",
       " 'stock': 786,\n",
       " 'product': 787,\n",
       " 'market': 788,\n",
       " 'cle': 789,\n",
       " 'battery': 790,\n",
       " 'track': 791,\n",
       " 'interesting': 792,\n",
       " 'spike': 793,\n",
       " 'sharply': 794,\n",
       " 'great': 795,\n",
       " 'atten': 796,\n",
       " 'insight': 797,\n",
       " 'trend': 798,\n",
       " 'sum': 799,\n",
       " 'mit': 800,\n",
       " 'dominate': 801,\n",
       " 'post': 802,\n",
       " 'move': 803,\n",
       " 'onwards': 804,\n",
       " 'conclusion': 805,\n",
       " 'consensus': 806,\n",
       " 'unassail': 807,\n",
       " 'urgent': 808,\n",
       " 'strong': 809,\n",
       " 'globally': 810,\n",
       " 'challenge': 811,\n",
       " 'deterrent': 812,\n",
       " 'effective': 813,\n",
       " 'state': 814,\n",
       " 'glimpse': 815,\n",
       " 'diverse': 816,\n",
       " 'system': 817,\n",
       " 'response': 818,\n",
       " 'tool': 819,\n",
       " 'mainly': 820,\n",
       " 'traditional': 821,\n",
       " 'titative': 822,\n",
       " 'qualitative': 823,\n",
       " 'research': 824,\n",
       " 'augment': 825,\n",
       " 'exist': 826,\n",
       " 'unfold': 827,\n",
       " 'integration': 828,\n",
       " 'demonstration': 829,\n",
       " 'alternative': 830,\n",
       " 'additionally': 831,\n",
       " 'further': 832,\n",
       " 'longitudinal': 833,\n",
       " 'component': 834,\n",
       " 'determine': 835,\n",
       " 'relative': 836,\n",
       " 'actual': 837,\n",
       " 'icantly': 838,\n",
       " 'comparative': 839,\n",
       " 'demonstrate': 840,\n",
       " 'frame': 841,\n",
       " 'evolution': 842,\n",
       " 'essential': 843,\n",
       " 'play': 844,\n",
       " 'gauge': 845,\n",
       " 'inform': 846}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[id2word.doc2bow(sentence) for sentence in data_lemmatized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1)],\n",
       " [(1, 1)],\n",
       " [(2, 1)],\n",
       " [],\n",
       " [(3, 1)],\n",
       " [],\n",
       " [(4, 1)],\n",
       " [(5, 1)],\n",
       " [(6, 1)],\n",
       " [(7, 1)],\n",
       " [(8, 1)],\n",
       " [(9, 1)],\n",
       " [(10, 1)],\n",
       " [(11, 1)],\n",
       " [(12, 1)],\n",
       " [(13, 1)],\n",
       " [],\n",
       " [(14, 1)],\n",
       " [(15, 1)],\n",
       " [(16, 1)],\n",
       " [(17, 1)],\n",
       " [(18, 1)],\n",
       " [(19, 1)],\n",
       " [(20, 1)],\n",
       " [(21, 1)],\n",
       " [(22, 1)],\n",
       " [(23, 1)],\n",
       " [(24, 1)],\n",
       " [(25, 1)],\n",
       " [(17, 1)],\n",
       " [(26, 1)],\n",
       " [(27, 1)],\n",
       " [(18, 1)],\n",
       " [(28, 1)],\n",
       " [(29, 1)],\n",
       " [(30, 1)],\n",
       " [(31, 1)],\n",
       " [(32, 1)],\n",
       " [(18, 1)],\n",
       " [(33, 1)],\n",
       " [(34, 1)],\n",
       " [(35, 1)],\n",
       " [(36, 1)],\n",
       " [(37, 1)],\n",
       " [(38, 1)],\n",
       " [(12, 1)],\n",
       " [(39, 1)],\n",
       " [(40, 1)],\n",
       " [(41, 1)],\n",
       " [(12, 1)],\n",
       " [(39, 1)],\n",
       " [(18, 1)],\n",
       " [(42, 1)],\n",
       " [(43, 1)],\n",
       " [(44, 1)],\n",
       " [(45, 1)],\n",
       " [(46, 1)],\n",
       " [(47, 1)],\n",
       " [(48, 1)],\n",
       " [(12, 1)],\n",
       " [(18, 1)],\n",
       " [(49, 1)],\n",
       " [(50, 1)],\n",
       " [(18, 1)],\n",
       " [(28, 1)],\n",
       " [(51, 1)],\n",
       " [],\n",
       " [(12, 1)],\n",
       " [(39, 1)],\n",
       " [(52, 1)],\n",
       " [(53, 1)],\n",
       " [(54, 1)],\n",
       " [(18, 1)],\n",
       " [(33, 1)],\n",
       " [(55, 1)],\n",
       " [(12, 1)],\n",
       " [(39, 1)],\n",
       " [(56, 1)],\n",
       " [(57, 1)],\n",
       " [(58, 1)],\n",
       " [(59, 1)],\n",
       " [(60, 1)],\n",
       " [(61, 1)],\n",
       " [],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(59, 1)],\n",
       " [],\n",
       " [(63, 1)],\n",
       " [(64, 1)],\n",
       " [(18, 1)],\n",
       " [(33, 1)],\n",
       " [],\n",
       " [],\n",
       " [(33, 1)],\n",
       " [(65, 1)],\n",
       " [(66, 1)],\n",
       " [(67, 1)],\n",
       " [(68, 1)],\n",
       " [(69, 1)],\n",
       " [(33, 1)],\n",
       " [(70, 1)],\n",
       " [(71, 1)],\n",
       " [(72, 1)],\n",
       " [],\n",
       " [(73, 1)],\n",
       " [(18, 1)],\n",
       " [(33, 1)],\n",
       " [(74, 1)],\n",
       " [(50, 1)],\n",
       " [(75, 1)],\n",
       " [(76, 1)],\n",
       " [(77, 1)],\n",
       " [(78, 1)],\n",
       " [(79, 1)],\n",
       " [(56, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(80, 1)],\n",
       " [],\n",
       " [(81, 1)],\n",
       " [(82, 1)],\n",
       " [(83, 1)],\n",
       " [(12, 1)],\n",
       " [(84, 1)],\n",
       " [(41, 1)],\n",
       " [(30, 1)],\n",
       " [(66, 1)],\n",
       " [(65, 1)],\n",
       " [(85, 1)],\n",
       " [],\n",
       " [(86, 1)],\n",
       " [(75, 1)],\n",
       " [(50, 1)],\n",
       " [(61, 1)],\n",
       " [(6, 1)],\n",
       " [],\n",
       " [],\n",
       " [(12, 1)],\n",
       " [(33, 1)],\n",
       " [(87, 1)],\n",
       " [(88, 1)],\n",
       " [(89, 1)],\n",
       " [(47, 1)],\n",
       " [(90, 1)],\n",
       " [(37, 1)],\n",
       " [(38, 1)],\n",
       " [(52, 1)],\n",
       " [(91, 1)],\n",
       " [],\n",
       " [(92, 1)],\n",
       " [(93, 1)],\n",
       " [(94, 1)],\n",
       " [(12, 1)],\n",
       " [(95, 1)],\n",
       " [(82, 1)],\n",
       " [(96, 1)],\n",
       " [],\n",
       " [(47, 1)],\n",
       " [(97, 1)],\n",
       " [(98, 1)],\n",
       " [(99, 1)],\n",
       " [(18, 1)],\n",
       " [(100, 1)],\n",
       " [(101, 1)],\n",
       " [(102, 1)],\n",
       " [(103, 1)],\n",
       " [(104, 1)],\n",
       " [(0, 1)],\n",
       " [(12, 1)],\n",
       " [(39, 1)],\n",
       " [(34, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [],\n",
       " [(105, 1)],\n",
       " [(106, 1)],\n",
       " [(107, 1)],\n",
       " [(61, 1)],\n",
       " [(108, 1)],\n",
       " [(12, 1)],\n",
       " [(39, 1)],\n",
       " [(109, 1)],\n",
       " [(110, 1)],\n",
       " [(111, 1)],\n",
       " [],\n",
       " [(82, 1)],\n",
       " [(112, 1)],\n",
       " [(113, 1)],\n",
       " [(114, 1)],\n",
       " [],\n",
       " [(5, 1)],\n",
       " [(6, 1)],\n",
       " [(7, 1)],\n",
       " [(115, 1)],\n",
       " [(82, 1)],\n",
       " [(12, 1)],\n",
       " [(116, 1)],\n",
       " [(102, 1)],\n",
       " [(117, 1)],\n",
       " [],\n",
       " [(118, 1)],\n",
       " [(119, 1)],\n",
       " [(120, 1)],\n",
       " [(121, 1)],\n",
       " [(2, 1)],\n",
       " [(122, 1)],\n",
       " [(123, 1)],\n",
       " [(2, 1)],\n",
       " [(124, 1)],\n",
       " [(125, 1)],\n",
       " [(126, 1)],\n",
       " [(127, 1)],\n",
       " [(128, 1)],\n",
       " [(129, 1)],\n",
       " [],\n",
       " [(14, 1)],\n",
       " [(15, 1)],\n",
       " [],\n",
       " [],\n",
       " [(20, 1)],\n",
       " [(130, 1)],\n",
       " [(25, 1)],\n",
       " [(131, 1)],\n",
       " [(28, 1)],\n",
       " [(132, 1)],\n",
       " [(133, 1)],\n",
       " [(134, 1)],\n",
       " [(96, 1)],\n",
       " [(119, 1)],\n",
       " [(123, 1)],\n",
       " [(2, 1)],\n",
       " [(135, 1)],\n",
       " [(136, 1)],\n",
       " [(137, 1)],\n",
       " [(138, 1)],\n",
       " [],\n",
       " [(139, 1)],\n",
       " [(140, 1)],\n",
       " [],\n",
       " [(141, 1)],\n",
       " [(142, 1)],\n",
       " [(143, 1)],\n",
       " [],\n",
       " [(45, 1)],\n",
       " [(89, 1)],\n",
       " [(14, 1)],\n",
       " [(144, 1)],\n",
       " [(12, 1)],\n",
       " [(39, 1)],\n",
       " [(145, 1)],\n",
       " [(33, 1)],\n",
       " [(146, 1)],\n",
       " [(33, 1)],\n",
       " [(34, 1)],\n",
       " [(36, 1)],\n",
       " [(147, 1)],\n",
       " [(128, 1)],\n",
       " [(148, 1)],\n",
       " [(18, 1)],\n",
       " [(149, 1)],\n",
       " [],\n",
       " [(150, 1)],\n",
       " [(151, 1)],\n",
       " [(33, 1)],\n",
       " [(133, 1)],\n",
       " [(18, 1)],\n",
       " [(42, 1)],\n",
       " [(43, 1)],\n",
       " [(152, 1)],\n",
       " [(153, 1)],\n",
       " [(46, 1)],\n",
       " [(154, 1)],\n",
       " [(12, 1)],\n",
       " [(39, 1)],\n",
       " [(18, 1)],\n",
       " [(34, 1)],\n",
       " [(110, 1)],\n",
       " [(52, 1)],\n",
       " [(155, 1)],\n",
       " [(45, 1)],\n",
       " [(131, 1)],\n",
       " [(28, 1)],\n",
       " [(144, 1)],\n",
       " [(110, 1)],\n",
       " [(12, 1)],\n",
       " [(39, 1)],\n",
       " [(156, 1)],\n",
       " [(157, 1)],\n",
       " [(145, 1)],\n",
       " [(6, 1)],\n",
       " [],\n",
       " [(158, 1)],\n",
       " [(159, 1)],\n",
       " [(59, 1)],\n",
       " [(106, 1)],\n",
       " [(107, 1)],\n",
       " [(160, 1)],\n",
       " [(56, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(161, 1)],\n",
       " [(162, 1)],\n",
       " [(46, 1)],\n",
       " [(149, 1)],\n",
       " [(163, 1)],\n",
       " [(164, 1)],\n",
       " [(106, 1)],\n",
       " [(34, 1)],\n",
       " [(157, 1)],\n",
       " [(100, 1)],\n",
       " [(101, 1)],\n",
       " [(21, 1)],\n",
       " [(22, 1)],\n",
       " [],\n",
       " [(89, 1)],\n",
       " [(53, 1)],\n",
       " [(165, 1)],\n",
       " [(166, 1)],\n",
       " [(167, 1)],\n",
       " [(168, 1)],\n",
       " [(169, 1)],\n",
       " [(170, 1)],\n",
       " [(165, 1)],\n",
       " [(41, 1)],\n",
       " [(171, 1)],\n",
       " [],\n",
       " [(172, 1)],\n",
       " [(165, 1), (173, 1)],\n",
       " [],\n",
       " [(174, 1)],\n",
       " [(175, 1)],\n",
       " [(153, 1)],\n",
       " [(12, 1)],\n",
       " [(47, 1)],\n",
       " [(145, 1)],\n",
       " [(18, 1)],\n",
       " [(33, 1)],\n",
       " [(56, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(57, 1)],\n",
       " [(58, 1)],\n",
       " [(176, 1)],\n",
       " [],\n",
       " [(59, 1)],\n",
       " [(46, 1)],\n",
       " [],\n",
       " [(177, 1)],\n",
       " [(178, 1)],\n",
       " [(179, 1)],\n",
       " [(180, 1)],\n",
       " [(181, 1)],\n",
       " [(182, 1)],\n",
       " [(183, 1)],\n",
       " [(184, 1)],\n",
       " [(185, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(186, 1)],\n",
       " [(46, 1)],\n",
       " [(187, 1)],\n",
       " [(76, 1)],\n",
       " [(84, 1)],\n",
       " [(0, 1)],\n",
       " [(188, 1)],\n",
       " [(46, 1)],\n",
       " [(46, 1)],\n",
       " [(189, 1)],\n",
       " [(190, 1)],\n",
       " [],\n",
       " [(191, 1)],\n",
       " [(192, 1)],\n",
       " [(193, 1)],\n",
       " [(194, 1)],\n",
       " [(67, 1)],\n",
       " [(195, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(196, 1)],\n",
       " [(49, 1)],\n",
       " [(197, 1)],\n",
       " [(198, 1)],\n",
       " [(199, 1)],\n",
       " [(200, 1)],\n",
       " [],\n",
       " [(6, 1)],\n",
       " [(201, 1)],\n",
       " [(57, 1)],\n",
       " [(58, 1)],\n",
       " [(176, 1)],\n",
       " [],\n",
       " [(202, 1)],\n",
       " [],\n",
       " [(143, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(203, 1)],\n",
       " [(82, 1)],\n",
       " [(204, 1)],\n",
       " [(205, 1)],\n",
       " [(179, 1)],\n",
       " [(206, 1)],\n",
       " [(207, 1)],\n",
       " [(62, 1)],\n",
       " [(143, 1)],\n",
       " [(208, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(209, 1)],\n",
       " [(210, 1)],\n",
       " [(211, 1)],\n",
       " [(76, 1)],\n",
       " [(59, 1)],\n",
       " [(212, 1)],\n",
       " [(213, 1)],\n",
       " [(214, 1)],\n",
       " [(62, 1)],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [(78, 1)],\n",
       " [(215, 1)],\n",
       " [(76, 1)],\n",
       " [(216, 1)],\n",
       " [(63, 1)],\n",
       " [(115, 1)],\n",
       " [(217, 1)],\n",
       " [],\n",
       " [(218, 1)],\n",
       " [(219, 1)],\n",
       " [(220, 1)],\n",
       " [(183, 1)],\n",
       " [(221, 1)],\n",
       " [(143, 1)],\n",
       " [(222, 1)],\n",
       " [(223, 1)],\n",
       " [],\n",
       " [(143, 1)],\n",
       " [(224, 1)],\n",
       " [(221, 1)],\n",
       " [(76, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(76, 1)],\n",
       " [(225, 1)],\n",
       " [(226, 1)],\n",
       " [(183, 1)],\n",
       " [(227, 1)],\n",
       " [(228, 1)],\n",
       " [(229, 1)],\n",
       " [(183, 1)],\n",
       " [(230, 1)],\n",
       " [(102, 1)],\n",
       " [(231, 1)],\n",
       " [],\n",
       " [(232, 1)],\n",
       " [(233, 1)],\n",
       " [(56, 1)],\n",
       " [(234, 1)],\n",
       " [(234, 1)],\n",
       " [(235, 1)],\n",
       " [],\n",
       " [(236, 1)],\n",
       " [(232, 1)],\n",
       " [(237, 1)],\n",
       " [(233, 1)],\n",
       " [(238, 1)],\n",
       " [(239, 1)],\n",
       " [(240, 1)],\n",
       " [(241, 1)],\n",
       " [(242, 1)],\n",
       " [(237, 1)],\n",
       " [(233, 1)],\n",
       " [(243, 1)],\n",
       " [],\n",
       " [(156, 1)],\n",
       " [(157, 1)],\n",
       " [(145, 1)],\n",
       " [(100, 1)],\n",
       " [(244, 1)],\n",
       " [],\n",
       " [(12, 1)],\n",
       " [(148, 1)],\n",
       " [(22, 1)],\n",
       " [(245, 1)],\n",
       " [(214, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(246, 1)],\n",
       " [(247, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [],\n",
       " [(248, 1)],\n",
       " [(213, 1)],\n",
       " [(56, 1)],\n",
       " [(249, 1)],\n",
       " [(61, 1)],\n",
       " [(50, 1)],\n",
       " [(75, 1)],\n",
       " [(78, 1)],\n",
       " [(250, 1)],\n",
       " [(106, 1)],\n",
       " [(107, 1)],\n",
       " [(134, 1)],\n",
       " [(251, 1)],\n",
       " [(252, 1)],\n",
       " [(253, 1)],\n",
       " [(50, 1)],\n",
       " [(75, 1)],\n",
       " [(208, 1)],\n",
       " [(221, 1)],\n",
       " [(76, 1)],\n",
       " [(62, 1)],\n",
       " [(234, 1)],\n",
       " [(76, 1)],\n",
       " [(62, 1)],\n",
       " [(160, 1)],\n",
       " [(254, 1)],\n",
       " [(61, 1)],\n",
       " [(56, 1)],\n",
       " [],\n",
       " [(63, 1)],\n",
       " [(64, 1)],\n",
       " [(80, 1)],\n",
       " [(255, 1)],\n",
       " [(62, 1)],\n",
       " [(76, 1)],\n",
       " [(256, 1)],\n",
       " [(78, 1)],\n",
       " [(56, 1)],\n",
       " [(257, 1)],\n",
       " [(47, 1)],\n",
       " [],\n",
       " [(71, 1)],\n",
       " [(72, 1)],\n",
       " [],\n",
       " [(253, 1)],\n",
       " [(258, 1)],\n",
       " [(259, 1)],\n",
       " [(260, 1)],\n",
       " [(261, 1)],\n",
       " [(22, 1)],\n",
       " [(156, 1)],\n",
       " [(157, 1)],\n",
       " [(145, 1)],\n",
       " [(12, 1)],\n",
       " [(33, 1)],\n",
       " [(262, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(56, 1)],\n",
       " [(22, 1)],\n",
       " [(260, 1)],\n",
       " [],\n",
       " [(88, 1)],\n",
       " [(67, 1)],\n",
       " [(216, 1)],\n",
       " [(260, 1)],\n",
       " [(263, 1)],\n",
       " [(264, 1)],\n",
       " [(61, 1)],\n",
       " [(93, 1)],\n",
       " [(260, 1)],\n",
       " [(88, 1)],\n",
       " [(93, 1)],\n",
       " [(265, 1)],\n",
       " [(266, 1)],\n",
       " [(80, 1)],\n",
       " [(62, 1)],\n",
       " [(22, 1)],\n",
       " [(138, 1)],\n",
       " [(267, 1)],\n",
       " [],\n",
       " [(59, 1)],\n",
       " [(106, 1)],\n",
       " [(107, 1)],\n",
       " [(171, 1)],\n",
       " [(268, 1)],\n",
       " [(269, 1)],\n",
       " [(102, 1)],\n",
       " [(92, 1)],\n",
       " [(58, 1)],\n",
       " [(106, 1)],\n",
       " [(270, 1)],\n",
       " [(32, 1)],\n",
       " [(271, 1)],\n",
       " [(272, 1)],\n",
       " [(273, 1)],\n",
       " [(274, 1)],\n",
       " [(275, 1)],\n",
       " [(58, 1)],\n",
       " [],\n",
       " [(276, 1)],\n",
       " [(277, 1)],\n",
       " [(162, 1)],\n",
       " [(41, 1)],\n",
       " [(155, 1)],\n",
       " [(278, 1)],\n",
       " [(279, 1)],\n",
       " [(280, 1)],\n",
       " [(281, 1)],\n",
       " [(282, 1)],\n",
       " [(283, 1)],\n",
       " [(152, 1)],\n",
       " [(284, 1)],\n",
       " [(269, 1)],\n",
       " [],\n",
       " [(216, 1)],\n",
       " [],\n",
       " [(106, 1)],\n",
       " [(107, 1)],\n",
       " [(285, 1)],\n",
       " [(106, 1)],\n",
       " [(107, 1)],\n",
       " [(218, 1)],\n",
       " [],\n",
       " [(209, 1)],\n",
       " [(45, 1)],\n",
       " [],\n",
       " [(286, 1)],\n",
       " [(287, 1)],\n",
       " [(288, 1)],\n",
       " [(289, 1)],\n",
       " [(290, 1)],\n",
       " [(291, 1)],\n",
       " [(292, 1)],\n",
       " [(34, 1)],\n",
       " [(293, 1)],\n",
       " [(294, 1)],\n",
       " [(128, 1)],\n",
       " [(295, 1)],\n",
       " [(296, 1)],\n",
       " [(56, 1)],\n",
       " [],\n",
       " [(297, 1)],\n",
       " [(106, 1)],\n",
       " [(148, 1)],\n",
       " [(298, 1)],\n",
       " [(12, 1)],\n",
       " [(39, 1)],\n",
       " [(299, 1)],\n",
       " [(300, 1)],\n",
       " [(301, 1)],\n",
       " [(302, 1)],\n",
       " [(76, 1)],\n",
       " [],\n",
       " [(303, 1)],\n",
       " [(106, 1)],\n",
       " [(34, 1)],\n",
       " [(35, 1)],\n",
       " [(304, 1)],\n",
       " [(33, 1)],\n",
       " [(305, 1)],\n",
       " [(306, 1)],\n",
       " [(182, 1)],\n",
       " [(279, 1)],\n",
       " [(31, 1)],\n",
       " [(307, 1)],\n",
       " [(308, 1)],\n",
       " [(51, 1)],\n",
       " [(12, 1)],\n",
       " [(39, 1)],\n",
       " [(31, 1)],\n",
       " [(309, 1)],\n",
       " [(310, 1)],\n",
       " [(311, 1)],\n",
       " [(312, 1)],\n",
       " [(313, 1)],\n",
       " [(314, 1)],\n",
       " [(315, 1)],\n",
       " [(145, 1)],\n",
       " [(278, 1)],\n",
       " [(12, 1)],\n",
       " [(316, 1)],\n",
       " [(317, 1)],\n",
       " [(19, 1)],\n",
       " [(51, 1)],\n",
       " [(318, 1)],\n",
       " [(12, 1)],\n",
       " [(39, 1)],\n",
       " [(319, 1)],\n",
       " [(189, 1)],\n",
       " [(320, 1)],\n",
       " [(321, 1)],\n",
       " [(322, 1)],\n",
       " [(323, 1)],\n",
       " [(65, 1)],\n",
       " [(324, 1)],\n",
       " [(325, 1)],\n",
       " [(12, 1)],\n",
       " [(326, 1)],\n",
       " [(180, 1)],\n",
       " [(279, 1)],\n",
       " [(327, 1)],\n",
       " [(328, 1)],\n",
       " [(329, 1)],\n",
       " [],\n",
       " [(178, 1)],\n",
       " [(179, 1)],\n",
       " [(59, 1)],\n",
       " [(330, 1)],\n",
       " [],\n",
       " [(51, 1)],\n",
       " [(318, 1)],\n",
       " [(12, 1)],\n",
       " [(319, 1)],\n",
       " [(323, 1)],\n",
       " [(65, 1)],\n",
       " [(157, 1)],\n",
       " [(145, 1)],\n",
       " [(12, 1)],\n",
       " [(33, 1)],\n",
       " [(331, 1)],\n",
       " [(332, 1)],\n",
       " [(56, 1)],\n",
       " [],\n",
       " [(307, 1)],\n",
       " [(12, 1)],\n",
       " [(33, 1)],\n",
       " [(333, 1)],\n",
       " [(334, 1)],\n",
       " [(335, 1)],\n",
       " [(31, 1)],\n",
       " [(106, 1)],\n",
       " [(34, 1)],\n",
       " [(33, 1)],\n",
       " [],\n",
       " [(303, 1)],\n",
       " [(336, 1)],\n",
       " [(337, 1)],\n",
       " [(338, 1)],\n",
       " [],\n",
       " [(85, 1)],\n",
       " [],\n",
       " [(12, 1)],\n",
       " [(39, 1)],\n",
       " [(253, 1)],\n",
       " [(40, 1)],\n",
       " [],\n",
       " [],\n",
       " [(74, 1)],\n",
       " [(161, 1)],\n",
       " [(47, 1)],\n",
       " [(209, 1)],\n",
       " [(339, 1)],\n",
       " [(340, 1)],\n",
       " [(39, 1)],\n",
       " [(341, 1)],\n",
       " [(342, 1)],\n",
       " [(12, 1)],\n",
       " [(107, 1)],\n",
       " [(343, 1)],\n",
       " [],\n",
       " [(344, 1)],\n",
       " [(149, 1)],\n",
       " [(76, 1)],\n",
       " [(345, 1)],\n",
       " [(346, 1)],\n",
       " [(347, 1)],\n",
       " [(348, 1)],\n",
       " [],\n",
       " [(56, 1)],\n",
       " [(349, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(350, 1)],\n",
       " [(46, 1)],\n",
       " [(145, 1)],\n",
       " [(351, 1)],\n",
       " [(352, 1)],\n",
       " [(46, 1)],\n",
       " [(353, 1)],\n",
       " [(354, 1)],\n",
       " [(56, 1)],\n",
       " [],\n",
       " [(33, 1)],\n",
       " [(253, 1)],\n",
       " [(307, 1)],\n",
       " [(12, 1)],\n",
       " [(355, 1)],\n",
       " [(99, 1)],\n",
       " [(48, 1)],\n",
       " [(56, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(356, 1)],\n",
       " [(357, 1)],\n",
       " [(358, 1)],\n",
       " [(56, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(250, 1)],\n",
       " [(213, 1)],\n",
       " [(162, 1)],\n",
       " [(359, 1)],\n",
       " [(360, 1)],\n",
       " [],\n",
       " [],\n",
       " [(361, 1)],\n",
       " [],\n",
       " [(362, 1)],\n",
       " [(363, 1)],\n",
       " [(364, 1)],\n",
       " [(365, 1)],\n",
       " [(366, 1)],\n",
       " [(367, 1)],\n",
       " [(226, 1)],\n",
       " [],\n",
       " [(182, 1)],\n",
       " [(368, 1)],\n",
       " [(369, 1)],\n",
       " [(183, 1)],\n",
       " [(370, 1)],\n",
       " [(233, 1)],\n",
       " [(227, 1)],\n",
       " [(226, 1)],\n",
       " [(167, 1)],\n",
       " [(62, 1)],\n",
       " [(76, 1)],\n",
       " [(220, 1)],\n",
       " [(183, 1)],\n",
       " [(233, 1)],\n",
       " [(232, 1)],\n",
       " [(226, 1)],\n",
       " [(371, 1)],\n",
       " [(372, 1)],\n",
       " [(373, 1)],\n",
       " [(94, 1)],\n",
       " [(183, 1)],\n",
       " [],\n",
       " [(58, 1)],\n",
       " [(374, 1)],\n",
       " [(375, 1)],\n",
       " [(376, 1)],\n",
       " [(377, 1)],\n",
       " [],\n",
       " [(378, 1)],\n",
       " [],\n",
       " [(29, 1)],\n",
       " [(379, 1)],\n",
       " [(46, 1)],\n",
       " [(161, 1)],\n",
       " [(380, 1)],\n",
       " [],\n",
       " [(381, 1)],\n",
       " [(382, 1)],\n",
       " [],\n",
       " [(383, 1)],\n",
       " [(379, 1)],\n",
       " [(110, 1)],\n",
       " [(381, 1)],\n",
       " [(384, 1)],\n",
       " [],\n",
       " [(44, 1)],\n",
       " [(261, 1)],\n",
       " [(385, 1)],\n",
       " [(386, 1)],\n",
       " [(387, 1)],\n",
       " [(193, 1)],\n",
       " [(161, 1)],\n",
       " [(46, 1)],\n",
       " [(115, 1)],\n",
       " [(388, 1)],\n",
       " [(389, 1)],\n",
       " [(390, 1)],\n",
       " [(391, 1)],\n",
       " [(392, 1)],\n",
       " [(393, 1)],\n",
       " [(152, 1)],\n",
       " [(394, 1)],\n",
       " [(46, 1)],\n",
       " [(395, 1)],\n",
       " [],\n",
       " [(396, 1)],\n",
       " [(397, 1)],\n",
       " [(41, 1)],\n",
       " [(398, 1)],\n",
       " [(399, 1)],\n",
       " [(400, 1)],\n",
       " [(4, 1)],\n",
       " [(401, 1)],\n",
       " [(195, 1)],\n",
       " [],\n",
       " [(62, 1)],\n",
       " [(402, 1)],\n",
       " [(403, 1)],\n",
       " [(365, 1)],\n",
       " [(404, 1)],\n",
       " [(84, 1)],\n",
       " [(379, 1)],\n",
       " [(46, 1)],\n",
       " [(405, 1)],\n",
       " [(56, 1)],\n",
       " [(406, 1)],\n",
       " [(407, 1)],\n",
       " [(216, 1)],\n",
       " [(63, 1)],\n",
       " [(219, 1)],\n",
       " [(110, 1)],\n",
       " [(55, 1)],\n",
       " [(221, 1)],\n",
       " [(232, 1)],\n",
       " [(408, 1)],\n",
       " [(409, 1)],\n",
       " [(410, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(411, 1)],\n",
       " [(412, 1)],\n",
       " [(413, 1)],\n",
       " [(134, 1)],\n",
       " [(279, 1)],\n",
       " [(414, 1)],\n",
       " [(231, 1)],\n",
       " [(415, 1)],\n",
       " [(311, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(76, 1)],\n",
       " [(234, 1)],\n",
       " [(100, 1)],\n",
       " [(101, 1)],\n",
       " [(102, 1)],\n",
       " [(103, 1)],\n",
       " [(234, 1)],\n",
       " [(236, 1)],\n",
       " [(233, 1)],\n",
       " [(232, 1)],\n",
       " [(183, 1)],\n",
       " [(241, 1)],\n",
       " [(242, 1)],\n",
       " [(174, 1)],\n",
       " [(175, 1)],\n",
       " [(153, 1)],\n",
       " [(12, 1)],\n",
       " [(47, 1)],\n",
       " [(145, 1)],\n",
       " [(18, 1)],\n",
       " [(33, 1)],\n",
       " [(56, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(416, 1)],\n",
       " [(417, 1)],\n",
       " [(232, 1)],\n",
       " [],\n",
       " [(418, 1)],\n",
       " [(419, 1)],\n",
       " [(420, 1)],\n",
       " [(239, 1)],\n",
       " [(183, 1)],\n",
       " [(234, 1)],\n",
       " [(243, 1)],\n",
       " [(421, 1)],\n",
       " [(183, 1)],\n",
       " [(422, 1)],\n",
       " [(235, 1)],\n",
       " [(423, 1)],\n",
       " [(183, 1)],\n",
       " [(424, 1)],\n",
       " [(76, 1)],\n",
       " [(221, 1)],\n",
       " [(232, 1)],\n",
       " [(233, 1)],\n",
       " [(425, 1)],\n",
       " [(234, 1)],\n",
       " [(76, 1)],\n",
       " [(62, 1)],\n",
       " [(151, 1)],\n",
       " [(426, 1)],\n",
       " [],\n",
       " [(75, 1)],\n",
       " [(224, 1)],\n",
       " [(218, 1)],\n",
       " [(234, 1)],\n",
       " [(76, 1)],\n",
       " [(46, 1)],\n",
       " [(62, 1)],\n",
       " [(106, 1)],\n",
       " [(107, 1)],\n",
       " [(427, 1)],\n",
       " [],\n",
       " [(268, 1)],\n",
       " [(428, 1)],\n",
       " [(106, 1)],\n",
       " [(107, 1)],\n",
       " [(76, 1)],\n",
       " [(218, 1)],\n",
       " [(429, 1)],\n",
       " [(430, 1)],\n",
       " [(61, 1)],\n",
       " [(62, 1)],\n",
       " [(431, 1)],\n",
       " [],\n",
       " [],\n",
       " [(432, 1)],\n",
       " [(80, 1)],\n",
       " [(433, 1)],\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=gensim.models.LdaModel(corpus=corpus, id2word=id2word, num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.332*\"topic\" + 0.074*\"figure\" + 0.038*\"action\" + 0.024*\"observe\" + 0.024*\"relationship\" + 0.017*\"choose\" + 0.014*\"context\" + 0.014*\"text\" + 0.010*\"commercial\" + 0.010*\"industry\"'),\n",
       " (1,\n",
       "  '0.123*\"energy\" + 0.050*\"represent\" + 0.047*\"word\" + 0.038*\"finally\" + 0.028*\"social\" + 0.025*\"recent\" + 0.025*\"technique\" + 0.024*\"well\" + 0.024*\"provide\" + 0.019*\"debate\"'),\n",
       " (2,\n",
       "  '0.078*\"article\" + 0.073*\"analysis\" + 0.063*\"experiment\" + 0.041*\"see\" + 0.040*\"study\" + 0.032*\"second\" + 0.032*\"stakeholder\" + 0.021*\"continue\" + 0.021*\"engagement\" + 0.017*\"several\"'),\n",
       " (3,\n",
       "  '0.154*\"theme\" + 0.085*\"business\" + 0.049*\"high\" + 0.034*\"give\" + 0.030*\"score\" + 0.029*\"topic\" + 0.021*\"present\" + 0.021*\"fig\" + 0.016*\"document\" + 0.015*\"coherence\"'),\n",
       " (4,\n",
       "  '0.049*\"volume\" + 0.043*\"result\" + 0.043*\"economic\" + 0.042*\"term\" + 0.038*\"base\" + 0.035*\"gain\" + 0.033*\"natural\" + 0.024*\"top\" + 0.022*\"representation\" + 0.022*\"experimental\"'),\n",
       " (5,\n",
       "  '0.076*\"dataset\" + 0.049*\"base\" + 0.046*\"discourse\" + 0.045*\"resource\" + 0.041*\"time\" + 0.029*\"contribution\" + 0.029*\"report\" + 0.028*\"impact\" + 0.022*\"understand\" + 0.021*\"show\"'),\n",
       " (6,\n",
       "  '0.072*\"country\" + 0.056*\"bertopic\" + 0.045*\"diversity\" + 0.036*\"umamaheswaran\" + 0.034*\"around\" + 0.034*\"performance\" + 0.033*\"focus\" + 0.027*\"emerge\" + 0.025*\"identify\" + 0.019*\"label\"'),\n",
       " (7,\n",
       "  '0.109*\"use\" + 0.096*\"emission\" + 0.049*\"mapping\" + 0.047*\"value\" + 0.031*\"medium\" + 0.030*\"discuss\" + 0.024*\"role\" + 0.023*\"management\" + 0.021*\"large\" + 0.018*\"cluster\"'),\n",
       " (8,\n",
       "  '0.189*\"news\" + 0.162*\"model\" + 0.037*\"cooperation\" + 0.026*\"embedding\" + 0.023*\"extract\" + 0.018*\"generate\" + 0.018*\"however\" + 0.017*\"first\" + 0.017*\"specifically\" + 0.017*\"relate\"'),\n",
       " (9,\n",
       "  '0.288*\"climate\" + 0.127*\"change\" + 0.051*\"also\" + 0.026*\"category\" + 0.020*\"include\" + 0.019*\"perspective\" + 0.014*\"panel\" + 0.013*\"perform\" + 0.013*\"consist\" + 0.013*\"derive\"')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lda_output(lda_output):\n",
    "    cleaned_output = []\n",
    "    for topic in lda_output:\n",
    "        cleaned_topic = []\n",
    "        for word_prob in topic[1].split('+'):\n",
    "            word = re.findall(r'\"([^\"]*)\"', word_prob)[0]\n",
    "            cleaned_topic.append(word)\n",
    "        cleaned_output.append(cleaned_topic)\n",
    "    return cleaned_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics=clean_lda_output(model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 : topic figure action observe relationship choose context text commercial industry\n",
      "Topic 2 : energy represent word finally social recent technique well provide debate\n",
      "Topic 3 : article analysis experiment see study second stakeholder continue engagement several\n",
      "Topic 4 : theme business high give score topic present fig document coherence\n",
      "Topic 5 : volume result economic term base gain natural top representation experimental\n",
      "Topic 6 : dataset base discourse resource time contribution report impact understand show\n",
      "Topic 7 : country bertopic diversity umamaheswaran around performance focus emerge identify label\n",
      "Topic 8 : use emission mapping value medium discuss role management large cluster\n",
      "Topic 9 : news model cooperation embedding extract generate however first specifically relate\n",
      "Topic 10 : climate change also category include perspective panel perform consist derive\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for topic in topics:\n",
    "    print(\"Topic\",i,\":\",(\" \".join(topic)))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots(figsize=(10,10))\n",
    "pyLDAvis.enable_notebook()\n",
    "# vis=pyLDAvis.gensim_models.prepare(model,corpus=corpus,dictionary=id2word)\n",
    "vis=pyLDAvis.gensim_models.prepare(topic_model=model,corpus=corpus,dictionary=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el285841652159311408638419384\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el285841652159311408638419384_data = {\"mdsDat\": {\"x\": [-0.011677756626303597, 0.2871221071201664, -0.05782713825990547, 0.0748774883002833, -0.09954527656932116, -0.06220284277003723, -0.07403302624503812, -0.01662849553590816, -0.05360427571704978, 0.013519216303113351], \"y\": [0.24391304199157968, 0.004307369461080937, 0.05353145196292525, -0.14354861485595516, -0.10295731183486793, -0.03939124070489855, 0.019638667635428764, 0.004383035452723874, -0.07184483581029762, 0.031968436702280924], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [10.820901263822865, 10.521426329246134, 10.221608715067404, 10.19424699631058, 10.14610872760545, 9.919700474709996, 9.90949012621246, 9.852548659852363, 9.41584814472603, 8.998120562446713]}, \"tinfo\": {\"Term\": [\"topic\", \"climate\", \"news\", \"model\", \"theme\", \"energy\", \"change\", \"use\", \"emission\", \"business\", \"article\", \"figure\", \"country\", \"dataset\", \"analysis\", \"experiment\", \"base\", \"bertopic\", \"also\", \"word\", \"mapping\", \"value\", \"volume\", \"represent\", \"discourse\", \"high\", \"resource\", \"diversity\", \"term\", \"result\", \"model\", \"cooperation\", \"news\", \"extract\", \"generate\", \"first\", \"scientific\", \"however\", \"suggest\", \"combination\", \"embedding\", \"relate\", \"specifically\", \"discussion\", \"selection\", \"absolute\", \"crucial\", \"issue\", \"information\", \"low\", \"content\", \"enable\", \"sentence\", \"machine\", \"become\", \"range\", \"mutual\", \"attribution\", \"relatively\", \"expression\", \"measure\", \"social\", \"performance\", \"several\", \"business\", \"climate\", \"change\", \"also\", \"perspective\", \"include\", \"consist\", \"category\", \"panel\", \"derive\", \"alternatively\", \"counter\", \"together\", \"display\", \"learn\", \"train\", \"citizen\", \"measure\", \"perform\", \"cid\", \"evaluate\", \"analyze\", \"curate\", \"require\", \"method\", \"frequency\", \"newspaper\", \"set\", \"org\", \"release\", \"expensive\", \"document\", \"study\", \"number\", \"model\", \"figure\", \"embedding\", \"action\", \"topic\", \"observe\", \"relationship\", \"figure\", \"text\", \"commercial\", \"industry\", \"remain\", \"aspect\", \"last\", \"choose\", \"discover\", \"similarity\", \"classical\", \"real\", \"create\", \"sport\", \"way\", \"begin\", \"unique\", \"context\", \"individ\", \"detect\", \"degree\", \"configure\", \"adoption\", \"subjectivity\", \"pol\", \"shot\", \"evaluation\", \"country\", \"coherence\", \"literature\", \"climate\", \"nmf\", \"technique\", \"news\", \"analysis\", \"emission\", \"use\", \"value\", \"discuss\", \"mapping\", \"role\", \"management\", \"large\", \"medium\", \"need\", \"improve\", \"new\", \"cluster\", \"rank\", \"contain\", \"collection\", \"multiple\", \"public\", \"semantic\", \"platform\", \"important\", \"reduce\", \"source\", \"science\", \"transformer\", \"available\", \"update\", \"typically\", \"test\", \"evaluation\", \"perform\", \"climate\", \"base\", \"represent\", \"report\", \"discourse\", \"contribution\", \"resource\", \"show\", \"impact\", \"report\", \"understand\", \"time\", \"similar\", \"different\", \"cli\", \"significantly\", \"attribute\", \"latent\", \"traction\", \"involve\", \"dataset\", \"relevant\", \"combine\", \"section\", \"decade\", \"non\", \"base\", \"tion\", \"assumption\", \"consider\", \"follow\", \"dense\", \"offer\", \"probability\", \"metric\", \"dimensional\", \"experiment\", \"nmf\", \"around\", \"bertopic\", \"emerge\", \"focus\", \"label\", \"country\", \"identify\", \"political\", \"note\", \"diversity\", \"umamaheswaran\", \"aggregate\", \"process\", \"performance\", \"difference\", \"come\", \"specific\", \"octis\", \"help\", \"common\", \"approach\", \"level\", \"rmn\", \"independent\", \"sis\", \"allocation\", \"standard\", \"mean\", \"algorithm\", \"difficult\", \"language\", \"nmf\", \"tion\", \"set\", \"group\", \"run\", \"dataset\", \"change\", \"theme\", \"give\", \"business\", \"high\", \"score\", \"example\", \"present\", \"semantically\", \"fig\", \"coherence\", \"year\", \"apply\", \"community\", \"choice\", \"decompose\", \"hence\", \"tweet\", \"probabilistic\", \"matrix\", \"vector\", \"automation\", \"know\", \"implement\", \"approve\", \"computational\", \"foundational\", \"calculate\", \"unstructure\", \"outline\", \"sure\", \"document\", \"dataset\", \"space\", \"topic\", \"application\", \"language\", \"represent\", \"nmf\", \"category\", \"news\", \"energy\", \"word\", \"finally\", \"well\", \"provide\", \"debate\", \"recent\", \"represent\", \"framework\", \"make\", \"embed\", \"policy\", \"food\", \"technique\", \"thus\", \"decision\", \"social\", \"daily\", \"work\", \"element\", \"underlie\", \"reduction\", \"form\", \"unsupervise\", \"define\", \"context\", \"notably\", \"multinomial\", \"internal\", \"automatically\", \"metric\", \"frequency\", \"therefore\", \"matrix\", \"literature\", \"evaluate\", \"time\", \"figure\", \"change\", \"topic\", \"volume\", \"term\", \"gain\", \"natural\", \"experimental\", \"result\", \"economic\", \"representation\", \"top\", \"scale\", \"yield\", \"distribution\", \"sector\", \"importance\", \"literature\", \"review\", \"shortcoming\", \"development\", \"semeval\", \"formulation\", \"joint\", \"extant\", \"group\", \"log\", \"prior\", \"pattern\", \"ument\", \"output\", \"nascent\", \"network\", \"base\", \"datum\", \"dimensional\", \"dirichlet\", \"nmf\", \"analysis\", \"change\", \"dataset\", \"business\", \"article\", \"performance\", \"science\", \"second\", \"article\", \"see\", \"stakeholder\", \"continue\", \"experiment\", \"engagement\", \"analysis\", \"consistent\", \"mate\", \"domain\", \"select\", \"application\", \"several\", \"study\", \"evolve\", \"number\", \"mechanism\", \"follow\", \"commonly\", \"partisan\", \"square\", \"pointwise\", \"actively\", \"remove\", \"module\", \"certain\", \"lazaro\", \"comparison\", \"repetitive\", \"popular\", \"document\", \"define\", \"evaluate\", \"common\", \"news\", \"science\", \"umamaheswaran\", \"high\", \"climate\", \"social\"], \"Freq\": [98.0, 86.0, 59.0, 48.0, 42.0, 32.0, 39.0, 31.0, 27.0, 25.0, 20.0, 22.0, 21.0, 28.0, 21.0, 17.0, 25.0, 16.0, 15.0, 13.0, 14.0, 13.0, 13.0, 16.0, 13.0, 15.0, 13.0, 14.0, 11.0, 12.0, 46.01750309235578, 10.443126252159342, 53.73875250595087, 6.648372911653953, 5.182405848525416, 4.815810730031976, 4.449193322378717, 5.18235861673437, 3.1841792211981765, 2.8176745860797183, 7.381029270193729, 4.8156419692504855, 4.81572210408698, 2.451195689633908, 2.4511781767226215, 2.451142354858626, 2.4511282914601686, 1.9190211691938028, 3.5506639552810824, 1.9189423610930125, 1.5524446250002124, 0.8196681968792664, 0.8196482295069281, 0.8196396057248551, 0.819631711647419, 0.8196143314097026, 0.4532254208129853, 0.4532224688260449, 0.4532189529764305, 0.4532045910624397, 0.8197146326288902, 1.1861797310233833, 1.1862396994771833, 0.8196912157437227, 1.1863530027062656, 79.87259124375372, 35.17347671810643, 14.172936160465863, 5.321602366745019, 5.624201771006055, 3.6723743376912674, 7.273290994177135, 3.9750328247536637, 3.67231396487241, 2.0232568631370045, 2.0232505420512696, 2.0232213876558385, 2.023205778444125, 1.886299059530648, 2.023177785064441, 2.023154951754745, 2.188880532548476, 3.6723975580062125, 1.886329890948825, 2.6281056899121524, 0.9789536428889714, 0.6765053641777038, 0.6764180944940347, 2.325594264844409, 0.9789644790359459, 0.6765057511829529, 1.1556789448757494, 0.37414516141104087, 0.374074339450458, 0.3740707274014665, 3.096046120751077, 3.6722713942950103, 0.9790007930284852, 0.9795609186256635, 0.6768338671333037, 0.6766166281868162, 10.302356311073627, 89.35730786731658, 6.360271145374603, 6.360259114104764, 19.995024012629433, 3.700572212115669, 2.7798201339379633, 2.7797291474598054, 2.418155149546617, 2.4181586586669863, 2.418131838961304, 4.587213384436534, 2.7796902964842833, 1.1700299631849005, 0.8085856147368005, 0.8085756513414649, 0.8085566644937502, 0.8085532806991079, 0.808549583590147, 0.808529656799476, 0.8084484457280623, 3.78945121284011, 0.44711125077696645, 0.44710545446207, 0.4471009114044485, 0.44708850415742696, 0.4470854963399672, 0.44708477571703414, 0.4470804519794357, 0.44708004467082135, 0.808600716486963, 2.4186998151582895, 0.8086104292308435, 0.8086497815092755, 1.8942413478090327, 0.8086764758892309, 0.8087240996656772, 1.1706696256980107, 0.808876558413169, 25.788262203797803, 29.334097585772955, 12.50875016627069, 8.018196983704533, 13.20439289772812, 6.469414807437269, 6.121203972091892, 5.616278362393416, 8.209057594269623, 4.572307304756316, 4.224228459114147, 4.224219959777198, 4.9201416694709526, 2.675297044489279, 2.327248946448426, 2.327231197833032, 2.3271852014213086, 2.675197802231375, 1.8219791136035162, 2.6752422987601077, 2.327110457252257, 2.327084709260912, 1.126135147844795, 3.7189386278293575, 0.7781842922178579, 0.7781738555320455, 0.7781516072676791, 0.7781401081647481, 0.7781244843835919, 1.1260927761502995, 2.3272774442252544, 3.0540669958541815, 1.8222300940240093, 1.126237264878432, 0.7783447172027697, 12.388913660419382, 7.809191147894784, 11.96828530375622, 5.519277643618788, 7.388694157666057, 7.808940357429042, 5.939210852899726, 10.938856125102522, 5.098904555822542, 5.51908706276883, 2.8092647316182884, 2.8092264164082446, 2.809194072399766, 2.619343450632484, 5.098805035796453, 3.2292641217165743, 20.328081943186262, 2.8091239107813735, 2.809160484390961, 3.2291832616953777, 0.9393784208478673, 1.15445991942915, 13.038651049940993, 2.8092786644219405, 1.3592874343224621, 1.3592591955150595, 2.8092410956120926, 0.9393005464274533, 0.5194290395626905, 0.5194066475568206, 1.9848889783171233, 0.9393113070302741, 1.779484371672533, 0.9395435618911572, 9.002928116176268, 14.583296455262127, 6.961483142208158, 8.627781097318032, 4.920103358742858, 18.833960486632172, 6.586618291197905, 4.545539162439596, 4.545518243099142, 11.79272941313926, 9.376973707434786, 2.878704601922262, 4.920025519336517, 8.833418213981052, 2.504295354687247, 2.5042335696584646, 3.2530333340213433, 4.001773016092453, 2.504214839551314, 3.2531067949610772, 1.960488829148065, 1.5860606085577549, 0.8374307530240287, 0.8374171919399553, 0.8374049079086422, 0.8373681774387752, 0.8373602110620326, 0.8373613664907205, 0.8373557109713535, 0.8373516973769641, 2.8784941922769978, 3.2528825201715583, 2.5042413535990984, 1.536700573387987, 2.504322355231322, 1.211661820992967, 2.7097743600561377, 1.2126171780817248, 40.1465518394764, 8.985908800703745, 22.222770037139547, 12.690515743440756, 7.736947407148235, 3.1995640236181044, 5.4683246052508165, 2.783325368348659, 5.468256079879824, 4.031783999519117, 2.7833054425315615, 2.7832580579665134, 2.7832728808304514, 0.9307202759987876, 0.9306806066129202, 0.930666937988387, 0.930664325762365, 1.3466885355002451, 2.5950332007168173, 2.179011542455517, 0.5146336765835938, 0.5146299101181668, 0.5146234706772757, 0.5146169097374997, 0.5146104095471662, 0.514612353529322, 0.5146077973211443, 0.5146027551174276, 0.5146001428914057, 0.5145964371754211, 4.259254211528542, 3.8438266828136176, 0.9307221592315011, 7.589883699072841, 0.9307301174084516, 0.9307607958768483, 1.3470834068756488, 0.9308533172775775, 0.9310351403585903, 0.7246097501749066, 31.93413772637473, 12.288751282968786, 9.773408455322587, 6.154174996970825, 6.153936294721797, 4.9345070106453806, 6.560423514279445, 13.024639391735878, 3.1250321015304108, 2.718582813403023, 2.7185779813736906, 2.7185610692710265, 2.718543673965429, 6.5604225478735785, 3.1250879114692016, 2.718489313635438, 7.373282467893693, 1.3155347234790438, 1.3154435189253917, 1.315439774102659, 1.3153541263827393, 0.9090943746059216, 0.9090908109842888, 0.9089607085945096, 1.7218090921445928, 4.20569492563289, 0.5026661964066808, 0.5026502809100666, 0.502645630081834, 0.5026405866512181, 1.929032119680247, 0.9090868245600895, 0.9090240685791329, 1.3155073017125816, 1.3155703596953714, 1.109916775286933, 1.3155661316697056, 1.3159709349270408, 1.316153706436545, 1.3167630253353808, 12.058059694343445, 10.317189728380312, 8.735618258755926, 8.289612762145108, 5.412744130028093, 10.720069988077837, 10.720010879516513, 5.455666181261835, 5.858627715231181, 2.9820737900246526, 2.982037078066643, 4.117569802514464, 3.873793165617572, 2.981968272006977, 3.8736361585015557, 0.9971880976519288, 0.9971406491778976, 0.9971293354298316, 0.9970928543646397, 0.9970952787392252, 0.9970920462397778, 0.9970654935657457, 2.9820950321638784, 0.9971023209701642, 0.9971076315049706, 0.5514039118068474, 0.551377994088064, 0.5513648909206611, 0.5513547316366836, 0.5513500560571257, 9.425489055105622, 0.9971334915005498, 0.9971227549845281, 0.9971326833756878, 1.4431638090892682, 1.4434633925201963, 0.9977364680938972, 0.997612363204399, 0.9974844485834095, 0.9973650769966735, 0.9973643843182206, 0.9971830180099401, 7.595559651943762, 18.547405240348663, 9.822853125782464, 7.59551552206022, 4.959604039352302, 14.909611246378923, 4.959444730472714, 17.32090338705061, 2.7324440584113177, 2.7324021350219523, 2.732378525534257, 2.732318729542057, 3.549409780613051, 3.957819496431946, 9.413397531723609, 1.3221580095142975, 3.95773035406719, 0.9136128701152816, 2.732426406457901, 0.5051891430583619, 0.5051812548416788, 0.5051819167899319, 0.5051770625027422, 0.5051756282815271, 0.5051703878578564, 0.5051662506812742, 0.5051690087989956, 0.5051673539283628, 0.5051609550952492, 0.5051553836974518, 0.9136101119975601, 2.547497040381003, 0.913681216272418, 1.1203255741440976, 0.9136675360085199, 2.751451019902511, 0.913734282457378, 0.9139039618595991, 0.9141244457902485, 0.5059059778541563, 0.5053729440233167], \"Total\": [98.0, 86.0, 59.0, 48.0, 42.0, 32.0, 39.0, 31.0, 27.0, 25.0, 20.0, 22.0, 21.0, 28.0, 21.0, 17.0, 25.0, 16.0, 15.0, 13.0, 14.0, 13.0, 13.0, 16.0, 13.0, 15.0, 13.0, 14.0, 11.0, 12.0, 48.57795550887146, 11.267848931965199, 59.58243477641002, 7.47363646892438, 6.0073221973181266, 5.640663829282312, 5.273897088123722, 6.381780949873084, 4.009144014541622, 3.6425417080189426, 9.62688314596804, 6.395115539430326, 6.431348464390031, 3.2758950223658383, 3.275884659570174, 3.27586544582967, 3.275865226863194, 2.7441583906608247, 5.09838840696706, 3.190012150279513, 2.725598218395462, 1.6444694573205316, 1.6444581468548307, 1.6444582384010744, 1.6444449940252917, 2.0903277543335856, 1.277917540084095, 1.2779145319103216, 1.2779146333494809, 1.2779105577200767, 3.7624887814241745, 10.8529357684422, 11.64842975120395, 5.951973714769258, 25.37680379449957, 86.68572275791603, 39.25055033790012, 15.833775062478937, 6.161472145432901, 6.872873602602066, 4.512274349171277, 8.945993192766132, 5.260955775903357, 4.878851270279821, 2.8631104012144317, 2.863112281453015, 2.863098119897294, 2.863090490815785, 2.7268002319426206, 3.269650098544287, 3.2793114704635307, 3.7624887814241745, 6.7572779364208575, 3.539822855318072, 5.505988542806374, 2.167061688759453, 1.5164912977846488, 1.5164700532788802, 5.494088553202219, 2.6321004943377218, 1.8780493110940326, 3.443746639347916, 1.213936428402559, 1.2139275015906295, 1.2139258055159106, 11.646790937486058, 14.593981343593795, 6.500860632948228, 48.57795550887146, 22.646828912833147, 9.62688314596804, 11.128255177588882, 98.90195258241536, 7.186152030275866, 7.186143770722684, 22.646828912833147, 4.527440108387565, 3.60577913219093, 3.6057318078791973, 3.244026172679505, 3.244031339255724, 3.2440166756184237, 6.20830930346952, 4.051643284247767, 2.3626991389500325, 1.6345675412407674, 1.6345764972260945, 1.6345597548291166, 1.6345604044300002, 1.6345607057887814, 1.634543542143292, 1.6345019657511843, 9.394471783977856, 1.27297805235036, 1.2729760248818105, 1.2729716857352218, 1.2729610678264403, 1.2729639130659425, 1.272965121832177, 1.272958696736279, 1.2729601813961444, 2.6786173571575076, 21.990011140951594, 5.568277325520302, 6.6226610551348575, 86.68572275791603, 8.112098657478176, 8.704557392559853, 59.58243477641002, 21.805502480767956, 27.06387512557202, 31.032724745387885, 13.338107742534953, 8.847296535122457, 14.849289059255543, 7.298647605870968, 6.950312814474781, 6.445785004646868, 9.455139361793362, 5.401545012952839, 5.053332331429884, 5.053332889752807, 6.354696041188539, 3.5044943747638824, 3.1563502332012185, 3.156338836386594, 3.156323355127188, 3.8660520574185906, 2.651510592691993, 3.924526197045778, 3.602199326826655, 3.8845227580776998, 1.9554486111582494, 6.673608363545615, 1.6074051775172182, 1.6073922638032507, 1.607385877329476, 1.6073809411859763, 1.6073589040711675, 2.6786173571575076, 6.7572779364208575, 86.68572275791603, 25.215293393521392, 16.133698663804026, 9.317297078605561, 13.647133200518192, 8.621363153148316, 13.226694321506827, 6.331432791092488, 8.548963699266281, 9.317297078605561, 7.1677121861466775, 13.318959270179844, 6.272676437711973, 7.080324093056229, 3.621291126714715, 3.6212726840146185, 3.621255168859203, 3.431892821690035, 6.693834025469704, 4.487340774912133, 28.807181333091904, 3.987763483973103, 4.027745507837349, 5.21601985204964, 1.7515093811309037, 2.1648679971196243, 25.215293393521392, 6.037045839841138, 2.9653815144254194, 2.965367327585514, 6.257094778573576, 2.117990534168992, 1.33144348510934, 1.3314346342666636, 5.4244596918225465, 3.0048245228520876, 17.70813404024572, 8.112098657478176, 9.825857088155104, 16.18843690266624, 7.7844488373537795, 9.7989256989513, 5.743062659561888, 21.990011140951594, 7.75765402344216, 5.368383491366451, 5.368373391852528, 14.137782835944355, 11.36550811973806, 3.701674663687478, 6.45265380467739, 11.64842975120395, 3.3271247584893637, 3.3270907344744742, 4.437738630239545, 5.679540571023595, 3.6936668976307936, 4.893321329998577, 3.1502632402172033, 2.8292949187994285, 1.6603369516670332, 1.660340350052065, 1.6603426330750255, 1.660316723750694, 1.6603136476580278, 1.6603209824885514, 1.6603114429036532, 1.6603081150215064, 5.708545836634246, 8.112098657478176, 6.037045839841138, 3.443746639347916, 6.203779190838607, 2.8871338797046393, 28.807181333091904, 39.25055033790012, 42.12977825715765, 10.205803269826859, 25.37680379449957, 15.47707534413717, 9.681407070154998, 4.012634022161966, 7.062466750924283, 3.596300566704592, 7.068057176594971, 5.568277325520302, 3.962854820670188, 3.96282979318064, 4.042210808082051, 1.74377405409385, 1.7437612997774283, 1.7437542863887086, 1.74375466807945, 2.526435998826759, 5.036631843363844, 4.365542225285046, 1.327575689584617, 1.327567590726857, 1.3275756078576826, 1.3275663288544952, 1.3275585553526332, 1.3275686536744693, 1.327558536531411, 1.327551693352007, 1.327558701922173, 1.327551129706729, 11.646790937486058, 28.807181333091904, 3.3199023463778823, 98.90195258241536, 5.196789773953063, 5.708545836634246, 16.133698663804026, 8.112098657478176, 8.945993192766132, 59.58243477641002, 32.74949074155874, 13.107126667433835, 10.58873785735036, 6.96973539177865, 6.9696187923717385, 5.7497612989125555, 7.750631273949734, 16.133698663804026, 3.940438627562426, 3.533822911740404, 3.533819586775256, 3.53381292795998, 3.533802758883135, 8.704557392559853, 4.356672143810193, 3.8818089161449456, 10.8529357684422, 2.1309614486362074, 2.130899372314279, 2.1309276070456775, 2.1308941660215823, 1.7244283782059378, 1.724427459644001, 1.7243475470366076, 3.3545569494181007, 9.394471783977856, 1.3178987504867183, 1.3178901493615809, 1.3178846969207412, 1.3178799769404466, 5.4244596918225465, 2.6321004943377218, 2.953245111664886, 5.036631843363844, 6.6226610551348575, 5.505988542806374, 13.318959270179844, 22.646828912833147, 39.25055033790012, 98.90195258241536, 13.280744190610243, 11.498759520736979, 9.889830551993281, 9.470217474121107, 6.218697933199863, 12.652272065296213, 12.69718443792749, 6.682631708913385, 7.434945472138531, 3.7879974194327315, 3.7879821710590704, 5.340636390850456, 5.088531901585706, 4.14956266659393, 6.6226610551348575, 1.8031861174051558, 1.8031774899834094, 1.8031607941531833, 1.803137786938377, 1.8031483862981, 1.8031441938116886, 1.8031314297919019, 6.203779190838607, 2.1776063784179875, 2.2116938666898323, 1.3572915354354715, 1.3572828898646543, 1.3572743064753612, 1.3572667420385087, 1.3572644542854615, 25.215293393521392, 2.5859005362412115, 3.0048245228520876, 3.0478351927281837, 8.112098657478176, 21.805502480767956, 39.25055033790012, 28.807181333091904, 25.37680379449957, 20.61635985109521, 11.64842975120395, 6.673608363545615, 8.410460229030928, 20.61635985109521, 10.999390494260346, 8.826664112395486, 5.774374059031821, 17.70813404024572, 6.140893170237726, 21.805502480767956, 3.5471850931266617, 3.5471589019050134, 3.547144482716, 3.9137006878794254, 5.196789773953063, 5.951973714769258, 14.593981343593795, 2.1371091187077638, 6.500860632948228, 1.7285008523654417, 6.257094778573576, 1.3199429471012318, 1.3199330359114123, 1.3199391550887596, 1.3199333236374071, 1.3199345814122574, 1.319935282769116, 1.319924571315167, 1.3199318977806032, 1.3199318897696344, 1.319928909105963, 1.3199223550893349, 2.5111875805187847, 11.646790937486058, 3.3545569494181007, 5.505988542806374, 4.893321329998577, 59.58243477641002, 6.673608363545615, 11.36550811973806, 15.47707534413717, 86.68572275791603, 10.8529357684422], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -1.8232, -3.3062, -1.6681, -3.7578, -4.0069, -4.0803, -4.1595, -4.0069, -4.494, -4.6163, -3.6533, -4.0803, -4.0803, -4.7556, -4.7556, -4.7556, -4.7556, -5.0004, -4.3851, -5.0004, -5.2124, -5.851, -5.8511, -5.8511, -5.8511, -5.8511, -6.4436, -6.4436, -6.4436, -6.4436, -5.851, -5.4815, -5.4814, -5.851, -5.4813, -1.2437, -2.0638, -2.9728, -3.9523, -3.897, -4.3233, -3.6399, -4.2441, -4.3233, -4.9194, -4.9194, -4.9194, -4.9194, -4.9895, -4.9195, -4.9195, -4.8407, -4.3233, -4.9895, -4.6579, -5.6454, -6.0149, -6.0151, -4.7801, -5.6454, -6.0149, -5.4794, -6.6072, -6.6074, -6.6074, -4.494, -4.3233, -5.6453, -5.6448, -6.0145, -6.0148, -3.2628, -1.1026, -3.7451, -3.7451, -2.5997, -4.2867, -4.5728, -4.5729, -4.7122, -4.7122, -4.7122, -4.0719, -4.5729, -5.4382, -5.8077, -5.8077, -5.8077, -5.8077, -5.8077, -5.8078, -5.8079, -4.263, -6.4002, -6.4002, -6.4002, -6.4002, -6.4002, -6.4002, -6.4002, -6.4002, -5.8077, -4.712, -5.8077, -5.8076, -4.9564, -5.8076, -5.8075, -5.4376, -5.8073, -2.3426, -2.2138, -3.0661, -3.5108, -3.012, -3.7254, -3.7808, -3.8669, -3.4873, -4.0725, -4.1517, -4.1517, -3.9992, -4.6085, -4.7478, -4.7479, -4.7479, -4.6085, -4.9926, -4.6085, -4.7479, -4.7479, -5.4737, -4.2791, -5.8433, -5.8433, -5.8434, -5.8434, -5.8434, -5.4738, -4.7478, -4.4761, -4.9925, -5.4737, -5.8431, -3.071, -3.5325, -3.1055, -3.8796, -3.5878, -3.5325, -3.8062, -3.1955, -3.9588, -3.8796, -4.5549, -4.5549, -4.5549, -4.6249, -3.9588, -4.4155, -2.5758, -4.5549, -4.5549, -4.4156, -5.6503, -5.4442, -3.0199, -4.5549, -5.2808, -5.2809, -4.5549, -5.6504, -6.2428, -6.2429, -4.9022, -5.6504, -5.0115, -5.6502, -3.3677, -2.8854, -3.6248, -3.4102, -3.9719, -2.6296, -3.6802, -4.0511, -4.0511, -3.0977, -3.327, -4.5079, -3.9719, -3.3867, -4.6472, -4.6472, -4.3856, -4.1785, -4.6473, -4.3856, -4.892, -5.104, -5.7426, -5.7427, -5.7427, -5.7427, -5.7427, -5.7427, -5.7427, -5.7427, -4.508, -4.3857, -4.6472, -5.1356, -4.6472, -5.3732, -4.5684, -5.3725, -1.8717, -3.3685, -2.4631, -3.0233, -3.5182, -4.4012, -3.8652, -4.5406, -3.8652, -4.17, -4.5406, -4.5406, -4.5406, -5.636, -5.636, -5.6361, -5.6361, -5.2666, -4.6106, -4.7853, -6.2285, -6.2285, -6.2285, -6.2285, -6.2285, -6.2285, -6.2286, -6.2286, -6.2286, -6.2286, -4.1151, -4.2177, -5.636, -3.5374, -5.636, -5.636, -5.2663, -5.6359, -5.6357, -5.8863, -2.0948, -3.0498, -3.2788, -3.7413, -3.7413, -3.9622, -3.6774, -2.9916, -4.419, -4.5583, -4.5583, -4.5583, -4.5583, -3.6774, -4.419, -4.5584, -3.5606, -5.2842, -5.2843, -5.2843, -5.2843, -5.6537, -5.6537, -5.6539, -5.0151, -4.122, -6.2463, -6.2463, -6.2463, -6.2463, -4.9014, -5.6538, -5.6538, -5.2842, -5.2842, -5.4542, -5.2842, -5.2839, -5.2837, -5.2833, -3.0234, -3.1793, -3.3457, -3.3981, -3.8243, -3.141, -3.141, -3.8164, -3.7452, -4.4205, -4.4205, -4.0978, -4.1589, -4.4205, -4.1589, -5.5159, -5.516, -5.516, -5.516, -5.516, -5.516, -5.516, -4.4205, -5.516, -5.516, -6.1084, -6.1084, -6.1085, -6.1085, -6.1085, -3.2697, -5.516, -5.516, -5.516, -5.1463, -5.1461, -5.5154, -5.5155, -5.5156, -5.5157, -5.5157, -5.5159, -3.4402, -2.5474, -3.183, -3.4402, -3.8664, -2.7657, -3.8664, -2.6158, -4.4625, -4.4625, -4.4626, -4.4626, -4.2009, -4.092, -3.2256, -5.1885, -4.0921, -5.5581, -4.4625, -6.1505, -6.1506, -6.1506, -6.1506, -6.1506, -6.1506, -6.1506, -6.1506, -6.1506, -6.1506, -6.1506, -5.5581, -4.5326, -5.558, -5.3541, -5.558, -4.4556, -5.5579, -5.5578, -5.5575, -6.1491, -6.1502], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.1695, 2.1477, 2.1205, 2.1067, 2.076, 2.0656, 2.0536, 2.0155, 1.9933, 1.9669, 1.958, 1.94, 1.9344, 1.9337, 1.9337, 1.9337, 1.9337, 1.866, 1.8619, 1.7154, 1.6608, 1.5274, 1.5274, 1.5274, 1.5274, 1.2874, 1.1871, 1.1871, 1.1871, 1.1871, 0.6998, 0.01, -0.0607, 0.2411, -0.8393, 2.1699, 2.1421, 2.1409, 2.1052, 2.0513, 2.0458, 2.0448, 1.9715, 1.9677, 1.9046, 1.9046, 1.9045, 1.9045, 1.8832, 1.7717, 1.7688, 1.7101, 1.642, 1.6223, 1.5122, 1.4571, 1.4445, 1.4444, 1.3921, 1.2627, 1.2307, 1.1599, 1.0748, 1.0746, 1.0746, 0.9269, 0.872, 0.3586, -1.6521, -1.2586, -0.4035, 2.2036, 2.1792, 2.1586, 2.1586, 2.1561, 2.079, 2.0205, 2.0205, 1.9869, 1.9869, 1.9868, 1.9781, 1.9039, 1.5779, 1.5768, 1.5768, 1.5768, 1.5768, 1.5768, 1.5768, 1.5767, 1.3728, 1.2344, 1.2343, 1.2343, 1.2343, 1.2343, 1.2343, 1.2343, 1.2343, 1.0829, 0.0733, 0.3511, 0.1778, -1.5428, -0.025, -0.0955, -1.6491, -1.0136, 2.2351, 2.2271, 2.2191, 2.1849, 2.1659, 2.1627, 2.1563, 2.1456, 2.142, 2.1167, 2.1041, 2.1041, 2.0275, 2.0134, 1.9786, 1.9786, 1.9786, 1.9151, 1.9081, 1.9001, 1.8464, 1.771, 1.7315, 1.6986, 1.5579, 1.5579, 1.5579, 1.5579, 1.5579, 1.4168, 1.2174, -1.0625, -0.344, -0.3787, -0.1991, 2.1914, 2.1891, 2.1881, 2.1508, 2.1422, 2.1115, 2.1001, 2.0912, 2.0809, 2.039, 2.0342, 2.0342, 2.0342, 2.0179, 2.0159, 1.9591, 1.9395, 1.9377, 1.9278, 1.8086, 1.6651, 1.6594, 1.6285, 1.5231, 1.508, 1.508, 1.4873, 1.475, 1.3468, 1.3468, 1.2827, 1.1253, -0.0096, 0.1324, 2.2232, 2.2062, 2.1989, 2.1834, 2.156, 2.1557, 2.147, 2.1443, 2.1443, 2.1293, 2.1183, 2.0592, 2.0395, 2.034, 2.0265, 2.0265, 2.0001, 1.9605, 1.922, 1.9024, 1.8364, 1.7319, 1.6262, 1.6262, 1.6262, 1.6261, 1.6261, 1.6261, 1.6261, 1.6261, 1.626, 1.3968, 1.4307, 1.5037, 1.4035, 1.4424, -0.0531, -1.1665, 2.2635, 2.1844, 2.179, 2.1132, 2.0875, 2.0852, 2.0559, 2.0554, 2.0551, 1.9888, 1.9584, 1.9583, 1.9385, 1.6838, 1.6838, 1.6838, 1.6838, 1.6825, 1.6485, 1.6168, 1.364, 1.364, 1.364, 1.364, 1.364, 1.364, 1.364, 1.364, 1.364, 1.364, 1.3057, 0.2975, 1.0399, -0.2556, 0.5919, 0.498, -0.1713, 0.1467, 0.049, -2.0978, 2.2922, 2.253, 2.2373, 2.193, 2.193, 2.1645, 2.1507, 2.1034, 2.0856, 2.0552, 2.0552, 2.0552, 2.0552, 2.0346, 1.9852, 1.9612, 1.9309, 1.8351, 1.8351, 1.8351, 1.835, 1.6772, 1.6772, 1.6771, 1.6505, 1.5138, 1.3536, 1.3535, 1.3535, 1.3535, 1.2835, 1.2543, 1.1392, 0.9749, 0.7012, 0.7159, 0.0025, -0.528, -1.0778, -2.0015, 2.2662, 2.2543, 2.2387, 2.2296, 2.224, 2.1971, 2.1935, 2.1599, 2.1245, 2.1236, 2.1235, 2.1027, 2.09, 2.0324, 1.8265, 1.7704, 1.7704, 1.7704, 1.7703, 1.7703, 1.7703, 1.7703, 1.6302, 1.5816, 1.5661, 1.462, 1.462, 1.4619, 1.4619, 1.4619, 1.3787, 1.4098, 1.2597, 1.2455, 0.6363, -0.3523, -1.3095, -1.0002, -0.8736, -0.6659, -0.095, 0.4618, 2.3062, 2.3024, 2.295, 2.2579, 2.2561, 2.2361, 2.1945, 2.1779, 2.1472, 2.1472, 2.1472, 2.0488, 2.0269, 2.0001, 1.9697, 1.928, 1.9119, 1.7706, 1.5796, 1.4477, 1.4477, 1.4477, 1.4477, 1.4477, 1.4477, 1.4477, 1.4477, 1.4477, 1.4477, 1.4477, 1.397, 0.8882, 1.1076, 0.8159, 0.73, -0.6671, 0.4198, -0.1125, -0.421, -2.7355, -0.6587]}, \"token.table\": {\"Topic\": [1, 3, 10, 6, 6, 6, 2, 9, 2, 3, 5, 8, 9, 10, 2, 7, 10, 7, 6, 7, 6, 9, 10, 3, 5, 9, 5, 8, 7, 4, 4, 5, 9, 1, 3, 5, 6, 1, 7, 9, 7, 2, 7, 10, 2, 6, 8, 9, 7, 3, 5, 2, 8, 2, 7, 3, 5, 2, 3, 4, 8, 10, 2, 4, 3, 7, 4, 1, 5, 8, 6, 3, 6, 10, 10, 7, 9, 10, 7, 5, 9, 2, 10, 4, 1, 3, 8, 10, 5, 1, 2, 3, 6, 3, 1, 2, 8, 5, 6, 7, 8, 9, 7, 9, 8, 5, 8, 7, 8, 10, 5, 2, 9, 6, 5, 6, 6, 5, 9, 5, 7, 9, 10, 5, 9, 3, 9, 4, 1, 2, 7, 9, 6, 8, 9, 2, 6, 7, 10, 10, 7, 8, 9, 8, 8, 1, 2, 8, 10, 6, 4, 9, 1, 8, 10, 2, 8, 10, 3, 4, 10, 7, 5, 10, 9, 9, 1, 5, 7, 2, 3, 8, 8, 1, 6, 5, 10, 8, 8, 9, 7, 8, 2, 8, 9, 1, 7, 8, 6, 9, 6, 7, 6, 7, 8, 10, 1, 6, 5, 7, 9, 4, 9, 4, 2, 10, 6, 3, 1, 5, 8, 5, 9, 1, 9, 7, 6, 6, 7, 8, 4, 3, 5, 10, 2, 5, 6, 3, 8, 9, 9, 1, 9, 1, 8, 4, 4, 8, 10, 10, 7, 8, 10, 6, 1, 2, 10, 4, 7, 2, 4, 7, 9, 10, 5, 8, 9, 1, 2, 7, 8, 10, 8, 4, 9, 9, 4, 9, 4, 1, 3, 7, 10, 2, 3, 5, 6, 7, 9, 5, 8, 6, 2, 9, 10, 3, 6, 9, 10, 5, 7, 9, 2, 9, 10, 9, 2, 4, 1, 6, 9, 2, 4, 5, 10, 8, 6, 7, 10, 7, 8, 9, 10, 7, 5, 6, 8, 4, 1, 9, 4, 3, 8, 4, 8, 1, 8, 3, 5, 3, 10, 10, 4, 5, 4, 7, 8, 5, 9, 2, 5, 9, 7, 9, 9, 6, 4, 6, 8, 9, 9, 4, 7, 9, 10, 1, 7, 10, 10, 5, 8, 9, 10, 10, 10, 1, 4, 7, 9, 1, 2, 6, 1, 9, 10, 9, 5, 5, 5, 3, 6, 1, 8, 9, 10, 4, 5, 7, 6, 1, 7, 3, 10, 7, 10, 6, 2, 7, 10, 1, 7, 2, 3, 8, 9, 4, 3, 5, 6, 7, 8, 9, 10, 7, 8, 5, 8, 5, 6, 2, 9, 10, 3, 7, 8, 5, 7, 2, 8, 4, 7, 4, 6, 10, 9, 8, 5, 7, 3, 7, 8, 4, 4, 5, 9, 4, 2, 5, 7, 7, 9, 3, 8, 8, 8, 7, 9], \"Freq\": [0.610525686439928, 0.8986134699839498, 0.7576133045397258, 0.8104439942897373, 0.602296637943505, 0.602294722262977, 0.8841858586949106, 0.06315613276392218, 0.6985409990308685, 0.04585998423480409, 0.04585998423480409, 0.04585998423480409, 0.04585998423480409, 0.7796197319916696, 0.46145433015912707, 0.19242648702322354, 0.7697059480928942, 0.7570347848808678, 0.6348675801016885, 0.753258031832474, 0.9159506309988307, 0.04850516809090702, 0.9215981937272333, 0.6165168553701021, 0.33722473655932356, 0.33722473655932356, 0.8284420346288616, 0.7587944406907009, 0.7532527206135331, 0.6221256767989539, 0.07931694344329396, 0.5155601323814107, 0.3569262454948228, 0.6081079048756677, 0.6117915945443412, 0.0617724864983907, 0.9265872974758604, 0.03940606579528153, 0.8669334474961936, 0.03940606579528153, 0.7532624532043294, 0.782473208861852, 0.11178188698026457, 0.7576148448881703, 0.8917072422855734, 0.02547734977958781, 0.02547734977958781, 0.02547734977958781, 0.5734687918152611, 0.8053722447761655, 0.16107444895523312, 0.5650000245055453, 0.28250001225277266, 0.6098841229367273, 0.30494206146836367, 0.6117826120791069, 0.8284338085575685, 0.9228740034089928, 0.02307185008522482, 0.03460777512783723, 0.01153592504261241, 0.01153592504261241, 0.157363938970237, 0.786819694851185, 0.1795887563675826, 0.7183550254703304, 0.6336455316342458, 0.8236007273151034, 0.7448335536995769, 0.2482778512331923, 0.9016886641878316, 0.8319977153390289, 0.6130805229586, 0.20436017431953335, 0.7576085028494083, 0.7421681209702768, 0.24738937365675895, 0.7576165603322813, 0.7532624425250867, 0.33722634990189504, 0.33722634990189504, 0.8864709214178519, 0.8457410372560102, 0.633643243694021, 0.7337838667862735, 0.4257823209200485, 0.4257823209200485, 0.8658947184378182, 0.9279275049536211, 0.8874808368819623, 0.6985405402910045, 0.090950385935705, 0.8640286663891974, 0.6117855263752924, 0.6105257272488895, 0.6594169062894327, 0.46927174615945744, 0.6942713266092868, 0.10414069899139303, 0.13885426532185735, 0.03471356633046434, 0.03471356633046434, 0.38671247636367734, 0.38671247636367734, 0.86960131735306, 0.5709361370101975, 0.7728355683667508, 0.5734729863127704, 0.5962039190740019, 0.29810195953700097, 0.4721456417614996, 0.8198651236545247, 0.55458171187092, 0.9016794432926855, 0.8474188357965539, 0.14123647263275896, 0.6022978451725791, 0.33279813592935886, 0.33279813592935886, 0.3281017301676598, 0.3281017301676598, 0.3281017301676598, 0.3281017301676598, 0.8793055525789365, 0.07327546271491138, 0.7404403076804881, 0.24681343589349605, 0.904231023368685, 0.6105201742867841, 0.6985458567990063, 0.18724360297458062, 0.7489744118983225, 0.848789385100103, 0.07073244875834192, 0.07073244875834192, 0.2575816820360601, 0.08586056067868669, 0.34344224271474677, 0.2575816820360601, 0.8457507199433109, 0.07875761787100778, 0.07875761787100778, 0.8663337965810857, 0.46927919873655494, 0.8489397736169133, 0.7271304630857351, 0.1038757804408193, 0.1038757804408193, 0.1038757804408193, 0.8992287246349939, 0.9606902145152602, 0.036949623635202315, 0.6080988586005006, 0.9771144306495233, 0.8142138059383371, 0.544861286338768, 0.18162042877958934, 0.18162042877958934, 0.3733269320188304, 0.3733269320188304, 0.4679218254445826, 0.7476385794046652, 0.11294244754724296, 0.8470683566043222, 0.8040268322579908, 0.5545907433466507, 0.9366257014381452, 0.14148159459029008, 0.7074079729514504, 0.04415629242614783, 0.8831258485229566, 0.04415629242614783, 0.9443996191725837, 0.8864204908017312, 0.9184680317520112, 0.4794557388315456, 0.4794557388315456, 0.8489438162497093, 0.5799026189286296, 0.554585528067948, 0.7532567127373649, 0.7613365626394274, 0.3799247035404763, 0.3799247035404763, 0.9100257029364435, 0.8323176010489616, 0.8818512136725407, 0.09798346818583785, 0.48357620536047313, 0.48357620536047313, 0.8122010140990981, 0.5734752928240747, 0.0646116903720319, 0.8399519748364148, 0.0646116903720319, 0.0646116903720319, 0.7834803543514661, 0.9023346463824408, 0.8188126942919149, 0.7532527669845536, 0.7229677537229433, 0.555216360489938, 0.277608180244969, 0.791556885170892, 0.8729972856955209, 0.14549954761592016, 0.6022861517330721, 0.8320086350971638, 0.7845616458985181, 0.19614041147462952, 0.7587917230820845, 0.6685473982213315, 0.22284913274044382, 0.7288209043641891, 0.5545868175334817, 0.7532573158497261, 0.8706156098915744, 0.5255278815048978, 0.17517596050163262, 0.17517596050163262, 0.9308408511414057, 0.6165196421558867, 0.8741531731526075, 0.7576148494863083, 0.7334604040924424, 0.3534449496075637, 0.7068898992151273, 0.15099670535375706, 0.15099670535375706, 0.6039868214150282, 0.4592198158082598, 0.6269568596548315, 0.31347842982741575, 0.6081030072082046, 0.8489389748516015, 0.8632704973370908, 0.8754627880246642, 0.06734329138651263, 0.06734329138651263, 0.845747281969477, 0.5956361499704875, 0.19854538332349586, 0.19854538332349586, 0.6022931773717408, 0.2657815233728034, 0.5315630467456068, 0.5785360178628242, 0.8461006965509851, 0.10576258706887313, 0.3640276236236316, 0.1820138118118158, 0.1820138118118158, 0.1820138118118158, 0.1820138118118158, 0.3687003155383438, 0.3687003155383438, 0.1843501577691719, 0.9469315766407529, 0.02058546905740767, 0.02058546905740767, 0.02058546905740767, 0.7576190501579984, 0.7587885837711323, 0.6336486395638661, 0.7367748497970842, 0.8447535678944319, 0.925661081784945, 0.7367760916765884, 0.7915567977148775, 0.9063073740212405, 0.01678346988928223, 0.01678346988928223, 0.05035040966784669, 0.5324673820292095, 0.12327266250369691, 0.12327266250369691, 0.36981798751109074, 0.12327266250369691, 0.12327266250369691, 0.4619219284180415, 0.7587836316186551, 0.9313808178075689, 0.1538257865322805, 0.1538257865322805, 0.615303146129122, 0.8349391962098064, 0.7042823182578482, 0.17607057956446204, 0.17607057956446204, 0.7510645485023186, 0.7532623593609077, 0.7367707435624055, 0.7603181190613908, 0.1900795297653477, 0.7576141916241237, 0.7367613912652607, 0.5919543398445275, 0.29597716992226375, 0.0858484809848849, 0.7726363288639642, 0.0858484809848849, 0.8114943769901118, 0.7644234869060822, 0.2548078289686941, 0.7576140264754051, 0.848941373286519, 0.9313790656053366, 0.39821796179535524, 0.39821796179535524, 0.7079679347652345, 0.14159358695304688, 0.4521421409449701, 0.4521421409449701, 0.3958144993438923, 0.7510695412777711, 0.774874981883517, 0.8608792214815266, 0.7759854123648651, 0.4783938776714987, 0.4783938776714987, 0.856043605492198, 0.6117792600695151, 0.9031522404539299, 0.5148637617944405, 0.5799023100283126, 0.781846703030075, 0.156369340606015, 0.8349401558656266, 0.7523013869947546, 0.6165178372614785, 0.7576129019765894, 0.7576203222441202, 0.10732726364346658, 0.8586181091477326, 0.06198206752451013, 0.06198206752451013, 0.8057668778186317, 0.1496416447230193, 0.7482082236150964, 0.6594261441812324, 0.9072561675888887, 0.0756046806324074, 0.07903718753747714, 0.8694090629122485, 0.5545739235387598, 0.6022873844950369, 0.8220701044908172, 0.34636426354509836, 0.34636426354509836, 0.34636426354509836, 0.791975196342468, 0.5993758971308355, 0.14984397428270887, 0.14984397428270887, 0.14984397428270887, 0.758452418233111, 0.8263261674701919, 0.10329077093377399, 0.9511964603775052, 0.5751511852128299, 0.19171706173760997, 0.7860813447496529, 0.1965203361874132, 0.9091412842573555, 0.7665379238864332, 0.6105221055806093, 0.7542870111521843, 0.8341905645414388, 0.5545887880803285, 0.6081030410610249, 0.2903814086013462, 0.5807628172026924, 0.16801149466077026, 0.16801149466077026, 0.672045978643081, 0.5545765769343098, 0.9476527980272063, 0.8284380276699123, 0.7971079091437728, 0.42324474729541456, 0.6022853235708086, 0.09214096732312434, 0.6449867712618703, 0.09214096732312434, 0.09214096732312434, 0.5113916030796027, 0.30121367909843233, 0.30121367909843233, 0.6760199844933324, 0.7774419358062594, 0.1554883871612519, 0.6117852832417763, 0.7576106793595003, 0.11329308414440255, 0.9063446731552204, 0.6022958381451361, 0.274085590890237, 0.06852139772255925, 0.6166925795030332, 0.7482894076936768, 0.7532666558921246, 0.11488234896983293, 0.11488234896983293, 0.8041764427888305, 0.8696590255640967, 0.6221385886295646, 0.883501471966371, 0.023736179998291463, 0.023736179998291463, 0.9494471999316585, 0.33861056640715204, 0.33861056640715204, 0.33861056640715204, 0.2295329937601031, 0.6885989812803093, 0.8258903550090568, 0.07508094136445971, 0.49693179074468374, 0.49693179074468374, 0.6985439954365744, 0.8069998660358979, 0.13449997767264965, 0.8998811214150294, 0.0808881906889914, 0.010111023836123926, 0.74695607643919, 0.14939121528783803, 0.611686247678441, 0.3058431238392205, 0.6221206787106347, 0.573475167295975, 0.6221300591396639, 0.7918695675708534, 0.0879855075078726, 0.7367660842609739, 0.4692865633336535, 0.8370871826573113, 0.13951453044288523, 0.6118071565245381, 0.7532663360739241, 0.5799294937488435, 0.6221281486318694, 0.9344973810045478, 0.03222404762084648, 0.03222404762084648, 0.9746509962986177, 0.22906661953881466, 0.22906661953881466, 0.4581332390776293, 0.07529698529296426, 0.9035638235155712, 0.6117851704488609, 0.8608648194990975, 0.9155324659992324, 0.4692854167552467, 0.7570300038124151, 0.7919783844075589], \"Term\": [\"absolute\", \"action\", \"actively\", \"aggregate\", \"algorithm\", \"allocation\", \"also\", \"also\", \"alternatively\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analysis\", \"analyze\", \"application\", \"application\", \"apply\", \"approach\", \"approve\", \"around\", \"article\", \"article\", \"aspect\", \"assumption\", \"assumption\", \"attribute\", \"automatically\", \"automation\", \"available\", \"base\", \"base\", \"base\", \"become\", \"begin\", \"bertopic\", \"bertopic\", \"business\", \"business\", \"business\", \"calculate\", \"category\", \"category\", \"certain\", \"change\", \"change\", \"change\", \"change\", \"choice\", \"choose\", \"choose\", \"cid\", \"cid\", \"citizen\", \"citizen\", \"classical\", \"cli\", \"climate\", \"climate\", \"climate\", \"climate\", \"climate\", \"cluster\", \"cluster\", \"coherence\", \"coherence\", \"collection\", \"combination\", \"combine\", \"combine\", \"come\", \"commercial\", \"common\", \"common\", \"commonly\", \"community\", \"community\", \"comparison\", \"computational\", \"consider\", \"consider\", \"consist\", \"consistent\", \"contain\", \"content\", \"context\", \"context\", \"continue\", \"contribution\", \"cooperation\", \"counter\", \"country\", \"country\", \"create\", \"crucial\", \"curate\", \"daily\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"dataset\", \"datum\", \"datum\", \"debate\", \"decade\", \"decision\", \"decompose\", \"define\", \"define\", \"dense\", \"derive\", \"development\", \"difference\", \"different\", \"different\", \"difficult\", \"dimensional\", \"dimensional\", \"dirichlet\", \"dirichlet\", \"dirichlet\", \"dirichlet\", \"discourse\", \"discourse\", \"discover\", \"discover\", \"discuss\", \"discussion\", \"display\", \"distribution\", \"distribution\", \"diversity\", \"diversity\", \"diversity\", \"document\", \"document\", \"document\", \"document\", \"domain\", \"economic\", \"economic\", \"economic\", \"element\", \"embed\", \"embedding\", \"embedding\", \"embedding\", \"embedding\", \"emerge\", \"emission\", \"emission\", \"enable\", \"energy\", \"engagement\", \"evaluate\", \"evaluate\", \"evaluate\", \"evaluation\", \"evaluation\", \"evolve\", \"example\", \"experiment\", \"experiment\", \"experimental\", \"extant\", \"extract\", \"fig\", \"fig\", \"figure\", \"figure\", \"figure\", \"finally\", \"first\", \"focus\", \"follow\", \"follow\", \"food\", \"form\", \"formulation\", \"foundational\", \"framework\", \"frequency\", \"frequency\", \"gain\", \"generate\", \"give\", \"give\", \"group\", \"group\", \"help\", \"hence\", \"high\", \"high\", \"high\", \"high\", \"however\", \"identify\", \"impact\", \"implement\", \"importance\", \"important\", \"important\", \"improve\", \"include\", \"include\", \"independent\", \"industry\", \"information\", \"information\", \"internal\", \"involve\", \"involve\", \"issue\", \"joint\", \"know\", \"label\", \"language\", \"language\", \"language\", \"large\", \"last\", \"latent\", \"lazaro\", \"learn\", \"level\", \"level\", \"literature\", \"literature\", \"literature\", \"log\", \"low\", \"low\", \"machine\", \"make\", \"management\", \"mapping\", \"mapping\", \"mapping\", \"mate\", \"matrix\", \"matrix\", \"matrix\", \"mean\", \"measure\", \"measure\", \"mechanism\", \"medium\", \"medium\", \"method\", \"method\", \"method\", \"method\", \"method\", \"metric\", \"metric\", \"metric\", \"model\", \"model\", \"model\", \"model\", \"module\", \"multinomial\", \"multiple\", \"nascent\", \"natural\", \"need\", \"network\", \"new\", \"news\", \"news\", \"news\", \"news\", \"newspaper\", \"nmf\", \"nmf\", \"nmf\", \"nmf\", \"nmf\", \"non\", \"notably\", \"note\", \"number\", \"number\", \"number\", \"observe\", \"octis\", \"octis\", \"octis\", \"offer\", \"outline\", \"output\", \"panel\", \"panel\", \"partisan\", \"pattern\", \"perform\", \"perform\", \"performance\", \"performance\", \"performance\", \"perspective\", \"platform\", \"platform\", \"pointwise\", \"policy\", \"political\", \"popular\", \"popular\", \"present\", \"present\", \"prior\", \"prior\", \"probabilistic\", \"probability\", \"process\", \"provide\", \"public\", \"range\", \"range\", \"rank\", \"real\", \"recent\", \"reduce\", \"reduction\", \"relate\", \"relate\", \"relationship\", \"relevant\", \"remain\", \"remove\", \"repetitive\", \"report\", \"report\", \"represent\", \"represent\", \"represent\", \"representation\", \"representation\", \"require\", \"resource\", \"resource\", \"result\", \"result\", \"review\", \"rmn\", \"role\", \"run\", \"run\", \"run\", \"scale\", \"science\", \"science\", \"science\", \"science\", \"scientific\", \"score\", \"score\", \"second\", \"section\", \"section\", \"sector\", \"sector\", \"see\", \"select\", \"selection\", \"semantic\", \"semantically\", \"semeval\", \"sentence\", \"set\", \"set\", \"several\", \"several\", \"several\", \"shortcoming\", \"show\", \"significantly\", \"similar\", \"similarity\", \"sis\", \"social\", \"social\", \"social\", \"social\", \"source\", \"space\", \"space\", \"specific\", \"specifically\", \"specifically\", \"sport\", \"square\", \"stakeholder\", \"stakeholder\", \"standard\", \"study\", \"study\", \"study\", \"suggest\", \"sure\", \"technique\", \"technique\", \"technique\", \"term\", \"test\", \"text\", \"theme\", \"theme\", \"theme\", \"therefore\", \"therefore\", \"therefore\", \"thus\", \"thus\", \"time\", \"time\", \"tion\", \"tion\", \"together\", \"top\", \"top\", \"topic\", \"topic\", \"topic\", \"traction\", \"traction\", \"train\", \"train\", \"transformer\", \"tweet\", \"typically\", \"umamaheswaran\", \"umamaheswaran\", \"ument\", \"underlie\", \"understand\", \"understand\", \"unique\", \"unstructure\", \"unsupervise\", \"update\", \"use\", \"use\", \"use\", \"value\", \"vector\", \"vector\", \"vector\", \"volume\", \"volume\", \"way\", \"well\", \"word\", \"work\", \"year\", \"yield\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [9, 10, 1, 8, 6, 7, 4, 2, 5, 3]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el285841652159311408638419384\", ldavis_el285841652159311408638419384_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el285841652159311408638419384\", ldavis_el285841652159311408638419384_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el285841652159311408638419384\", ldavis_el285841652159311408638419384_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "8     -0.011678  0.243913       1        1  10.820901\n",
       "9      0.287122  0.004307       2        1  10.521426\n",
       "0     -0.057827  0.053531       3        1  10.221609\n",
       "7      0.074877 -0.143549       4        1  10.194247\n",
       "5     -0.099545 -0.102957       5        1  10.146109\n",
       "6     -0.062203 -0.039391       6        1   9.919700\n",
       "3     -0.074033  0.019639       7        1   9.909490\n",
       "1     -0.016628  0.004383       8        1   9.852549\n",
       "4     -0.053604 -0.071845       9        1   9.415848\n",
       "2      0.013519  0.031968      10        1   8.998121, topic_info=              Term       Freq      Total Category  logprob  loglift\n",
       "46           topic  98.000000  98.000000  Default  30.0000  30.0000\n",
       "12         climate  86.000000  86.000000  Default  29.0000  29.0000\n",
       "33            news  59.000000  59.000000  Default  28.0000  28.0000\n",
       "62           model  48.000000  48.000000  Default  27.0000  27.0000\n",
       "47           theme  42.000000  42.000000  Default  26.0000  26.0000\n",
       "..             ...        ...        ...      ...      ...      ...\n",
       "107        science   0.913734   6.673608  Topic10  -5.5579   0.4198\n",
       "175  umamaheswaran   0.913904  11.365508  Topic10  -5.5578  -0.1125\n",
       "82            high   0.914124  15.477075  Topic10  -5.5575  -0.4210\n",
       "12         climate   0.505906  86.685723  Topic10  -6.1491  -2.7355\n",
       "106         social   0.505373  10.852936  Topic10  -6.1502  -0.6587\n",
       "\n",
       "[410 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "1         1  0.610526   absolute\n",
       "13        3  0.898613     action\n",
       "623      10  0.757613   actively\n",
       "598       6  0.810444  aggregate\n",
       "205       6  0.602297  algorithm\n",
       "...     ...       ...        ...\n",
       "423       8  0.860865       well\n",
       "183       8  0.915532       word\n",
       "53        8  0.469285       work\n",
       "103       7  0.757030       year\n",
       "186       9  0.791978      yield\n",
       "\n",
       "[457 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[9, 10, 1, 8, 6, 7, 4, 2, 5, 3])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.8184743007946306\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
